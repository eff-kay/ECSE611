From 160e4c90c78413c4284091f832fd9a3a3f75f454 Mon Sep 17 00:00:00 2001
From: Duo Zhang <zhangduo@apache.org>
Date: Wed, 27 Feb 2019 18:47:07 +0800
Subject: [PATCH] HBASE-21717 Implement Connection based on AsyncConnection

---
 .../hadoop/hbase/backup/TestBackupBase.java   |   5 +-
 .../hadoop/hbase/backup/TestBackupMerge.java  |  11 +-
 .../backup/TestBackupMultipleDeletes.java     |  10 +-
 .../hbase/backup/TestIncrementalBackup.java   |  16 +-
 .../TestIncrementalBackupDeleteTable.java     |  10 +-
 ...estIncrementalBackupMergeWithFailures.java |   7 +-
 .../TestIncrementalBackupWithBulkLoad.java    |   6 +-
 .../TestIncrementalBackupWithFailures.java    |   6 +-
 .../hadoop/hbase/backup/TestRemoteBackup.java |   8 +-
 .../backup/master/TestBackupLogCleaner.java   |   6 +-
 .../hadoop/hbase/client/AsyncConnection.java  |   8 +
 .../hbase/client/AsyncConnectionImpl.java     |  49 +-
 .../client/AsyncRegionLocatorHelper.java      |   2 +-
 .../hbase/client/AsyncTableResultScanner.java |   4 +
 .../hadoop/hbase/client/Connection.java       |  17 +-
 .../hbase/client/ConnectionFactory.java       |  44 +-
 .../client/ConnectionImplementation.java      |  54 +-
 .../client/ConnectionOverAsyncConnection.java | 180 ++++++
 .../hadoop/hbase/client/ConnectionUtils.java  | 104 ++--
 .../hadoop/hbase/client/HBaseAdmin.java       |   5 +-
 .../apache/hadoop/hbase/client/HTable.java    | 213 +------
 .../RegionCoprocessorRpcChannelImpl.java      |  37 +-
 .../org/apache/hadoop/hbase/client/Table.java | 540 ++++--------------
 .../hbase/client/TableOverAsyncTable.java     | 527 +++++++++++++++++
 .../hbase/ipc/CoprocessorRpcChannel.java      |  12 +-
 .../hadoop/hbase/client/SimpleRegistry.java   |  83 +++
 .../hadoop/hbase/client/TestAsyncProcess.java |   2 +-
 .../hbase/client/TestBufferedMutator.java     |  14 +-
 .../hbase/client/TestClientNoCluster.java     |  33 +-
 .../mapreduce/TestHFileOutputFormat2.java     |  27 +-
 .../TestMultiTableInputFormatBase.java        |   6 +
 .../mapreduce/TestTableInputFormatBase.java   |   6 +
 .../hadoop/hbase/rest/ResourceBase.java       |   8 +-
 .../hadoop/hbase/rest/SchemaResource.java     |  18 +-
 .../hbase/rest/client/RemoteHTable.java       | 268 +++------
 .../hbase/rest/TestScannerResource.java       |  10 +-
 .../hbase/rest/client/TestRemoteTable.java    |   7 +-
 .../hbase/client/SharedAsyncConnection.java   | 112 ++++
 .../hbase/{ => client}/SharedConnection.java  |  18 +-
 .../apache/hadoop/hbase/master/HMaster.java   |   5 +-
 .../hbase/master/MasterCoprocessorHost.java   |   2 +-
 .../hbase/regionserver/HRegionServer.java     |  68 +--
 .../regionserver/RegionCoprocessorHost.java   |   2 +-
 .../RegionServerCoprocessorHost.java          |   2 +-
 .../hadoop/hbase/util/MultiHConnection.java   | 141 -----
 .../resources/hbase-webapps/master/table.jsp  |   5 +-
 .../hadoop/hbase/HBaseTestingUtility.java     |  95 +--
 .../TestPartialResultsFromClientSide.java     |  23 +-
 .../AbstractTestCIOperationTimeout.java       |   4 +-
 .../client/AbstractTestCIRpcTimeout.java      |   2 +-
 .../hbase/client/AbstractTestCITimeout.java   |   2 +-
 .../client/DummyAsyncClusterConnection.java   |   5 +
 .../hadoop/hbase/client/TestAdmin1.java       | 288 +++++-----
 .../hadoop/hbase/client/TestAdmin2.java       | 139 +++--
 .../hbase/client/TestAlwaysSetScannerId.java  |  34 +-
 .../hbase/client/TestAsyncTableAdminApi.java  |   2 +-
 .../hbase/client/TestCIBadHostname.java       |  28 +-
 .../hadoop/hbase/client/TestCISleep.java      |  71 +--
 .../hbase/client/TestCheckAndMutate.java      |  30 +-
 .../hbase/client/TestClientPushback.java      | 214 +++----
 .../client/TestConnectionImplementation.java  |   3 +
 .../hbase/client/TestFromClientSide.java      | 182 +++---
 .../hbase/client/TestFromClientSide3.java     |  87 ++-
 .../TestFromClientSideScanExcpetion.java      |   9 +-
 .../hbase/client/TestGetProcedureResult.java  |   7 +-
 .../client/TestIncrementsFromClientSide.java  |  61 +-
 .../client/TestMalformedCellFromClient.java   |  36 +-
 .../hadoop/hbase/client/TestMetaCache.java    |   5 +
 .../hbase/client/TestMetaWithReplicas.java    |   9 +-
 .../TestMultiActionMetricsFromClient.java     |  13 +-
 .../hbase/client/TestMultiParallel.java       | 103 ++--
 .../client/TestRegionLocationCaching.java     |   5 +
 .../hbase/client/TestReplicaWithCluster.java  |   5 +
 .../hbase/client/TestReplicasClient.java      |  38 +-
 .../client/TestScanWithoutFetchingData.java   |  32 +-
 .../client/TestScannersFromClientSide.java    | 168 +-----
 .../client/TestShortCircuitConnection.java    |  95 ---
 .../client/TestSnapshotCloneIndependence.java |   2 +-
 .../hbase/client/TestSnapshotMetadata.java    |   2 +-
 ...a => TestCoprocessorSharedConnection.java} |  23 +-
 .../TestPassCustomCellViaRegionObserver.java  |   5 +-
 .../hbase/filter/TestMultiRowRangeFilter.java |  45 +-
 .../hadoop/hbase/master/TestWarmupRegion.java |   4 +-
 .../hbase/regionserver/RegionAsTable.java     | 120 +---
 .../TestEndToEndSplitTransaction.java         |   2 +-
 .../regionserver/TestHRegionFileSystem.java   |  17 +-
 .../TestNewVersionBehaviorFromClientSide.java |   7 +-
 .../TestPerColumnFamilyFlush.java             |   7 -
 .../TestSettingTimeoutOnBlockingPoint.java    |  14 +-
 .../replication/TestReplicationBase.java      |   2 +-
 .../replication/TestReplicationWithTags.java  |   2 +-
 ...estCoprocessorWhitelistMasterObserver.java |  10 +-
 .../snapshot/TestRegionSnapshotTask.java      |   2 +-
 .../hadoop/hbase/tool/TestCanaryTool.java     |   3 +-
 .../hbase/util/MultiThreadedAction.java       |   5 +-
 .../util/hbck/OfflineMetaRebuildTestCore.java |   9 +-
 .../thrift/ThriftHBaseServiceHandler.java     |  12 +-
 .../thrift2/ThriftHBaseServiceHandler.java    |   2 +-
 .../thrift2/client/ThriftConnection.java      |   6 +
 .../hbase/thrift2/client/ThriftTable.java     |   4 +-
 .../hbase/thrift2/TestThriftConnection.java   |   2 +-
 101 files changed, 2282 insertions(+), 2538 deletions(-)
 create mode 100644 hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java
 create mode 100644 hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java
 create mode 100644 hbase-client/src/test/java/org/apache/hadoop/hbase/client/SimpleRegistry.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedAsyncConnection.java
 rename hbase-server/src/main/java/org/apache/hadoop/hbase/{ => client}/SharedConnection.java (85%)
 delete mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/util/MultiHConnection.java
 delete mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShortCircuitConnection.java
 rename hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/{TestCoprocessorShortCircuitRPC.java => TestCoprocessorSharedConnection.java} (80%)

diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java
index 2afdb4f4c1..e0fca20b54 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java
@@ -54,7 +54,6 @@ import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.Durability;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.master.cleaner.LogCleaner;
@@ -354,9 +353,9 @@ public class TestBackupBase {
     TEST_UTIL.shutdownMiniMapReduceCluster();
   }
 
-  HTable insertIntoTable(Connection conn, TableName table, byte[] family, int id, int numRows)
+  Table insertIntoTable(Connection conn, TableName table, byte[] family, int id, int numRows)
       throws IOException {
-    HTable t = (HTable) conn.getTable(table);
+    Table t = conn.getTable(table);
     Put p1;
     for (int i = 0; i < numRows; i++) {
       p1 = new Put(Bytes.toBytes("row-" + table + "-" + id + "-" + i));
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java
index 8ead548110..beacef3c14 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java
@@ -20,7 +20,6 @@ package org.apache.hadoop.hbase.backup;
 import static org.junit.Assert.assertTrue;
 
 import java.util.List;
-
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.backup.impl.BackupAdminImpl;
@@ -28,10 +27,8 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
-import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
 import org.junit.Assert;
 import org.junit.ClassRule;
 import org.junit.Test;
@@ -39,6 +36,8 @@ import org.junit.experimental.categories.Category;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
+
 @Category(LargeTests.class)
 public class TestBackupMerge extends TestBackupBase {
 
@@ -72,14 +71,14 @@ public class TestBackupMerge extends TestBackupBase {
     assertTrue(checkSucceeded(backupIdFull));
 
     // #2 - insert some data to table1
-    HTable t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
+    Table t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
     LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
 
     Assert.assertEquals(TEST_UTIL.countRows(t1), NB_ROWS_IN_BATCH + ADD_ROWS);
     t1.close();
     LOG.debug("written " + ADD_ROWS + " rows to " + table1);
 
-    HTable t2 = insertIntoTable(conn, table2, famName, 1, ADD_ROWS);
+    Table t2 = insertIntoTable(conn, table2, famName, 1, ADD_ROWS);
 
     Assert.assertEquals(TEST_UTIL.countRows(t2), NB_ROWS_IN_BATCH + ADD_ROWS);
     t2.close();
@@ -115,7 +114,7 @@ public class TestBackupMerge extends TestBackupBase {
       tablesRestoreIncMultiple, tablesMapIncMultiple, true));
 
     Table hTable = conn.getTable(table1_restore);
-    LOG.debug("After incremental restore: " + hTable.getTableDescriptor());
+    LOG.debug("After incremental restore: " + hTable.getDescriptor());
     int countRows = TEST_UTIL.countRows(hTable, famName);
     LOG.debug("f1 has " + countRows + " rows");
     Assert.assertEquals(NB_ROWS_IN_BATCH + 2 * ADD_ROWS, countRows);
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMultipleDeletes.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMultipleDeletes.java
index db1a4e2e31..bffa480817 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMultipleDeletes.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMultipleDeletes.java
@@ -29,8 +29,8 @@ import org.apache.hadoop.hbase.backup.impl.BackupAdminImpl;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.Assert;
@@ -67,7 +67,7 @@ public class TestBackupMultipleDeletes extends TestBackupBase {
     String backupIdFull = client.backupTables(request);
     assertTrue(checkSucceeded(backupIdFull));
     // #2 - insert some data to table table1
-    HTable t1 = (HTable) conn.getTable(table1);
+    Table t1 = conn.getTable(table1);
     Put p1;
     for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
       p1 = new Put(Bytes.toBytes("row-t1" + i));
@@ -82,7 +82,7 @@ public class TestBackupMultipleDeletes extends TestBackupBase {
     String backupIdInc1 = client.backupTables(request);
     assertTrue(checkSucceeded(backupIdInc1));
     // #4 - insert some data to table table2
-    HTable t2 = (HTable) conn.getTable(table2);
+    Table t2 = conn.getTable(table2);
     Put p2 = null;
     for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
       p2 = new Put(Bytes.toBytes("row-t2" + i));
@@ -95,7 +95,7 @@ public class TestBackupMultipleDeletes extends TestBackupBase {
     String backupIdInc2 = client.backupTables(request);
     assertTrue(checkSucceeded(backupIdInc2));
     // #6 - insert some data to table table1
-    t1 = (HTable) conn.getTable(table1);
+    t1 = conn.getTable(table1);
     for (int i = NB_ROWS_IN_BATCH; i < 2 * NB_ROWS_IN_BATCH; i++) {
       p1 = new Put(Bytes.toBytes("row-t1" + i));
       p1.addColumn(famName, qualName, Bytes.toBytes("val" + i));
@@ -107,7 +107,7 @@ public class TestBackupMultipleDeletes extends TestBackupBase {
     String backupIdInc3 = client.backupTables(request);
     assertTrue(checkSucceeded(backupIdInc3));
     // #8 - insert some data to table table2
-    t2 = (HTable) conn.getTable(table2);
+    t2 = conn.getTable(table2);
     for (int i = NB_ROWS_IN_BATCH; i < 2 * NB_ROWS_IN_BATCH; i++) {
       p2 = new Put(Bytes.toBytes("row-t1" + i));
       p2.addColumn(famName, qualName, Bytes.toBytes("val" + i));
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java
index 6e1523812a..064c45e80c 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java
@@ -33,8 +33,8 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -102,7 +102,7 @@ public class TestIncrementalBackup extends TestBackupBase {
       assertTrue(checkSucceeded(backupIdFull));
 
       // #2 - insert some data to table
-      HTable t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
+      Table t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
       LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
       Assert.assertEquals(HBaseTestingUtility.countRows(t1),
               NB_ROWS_IN_BATCH + ADD_ROWS + NB_ROWS_FAM3);
@@ -116,7 +116,7 @@ public class TestIncrementalBackup extends TestBackupBase {
       Assert.assertEquals(HBaseTestingUtility.countRows(t1),
               NB_ROWS_IN_BATCH + ADD_ROWS + NB_ROWS_MOB);
 
-      HTable t2 = (HTable) conn.getTable(table2);
+      Table t2 = conn.getTable(table2);
       Put p2;
       for (int i = 0; i < 5; i++) {
         p2 = new Put(Bytes.toBytes("row-t2" + i));
@@ -163,7 +163,7 @@ public class TestIncrementalBackup extends TestBackupBase {
       HBaseTestingUtility.modifyTableSync(TEST_UTIL.getAdmin(), table1Desc);
 
       int NB_ROWS_FAM2 = 7;
-      HTable t3 = insertIntoTable(conn, table1, fam2Name, 2, NB_ROWS_FAM2);
+      Table t3 = insertIntoTable(conn, table1, fam2Name, 2, NB_ROWS_FAM2);
       t3.close();
 
       // Wait for 5 sec to make sure that old WALs were deleted
@@ -189,11 +189,11 @@ public class TestIncrementalBackup extends TestBackupBase {
       hAdmin.close();
 
       // #6.2 - checking row count of tables for full restore
-      HTable hTable = (HTable) conn.getTable(table1_restore);
+      Table hTable = conn.getTable(table1_restore);
       Assert.assertEquals(HBaseTestingUtility.countRows(hTable), NB_ROWS_IN_BATCH + NB_ROWS_FAM3);
       hTable.close();
 
-      hTable = (HTable) conn.getTable(table2_restore);
+      hTable = conn.getTable(table2_restore);
       Assert.assertEquals(NB_ROWS_IN_BATCH, HBaseTestingUtility.countRows(hTable));
       hTable.close();
 
@@ -202,7 +202,7 @@ public class TestIncrementalBackup extends TestBackupBase {
       TableName[] tablesMapIncMultiple = new TableName[] { table1_restore, table2_restore };
       client.restore(BackupUtils.createRestoreRequest(BACKUP_ROOT_DIR, backupIdIncMultiple2,
               false, tablesRestoreIncMultiple, tablesMapIncMultiple, true));
-      hTable = (HTable) conn.getTable(table1_restore);
+      hTable = conn.getTable(table1_restore);
 
       LOG.debug("After incremental restore: " + hTable.getDescriptor());
       int countFamName = TEST_UTIL.countRows(hTable, famName);
@@ -218,7 +218,7 @@ public class TestIncrementalBackup extends TestBackupBase {
       Assert.assertEquals(countMobName, NB_ROWS_MOB);
       hTable.close();
 
-      hTable = (HTable) conn.getTable(table2_restore);
+      hTable = conn.getTable(table2_restore);
       Assert.assertEquals(NB_ROWS_IN_BATCH + 5, HBaseTestingUtility.countRows(hTable));
       hTable.close();
       admin.close();
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupDeleteTable.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupDeleteTable.java
index f8129d9223..08834f2fae 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupDeleteTable.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupDeleteTable.java
@@ -27,8 +27,8 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.Assert;
@@ -75,7 +75,7 @@ public class TestIncrementalBackupDeleteTable extends TestBackupBase {
     assertTrue(checkSucceeded(backupIdFull));
 
     // #2 - insert some data to table table1
-    HTable t1 = (HTable) conn.getTable(table1);
+    Table t1 = conn.getTable(table1);
     Put p1;
     for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
       p1 = new Put(Bytes.toBytes("row-t1" + i));
@@ -110,11 +110,11 @@ public class TestIncrementalBackupDeleteTable extends TestBackupBase {
     assertTrue(hAdmin.tableExists(table2_restore));
 
     // #5.2 - checking row count of tables for full restore
-    HTable hTable = (HTable) conn.getTable(table1_restore);
+    Table hTable = conn.getTable(table1_restore);
     Assert.assertEquals(TEST_UTIL.countRows(hTable), NB_ROWS_IN_BATCH);
     hTable.close();
 
-    hTable = (HTable) conn.getTable(table2_restore);
+    hTable = conn.getTable(table2_restore);
     Assert.assertEquals(TEST_UTIL.countRows(hTable), NB_ROWS_IN_BATCH);
     hTable.close();
 
@@ -124,7 +124,7 @@ public class TestIncrementalBackupDeleteTable extends TestBackupBase {
     client.restore(BackupUtils.createRestoreRequest(BACKUP_ROOT_DIR, backupIdIncMultiple,
       false, tablesRestoreIncMultiple, tablesMapIncMultiple, true));
 
-    hTable = (HTable) conn.getTable(table1_restore);
+    hTable = conn.getTable(table1_restore);
     Assert.assertEquals(TEST_UTIL.countRows(hTable), NB_ROWS_IN_BATCH * 2);
     hTable.close();
     admin.close();
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java
index 57bdc46409..73512587c4 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java
@@ -39,7 +39,6 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Pair;
@@ -245,14 +244,14 @@ public class TestIncrementalBackupMergeWithFailures extends TestBackupBase {
     assertTrue(checkSucceeded(backupIdFull));
 
     // #2 - insert some data to table1
-    HTable t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
+    Table t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
     LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
 
     Assert.assertEquals(TEST_UTIL.countRows(t1), NB_ROWS_IN_BATCH + ADD_ROWS);
     t1.close();
     LOG.debug("written " + ADD_ROWS + " rows to " + table1);
 
-    HTable t2 = insertIntoTable(conn, table2, famName, 1, ADD_ROWS);
+    Table t2 = insertIntoTable(conn, table2, famName, 1, ADD_ROWS);
 
     Assert.assertEquals(TEST_UTIL.countRows(t2), NB_ROWS_IN_BATCH + ADD_ROWS);
     t2.close();
@@ -334,7 +333,7 @@ public class TestIncrementalBackupMergeWithFailures extends TestBackupBase {
       tablesRestoreIncMultiple, tablesMapIncMultiple, true));
 
     Table hTable = conn.getTable(table1_restore);
-    LOG.debug("After incremental restore: " + hTable.getTableDescriptor());
+    LOG.debug("After incremental restore: " + hTable.getDescriptor());
     LOG.debug("f1 has " + TEST_UTIL.countRows(hTable, famName) + " rows");
     Assert.assertEquals(TEST_UTIL.countRows(hTable, famName), NB_ROWS_IN_BATCH + 2 * ADD_ROWS);
 
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java
index 82f0fb7b05..4b02077934 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java
@@ -29,8 +29,8 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.tool.TestBulkLoadHFiles;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -79,7 +79,7 @@ public class TestIncrementalBackupWithBulkLoad extends TestBackupBase {
     assertTrue(checkSucceeded(backupIdFull));
 
     // #2 - insert some data to table table1
-    HTable t1 = (HTable) conn.getTable(table1);
+    Table t1 = conn.getTable(table1);
     Put p1;
     for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
       p1 = new Put(Bytes.toBytes("row-t1" + i));
@@ -127,7 +127,7 @@ public class TestIncrementalBackupWithBulkLoad extends TestBackupBase {
     client.restore(BackupUtils.createRestoreRequest(BACKUP_ROOT_DIR, backupIdIncMultiple1,
       false, tablesRestoreIncMultiple, tablesRestoreIncMultiple, true));
 
-    HTable hTable = (HTable) conn.getTable(table1);
+    Table hTable = conn.getTable(table1);
     Assert.assertEquals(TEST_UTIL.countRows(hTable), NB_ROWS_IN_BATCH * 2 + actual + actual1);
     request = createBackupRequest(BackupType.FULL, tables, BACKUP_ROOT_DIR);
 
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java
index d5829b2cbe..f6725d9e25 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java
@@ -35,8 +35,8 @@ import org.apache.hadoop.hbase.backup.impl.TableBackupClient.Stage;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.util.ToolRunner;
@@ -100,14 +100,14 @@ public class TestIncrementalBackupWithFailures extends TestBackupBase {
     assertTrue(checkSucceeded(backupIdFull));
 
     // #2 - insert some data to table
-    HTable t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
+    Table t1 = insertIntoTable(conn, table1, famName, 1, ADD_ROWS);
     LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
 
     Assert.assertEquals(TEST_UTIL.countRows(t1), NB_ROWS_IN_BATCH + ADD_ROWS + NB_ROWS_FAM3);
     t1.close();
     LOG.debug("written " + ADD_ROWS + " rows to " + table1);
 
-    HTable t2 = (HTable) conn.getTable(table2);
+    Table t2 = conn.getTable(table2);
     Put p2;
     for (int i = 0; i < 5; i++) {
       p2 = new Put(Bytes.toBytes("row-t2" + i));
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestRemoteBackup.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestRemoteBackup.java
index a0226e6f2b..05826e204b 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestRemoteBackup.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestRemoteBackup.java
@@ -29,8 +29,8 @@ import org.apache.hadoop.hbase.backup.util.BackupUtils;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.snapshot.MobSnapshotTestingUtils;
 import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
@@ -78,7 +78,7 @@ public class TestRemoteBackup extends TestBackupBase {
       } catch (InterruptedException ie) {
       }
       try {
-        HTable t1 = (HTable) conn.getTable(table1);
+        Table t1 = conn.getTable(table1);
         Put p1;
         for (int i = 0; i < NB_ROWS_IN_FAM3; i++) {
           p1 = new Put(Bytes.toBytes("row-t1" + i));
@@ -102,7 +102,7 @@ public class TestRemoteBackup extends TestBackupBase {
     HBaseTestingUtility.modifyTableSync(TEST_UTIL.getAdmin(), table1Desc);
 
     SnapshotTestingUtils.loadData(TEST_UTIL, table1, 50, fam2Name);
-    HTable t1 = (HTable) conn.getTable(table1);
+    Table t1 = conn.getTable(table1);
     int rows0 = MobSnapshotTestingUtils.countMobRows(t1, fam2Name);
 
     latch.countDown();
@@ -130,7 +130,7 @@ public class TestRemoteBackup extends TestBackupBase {
     assertTrue(hAdmin.tableExists(table1_restore));
 
     // #5.2 - checking row count of tables for full restore
-    HTable hTable = (HTable) conn.getTable(table1_restore);
+    Table hTable = conn.getTable(table1_restore);
     Assert.assertEquals(TEST_UTIL.countRows(hTable, famName), NB_ROWS_IN_BATCH);
     int cnt3 = TEST_UTIL.countRows(hTable, fam3Name);
     Assert.assertTrue(cnt3 >= 0 && cnt3 <= NB_ROWS_IN_FAM3);
diff --git a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java
index 9273487f92..6b8011eca5 100644
--- a/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java
+++ b/hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java
@@ -38,8 +38,8 @@ import org.apache.hadoop.hbase.backup.TestBackupBase;
 import org.apache.hadoop.hbase.backup.impl.BackupSystemTable;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.CommonFSUtils;
@@ -107,7 +107,7 @@ public class TestBackupLogCleaner extends TestBackupBase {
       assertTrue(walFiles.size() < newWalFiles.size());
       Connection conn = ConnectionFactory.createConnection(conf1);
       // #2 - insert some data to table
-      HTable t1 = (HTable) conn.getTable(table1);
+      Table t1 = conn.getTable(table1);
       Put p1;
       for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
         p1 = new Put(Bytes.toBytes("row-t1" + i));
@@ -117,7 +117,7 @@ public class TestBackupLogCleaner extends TestBackupBase {
 
       t1.close();
 
-      HTable t2 = (HTable) conn.getTable(table2);
+      Table t2 = conn.getTable(table2);
       Put p2;
       for (int i = 0; i < 5; i++) {
         p2 = new Put(Bytes.toBytes("row-t2" + i));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnection.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnection.java
index 75971ad610..0546520bbf 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnection.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnection.java
@@ -201,6 +201,14 @@ public interface AsyncConnection extends Closeable {
    */
   boolean isClosed();
 
+  /**
+   * Convert this connection to a {@link Connection}.
+   * <p/>
+   * Usually we will return the same instance if you call this method multiple times so you can
+   * consider this as a light-weighted operation.
+   */
+  Connection toConnection();
+
   /**
    * Retrieve an Hbck implementation to fix an HBase cluster. The returned Hbck is not guaranteed to
    * be thread-safe. A new instance should be created by each thread. This is a lightweight
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java
index 7c91e49935..d05062ce54 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java
@@ -23,6 +23,7 @@ import static org.apache.hadoop.hbase.client.NonceGenerator.CLIENT_NONCES_ENABLE
 import static org.apache.hadoop.hbase.util.FutureUtils.addListener;
 
 import java.io.IOException;
+import java.io.UncheckedIOException;
 import java.net.SocketAddress;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
@@ -108,6 +109,8 @@ class AsyncConnectionImpl implements AsyncConnection {
 
   private volatile boolean closed = false;
 
+  private volatile ConnectionOverAsyncConnection conn;
+
   public AsyncConnectionImpl(Configuration conf, AsyncRegistry registry, String clusterId,
       SocketAddress localAddress, User user) {
     this.conf = conf;
@@ -141,6 +144,11 @@ class AsyncConnectionImpl implements AsyncConnection {
     return conf;
   }
 
+  @Override
+  public boolean isClosed() {
+    return closed;
+  }
+
   @Override
   public void close() {
     // As the code below is safe to be executed in parallel, here we do not use CAS or lock, just a
@@ -153,17 +161,21 @@ class AsyncConnectionImpl implements AsyncConnection {
     if (authService != null) {
       authService.shutdown();
     }
+    ConnectionOverAsyncConnection c = this.conn;
+    if (c != null) {
+      c.closeConnImpl();
+    }
     closed = true;
   }
 
   @Override
-  public boolean isClosed() {
-    return closed;
+  public AsyncTableRegionLocator getRegionLocator(TableName tableName) {
+    return new AsyncTableRegionLocatorImpl(tableName, this);
   }
 
   @Override
-  public AsyncTableRegionLocator getRegionLocator(TableName tableName) {
-    return new AsyncTableRegionLocatorImpl(tableName, this);
+  public void clearRegionLocationCache() {
+    locator.clearCache();
   }
 
   // we will override this method for testing retry caller, so do not remove this method.
@@ -341,6 +353,30 @@ class AsyncConnectionImpl implements AsyncConnection {
       RETRY_TIMER);
   }
 
+  @Override
+  public Connection toConnection() {
+    ConnectionOverAsyncConnection c = this.conn;
+    if (c != null) {
+      return c;
+    }
+    synchronized (this) {
+      c = this.conn;
+      if (c != null) {
+        return c;
+      }
+      try {
+        c = new ConnectionOverAsyncConnection(this,
+          ConnectionFactory.createConnectionImpl(conf, null, user));
+      } catch (IOException e) {
+        // TODO: finally we will not rely on ConnectionImplementation anymore and there will no
+        // IOException here.
+        throw new UncheckedIOException(e);
+      }
+      this.conn = c;
+    }
+    return c;
+  }
+
   @Override
   public CompletableFuture<Hbck> getHbck() {
     CompletableFuture<Hbck> future = new CompletableFuture<>();
@@ -366,9 +402,4 @@ class AsyncConnectionImpl implements AsyncConnection {
     return new HBaseHbck(MasterProtos.HbckService.newBlockingStub(
       rpcClient.createBlockingRpcChannel(masterServer, user, rpcTimeout)), rpcControllerFactory);
   }
-
-  @Override
-  public void clearRegionLocationCache() {
-    locator.clearCache();
-  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java
index 4dde1bbc89..c8c6095b02 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java
@@ -74,7 +74,7 @@ final class AsyncRegionLocatorHelper {
       RegionMovedException rme = (RegionMovedException) cause;
       HRegionLocation newLoc =
         new HRegionLocation(loc.getRegion(), rme.getServerName(), rme.getLocationSeqNum());
-      LOG.debug("Try updating {} with the new location {} constructed by {}", loc, newLoc, rme);
+      LOG.debug("Try updating {} with the new location {} constructed by {}", loc, newLoc, rme.toString());
       addToCache.accept(newLoc);
     } else {
       LOG.debug("Try removing {} from cache", loc);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java
index 9b97e93ef2..cd5d5adb29 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java
@@ -187,4 +187,8 @@ class AsyncTableResultScanner implements ResultScanner, AdvancedScanResultConsum
   public ScanMetrics getScanMetrics() {
     return scanMetrics;
   }
+
+  int getCacheSize() {
+    return queue.size();
+  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Connection.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Connection.java
index 90891f411b..b88c40c6eb 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Connection.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Connection.java
@@ -25,6 +25,7 @@ import org.apache.hadoop.hbase.Abortable;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.util.FutureUtils;
 import org.apache.yetus.audience.InterfaceAudience;
 
 /**
@@ -124,7 +125,9 @@ public interface Connection extends Abortable, Closeable {
    *
    * @return a {@link BufferedMutator} for the supplied tableName.
    */
-  BufferedMutator getBufferedMutator(TableName tableName) throws IOException;
+  default BufferedMutator getBufferedMutator(TableName tableName) throws IOException {
+    return getBufferedMutator(new BufferedMutatorParams(tableName));
+  }
 
   /**
    * Retrieve a {@link BufferedMutator} for performing client-side buffering of writes. The
@@ -193,6 +196,14 @@ public interface Connection extends Abortable, Closeable {
    */
   TableBuilder getTableBuilder(TableName tableName, ExecutorService pool);
 
+  /**
+   * Convert this connection to an {@link AsyncConnection}.
+   * <p/>
+   * Usually we will return the same instance if you call this method multiple times so you can
+   * consider this as a light-weighted operation.
+   */
+  AsyncConnection toAsyncConnection();
+
   /**
    * Retrieve an Hbck implementation to fix an HBase cluster.
    * The returned Hbck is not guaranteed to be thread-safe. A new instance should be created by
@@ -207,7 +218,7 @@ public interface Connection extends Abortable, Closeable {
    */
   @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.HBCK)
   default Hbck getHbck() throws IOException {
-    throw new UnsupportedOperationException("Not implemented");
+    return FutureUtils.get(toAsyncConnection().getHbck());
   }
 
   /**
@@ -228,6 +239,6 @@ public interface Connection extends Abortable, Closeable {
    */
   @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.HBCK)
   default Hbck getHbck(ServerName masterServer) throws IOException {
-    throw new UnsupportedOperationException("Not implemented");
+    return toAsyncConnection().getHbck(masterServer);
   }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
index bcb5d1ab50..1945bd0db3 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
@@ -30,6 +30,7 @@ import org.apache.hadoop.hbase.AuthUtil;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.UserProvider;
+import org.apache.hadoop.hbase.util.FutureUtils;
 import org.apache.hadoop.hbase.util.ReflectionUtils;
 import org.apache.yetus.audience.InterfaceAudience;
 
@@ -211,28 +212,33 @@ public class ConnectionFactory {
    * @return Connection object for <code>conf</code>
    */
   public static Connection createConnection(Configuration conf, ExecutorService pool,
-    final User user) throws IOException {
-    String className = conf.get(ConnectionUtils.HBASE_CLIENT_CONNECTION_IMPL,
-      ConnectionImplementation.class.getName());
-    Class<?> clazz;
-    try {
-      clazz = Class.forName(className);
-    } catch (ClassNotFoundException e) {
-      throw new IOException(e);
-    }
-    try {
-      // Default HCM#HCI is not accessible; make it so before invoking.
-      Constructor<?> constructor = clazz.getDeclaredConstructor(Configuration.class,
-        ExecutorService.class, User.class);
-      constructor.setAccessible(true);
-      return user.runAs(
-        (PrivilegedExceptionAction<Connection>)() ->
-          (Connection) constructor.newInstance(conf, pool, user));
-    } catch (Exception e) {
-      throw new IOException(e);
+      final User user) throws IOException {
+    Class<?> clazz = conf.getClass(ConnectionUtils.HBASE_CLIENT_CONNECTION_IMPL,
+      ConnectionOverAsyncConnection.class, Connection.class);
+    if (clazz != ConnectionOverAsyncConnection.class) {
+      try {
+        // Default HCM#HCI is not accessible; make it so before invoking.
+        Constructor<?> constructor =
+          clazz.getDeclaredConstructor(Configuration.class, ExecutorService.class, User.class);
+        constructor.setAccessible(true);
+        return user.runAs((PrivilegedExceptionAction<Connection>) () -> (Connection) constructor
+          .newInstance(conf, pool, user));
+      } catch (Exception e) {
+        throw new IOException(e);
+      }
+    } else {
+      return FutureUtils.get(createAsyncConnection(conf, user)).toConnection();
     }
   }
 
+  /**
+   * Create a {@link ConnectionImplementation}, internal use only.
+   */
+  static ConnectionImplementation createConnectionImpl(Configuration conf, ExecutorService pool,
+      User user) throws IOException {
+    return new ConnectionImplementation(conf, pool, user);
+  }
+
   /**
    * Call {@link #createAsyncConnection(Configuration)} using default HBaseConfiguration.
    * @see #createAsyncConnection(Configuration)
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
index 87ca5e33c4..4d689356cd 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
@@ -41,8 +41,6 @@ import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.hadoop.conf.Configuration;
@@ -76,7 +74,6 @@ import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.ExceptionUtil;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.ReflectionUtils;
-import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.yetus.audience.InterfaceAudience;
@@ -414,11 +411,6 @@ class ConnectionImplementation implements Connection, Closeable {
     }
   }
 
-  @Override
-  public BufferedMutator getBufferedMutator(TableName tableName) {
-    return getBufferedMutator(new BufferedMutatorParams(tableName));
-  }
-
   @Override
   public RegionLocator getRegionLocator(TableName tableName) throws IOException {
     return new HRegionLocator(tableName, this);
@@ -474,30 +466,8 @@ class ConnectionImplementation implements Connection, Closeable {
   private ExecutorService getThreadPool(int maxThreads, int coreThreads, String nameHint,
       BlockingQueue<Runnable> passedWorkQueue) {
     // shared HTable thread executor not yet initialized
-    if (maxThreads == 0) {
-      maxThreads = Runtime.getRuntime().availableProcessors() * 8;
-    }
-    if (coreThreads == 0) {
-      coreThreads = Runtime.getRuntime().availableProcessors() * 8;
-    }
-    long keepAliveTime = conf.getLong("hbase.hconnection.threads.keepalivetime", 60);
-    BlockingQueue<Runnable> workQueue = passedWorkQueue;
-    if (workQueue == null) {
-      workQueue =
-        new LinkedBlockingQueue<>(maxThreads *
-            conf.getInt(HConstants.HBASE_CLIENT_MAX_TOTAL_TASKS,
-                HConstants.DEFAULT_HBASE_CLIENT_MAX_TOTAL_TASKS));
-      coreThreads = maxThreads;
-    }
-    ThreadPoolExecutor tpe = new ThreadPoolExecutor(
-        coreThreads,
-        maxThreads,
-        keepAliveTime,
-        TimeUnit.SECONDS,
-        workQueue,
-        Threads.newDaemonThreadFactory(toString() + nameHint));
-    tpe.allowCoreThreadTimeOut(true);
-    return tpe;
+    return ConnectionUtils.getThreadPool(conf, maxThreads, coreThreads, () -> toString() + nameHint,
+      passedWorkQueue);
   }
 
   private ExecutorService getMetaLookupPool() {
@@ -529,21 +499,10 @@ class ConnectionImplementation implements Connection, Closeable {
 
   private void shutdownPools() {
     if (this.cleanupPool && this.batchPool != null && !this.batchPool.isShutdown()) {
-      shutdownBatchPool(this.batchPool);
+      ConnectionUtils.shutdownPool(this.batchPool);
     }
     if (this.metaLookupPool != null && !this.metaLookupPool.isShutdown()) {
-      shutdownBatchPool(this.metaLookupPool);
-    }
-  }
-
-  private void shutdownBatchPool(ExecutorService pool) {
-    pool.shutdown();
-    try {
-      if (!pool.awaitTermination(10, TimeUnit.SECONDS)) {
-        pool.shutdownNow();
-      }
-    } catch (InterruptedException e) {
-      pool.shutdownNow();
+      ConnectionUtils.shutdownPool(this.metaLookupPool);
     }
   }
 
@@ -2194,4 +2153,9 @@ class ConnectionImplementation implements Connection, Closeable {
       throw new IOException(cause);
     }
   }
+
+  @Override
+  public AsyncConnection toAsyncConnection() {
+    throw new UnsupportedOperationException();
+  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java
new file mode 100644
index 0000000000..61cc708319
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java
@@ -0,0 +1,180 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.io.IOException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.TimeUnit;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.log.HBaseMarkers;
+import org.apache.hadoop.hbase.util.FutureUtils;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
+
+/**
+ * The connection implementation based on {@link AsyncConnection}.
+ */
+@InterfaceAudience.Private
+class ConnectionOverAsyncConnection implements Connection {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ConnectionOverAsyncConnection.class);
+
+  private volatile boolean aborted = false;
+
+  private volatile ExecutorService batchPool = null;
+
+  protected final AsyncConnectionImpl conn;
+
+  /**
+   * @deprecated we can not implement all the related stuffs at once so keep it here for now, will
+   *             remove it after we implement all the stuffs, like Admin, RegionLocator, etc.
+   */
+  @Deprecated
+  private final ConnectionImplementation oldConn;
+
+  private final ConnectionConfiguration connConf;
+
+  ConnectionOverAsyncConnection(AsyncConnectionImpl conn, ConnectionImplementation oldConn) {
+    this.conn = conn;
+    this.oldConn = oldConn;
+    this.connConf = new ConnectionConfiguration(conn.getConfiguration());
+  }
+
+  @Override
+  public void abort(String why, Throwable error) {
+    if (error != null) {
+      LOG.error(HBaseMarkers.FATAL, why, error);
+    } else {
+      LOG.error(HBaseMarkers.FATAL, why);
+    }
+    aborted = true;
+    try {
+      Closeables.close(this, true);
+    } catch (IOException e) {
+      throw new AssertionError(e);
+    }
+  }
+
+  @Override
+  public boolean isAborted() {
+    return aborted;
+  }
+
+  @Override
+  public Configuration getConfiguration() {
+    return conn.getConfiguration();
+  }
+
+  @Override
+  public BufferedMutator getBufferedMutator(BufferedMutatorParams params) throws IOException {
+    return oldConn.getBufferedMutator(params);
+  }
+
+  @Override
+  public RegionLocator getRegionLocator(TableName tableName) throws IOException {
+    return oldConn.getRegionLocator(tableName);
+  }
+
+  @Override
+  public void clearRegionLocationCache() {
+    conn.clearRegionLocationCache();
+  }
+
+  @Override
+  public Admin getAdmin() throws IOException {
+    return oldConn.getAdmin();
+  }
+
+  @Override
+  public void close() throws IOException {
+    conn.close();
+  }
+
+  // will be called from AsyncConnection, to avoid infinite loop as in the above method we will call
+  // AsyncConnection.close.
+  void closeConnImpl() {
+    ExecutorService batchPool = this.batchPool;
+    if (batchPool != null) {
+      ConnectionUtils.shutdownPool(batchPool);
+      this.batchPool = null;
+    }
+  }
+
+  @Override
+  public boolean isClosed() {
+    return conn.isClosed();
+  }
+
+  private ExecutorService getBatchPool() {
+    if (batchPool == null) {
+      synchronized (this) {
+        if (batchPool == null) {
+          int threads = conn.getConfiguration().getInt("hbase.hconnection.threads.max", 256);
+          this.batchPool = ConnectionUtils.getThreadPool(conn.getConfiguration(), threads, threads,
+            () -> toString() + "-shared", null);
+        }
+      }
+    }
+    return this.batchPool;
+  }
+
+  @Override
+  public TableBuilder getTableBuilder(TableName tableName, ExecutorService pool) {
+    return new TableBuilderBase(tableName, connConf) {
+
+      @Override
+      public Table build() {
+        ExecutorService p = pool != null ? pool : getBatchPool();
+        return new TableOverAsyncTable(conn,
+          conn.getTableBuilder(tableName).setRpcTimeout(rpcTimeout, TimeUnit.MILLISECONDS)
+            .setReadRpcTimeout(readRpcTimeout, TimeUnit.MILLISECONDS)
+            .setWriteRpcTimeout(writeRpcTimeout, TimeUnit.MILLISECONDS)
+            .setOperationTimeout(operationTimeout, TimeUnit.MILLISECONDS).build(),
+          p);
+      }
+    };
+  }
+
+  @Override
+  public AsyncConnection toAsyncConnection() {
+    return conn;
+  }
+
+  @Override
+  public Hbck getHbck() throws IOException {
+    return FutureUtils.get(conn.getHbck());
+  }
+
+  @Override
+  public Hbck getHbck(ServerName masterServer) throws IOException {
+    return conn.getHbck(masterServer);
+  }
+
+  /**
+   * An identifier that will remain the same for a given connection.
+   */
+  @Override
+  public String toString() {
+    return "connection-over-async-connection-0x" + Integer.toHexString(hashCode());
+  }
+}
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
index 877ce2bfeb..d2cae49906 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
@@ -28,11 +28,15 @@ import java.net.InetAddress;
 import java.net.UnknownHostException;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.ThreadLocalRandom;
+import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Function;
+import java.util.function.Supplier;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
@@ -45,9 +49,9 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.metrics.ScanMetrics;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
 import org.apache.hadoop.hbase.security.User;
-import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ReflectionUtils;
+import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.net.DNS;
 import org.apache.yetus.audience.InterfaceAudience;
@@ -61,10 +65,8 @@ import org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;
 import org.apache.hbase.thirdparty.io.netty.util.Timer;
 
 import org.apache.hadoop.hbase.shaded.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.AdminService;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.ClientService;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.MasterService;
 
 /**
  * Utility used by client connections.
@@ -134,68 +136,6 @@ public final class ConnectionUtils {
     log.info(sn + " server-side Connection retries=" + retries);
   }
 
-  /**
-   * A ClusterConnection that will short-circuit RPC making direct invocations against the localhost
-   * if the invocation target is 'this' server; save on network and protobuf invocations.
-   */
-  // TODO This has to still do PB marshalling/unmarshalling stuff. Check how/whether we can avoid.
-  @VisibleForTesting // Class is visible so can assert we are short-circuiting when expected.
-  public static class ShortCircuitingClusterConnection extends ConnectionImplementation {
-    private final ServerName serverName;
-    private final AdminService.BlockingInterface localHostAdmin;
-    private final ClientService.BlockingInterface localHostClient;
-
-    private ShortCircuitingClusterConnection(Configuration conf, ExecutorService pool, User user,
-        ServerName serverName, AdminService.BlockingInterface admin,
-        ClientService.BlockingInterface client) throws IOException {
-      super(conf, pool, user);
-      this.serverName = serverName;
-      this.localHostAdmin = admin;
-      this.localHostClient = client;
-    }
-
-    @Override
-    public AdminService.BlockingInterface getAdmin(ServerName sn) throws IOException {
-      return serverName.equals(sn) ? this.localHostAdmin : super.getAdmin(sn);
-    }
-
-    @Override
-    public ClientService.BlockingInterface getClient(ServerName sn) throws IOException {
-      return serverName.equals(sn) ? this.localHostClient : super.getClient(sn);
-    }
-
-    @Override
-    public MasterKeepAliveConnection getMaster() throws IOException {
-      if (this.localHostClient instanceof MasterService.BlockingInterface) {
-        return new ShortCircuitMasterConnection(
-          (MasterService.BlockingInterface) this.localHostClient);
-      }
-      return super.getMaster();
-    }
-  }
-
-  /**
-   * Creates a short-circuit connection that can bypass the RPC layer (serialization,
-   * deserialization, networking, etc..) when talking to a local server.
-   * @param conf the current configuration
-   * @param pool the thread pool to use for batch operations
-   * @param user the user the connection is for
-   * @param serverName the local server name
-   * @param admin the admin interface of the local server
-   * @param client the client interface of the local server
-   * @return an short-circuit connection.
-   * @throws IOException if IO failure occurred
-   */
-  public static ConnectionImplementation createShortCircuitConnection(final Configuration conf,
-      ExecutorService pool, User user, final ServerName serverName,
-      final AdminService.BlockingInterface admin, final ClientService.BlockingInterface client)
-      throws IOException {
-    if (user == null) {
-      user = UserProvider.instantiate(conf).getCurrent();
-    }
-    return new ShortCircuitingClusterConnection(conf, pool, user, serverName, admin, client);
-  }
-
   /**
    * Setup the connection class, so that it will not depend on master being online. Used for testing
    * @param conf configuration to set
@@ -664,4 +604,38 @@ public final class ConnectionUtils {
     }
     return future;
   }
+
+  static ExecutorService getThreadPool(Configuration conf, int maxThreads, int coreThreads,
+      Supplier<String> threadName, BlockingQueue<Runnable> passedWorkQueue) {
+    // shared HTable thread executor not yet initialized
+    if (maxThreads == 0) {
+      maxThreads = Runtime.getRuntime().availableProcessors() * 8;
+    }
+    if (coreThreads == 0) {
+      coreThreads = Runtime.getRuntime().availableProcessors() * 8;
+    }
+    long keepAliveTime = conf.getLong("hbase.hconnection.threads.keepalivetime", 60);
+    BlockingQueue<Runnable> workQueue = passedWorkQueue;
+    if (workQueue == null) {
+      workQueue =
+        new LinkedBlockingQueue<>(maxThreads * conf.getInt(HConstants.HBASE_CLIENT_MAX_TOTAL_TASKS,
+          HConstants.DEFAULT_HBASE_CLIENT_MAX_TOTAL_TASKS));
+      coreThreads = maxThreads;
+    }
+    ThreadPoolExecutor tpe = new ThreadPoolExecutor(coreThreads, maxThreads, keepAliveTime,
+      TimeUnit.SECONDS, workQueue, Threads.newDaemonThreadFactory(threadName.get()));
+    tpe.allowCoreThreadTimeOut(true);
+    return tpe;
+  }
+
+  static void shutdownPool(ExecutorService pool) {
+    pool.shutdown();
+    try {
+      if (!pool.awaitTermination(10, TimeUnit.SECONDS)) {
+        pool.shutdownNow();
+      }
+    } catch (InterruptedException e) {
+      pool.shutdownNow();
+    }
+  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index 2cf751458b..010b1d9f66 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -89,6 +89,7 @@ import org.apache.hadoop.hbase.replication.ReplicationException;
 import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
 import org.apache.hadoop.hbase.replication.ReplicationPeerDescription;
 import org.apache.hadoop.hbase.replication.SyncReplicationState;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil;
 import org.apache.hadoop.hbase.security.access.UserPermission;
 import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
@@ -2423,8 +2424,8 @@ public class HBaseAdmin implements Admin {
 
     // Check ZK first.
     // If the connection exists, we may have a connection to ZK that does not work anymore
-    try (ConnectionImplementation connection =
-      (ConnectionImplementation) ConnectionFactory.createConnection(copyOfConf)) {
+    try (ConnectionImplementation connection = ConnectionFactory.createConnectionImpl(copyOfConf,
+      null, UserProvider.instantiate(copyOfConf).getCurrent())) {
       // can throw MasterNotRunningException
       connection.isMasterRunning();
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
index bae20c5764..6ec6df60af 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
@@ -18,6 +18,8 @@
  */
 package org.apache.hadoop.hbase.client;
 
+import static org.apache.hadoop.hbase.client.ConnectionUtils.checkHasFamilies;
+
 // DO NOT MAKE USE OF THESE IMPORTS! THEY ARE HERE FOR COPROCESSOR ENDPOINTS ONLY.
 // Internally, we use shaded protobuf. This below are part of our public API.
 //SEE ABOVE NOTE!
@@ -25,26 +27,43 @@ import com.google.protobuf.Descriptors;
 import com.google.protobuf.Message;
 import com.google.protobuf.Service;
 import com.google.protobuf.ServiceException;
-
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.SynchronousQueue;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.CompareOperator;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionLocation;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.io.TimeRange;
-import org.apache.yetus.audience.InterfaceAudience;
-import org.apache.yetus.audience.InterfaceStability;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
 import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
-import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
+import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
 import org.apache.hadoop.hbase.ipc.RpcControllerFactory;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.ReflectionUtils;
+import org.apache.hadoop.hbase.util.Threads;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.apache.yetus.audience.InterfaceStability;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
 import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;
+
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.shaded.protobuf.ResponseConverter;
@@ -54,27 +73,6 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.MutateRequ
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.RegionAction;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.CompareType;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.util.ReflectionUtils;
-import org.apache.hadoop.hbase.util.Threads;
-
-import java.io.IOException;
-import java.io.InterruptedIOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeMap;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-import java.util.concurrent.SynchronousQueue;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-
-import static org.apache.hadoop.hbase.client.ConnectionUtils.checkHasFamilies;
 
 /**
  * An implementation of {@link Table}. Used to communicate with a single HBase table.
@@ -101,7 +99,7 @@ import static org.apache.hadoop.hbase.client.ConnectionUtils.checkHasFamilies;
  */
 @InterfaceAudience.Private
 @InterfaceStability.Stable
-public class HTable implements Table {
+class HTable implements Table {
   private static final Logger LOG = LoggerFactory.getLogger(HTable.class);
   private static final Consistency DEFAULT_CONSISTENCY = Consistency.STRONG;
   private final ConnectionImplementation connection;
@@ -222,17 +220,6 @@ public class HTable implements Table {
     return this.connection;
   }
 
-  @Override
-  @Deprecated
-  public HTableDescriptor getTableDescriptor() throws IOException {
-    HTableDescriptor htd = HBaseAdmin.getHTableDescriptor(tableName, connection, rpcCallerFactory,
-      rpcControllerFactory, operationTimeoutMs, readRpcTimeoutMs);
-    if (htd != null) {
-      return new ImmutableHTableDescriptor(htd);
-    }
-    return null;
-  }
-
   @Override
   public TableDescriptor getDescriptor() throws IOException {
     return HBaseAdmin.getTableDescriptor(tableName, connection, rpcCallerFactory,
@@ -667,29 +654,6 @@ public class HTable implements Table {
         callWithRetries(callable, this.operationTimeoutMs);
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndPut(final byte [] row, final byte [] family, final byte [] qualifier,
-      final byte [] value, final Put put) throws IOException {
-    return doCheckAndPut(row, family, qualifier, CompareOperator.EQUAL.name(), value, null, put);
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndPut(final byte [] row, final byte [] family, final byte [] qualifier,
-      final CompareOp compareOp, final byte [] value, final Put put) throws IOException {
-    return doCheckAndPut(row, family, qualifier, compareOp.name(), value, null, put);
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndPut(final byte [] row, final byte [] family, final byte [] qualifier,
-      final CompareOperator op, final byte [] value, final Put put) throws IOException {
-    // The name of the operators in CompareOperator are intentionally those of the
-    // operators in the filter's CompareOp enum.
-    return doCheckAndPut(row, family, qualifier, op.name(), value, null, put);
-  }
-
   private boolean doCheckAndPut(final byte[] row, final byte[] family, final byte[] qualifier,
     final String opName, final byte[] value, final TimeRange timeRange, final Put put)
     throws IOException {
@@ -710,28 +674,6 @@ public class HTable implements Table {
         .callWithRetries(callable, this.operationTimeoutMs);
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(final byte[] row, final byte[] family, final byte[] qualifier,
-    final byte[] value, final Delete delete) throws IOException {
-    return doCheckAndDelete(row, family, qualifier, CompareOperator.EQUAL.name(), value, null,
-      delete);
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(final byte[] row, final byte[] family, final byte[] qualifier,
-    final CompareOp compareOp, final byte[] value, final Delete delete) throws IOException {
-    return doCheckAndDelete(row, family, qualifier, compareOp.name(), value, null, delete);
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(final byte[] row, final byte[] family, final byte[] qualifier,
-    final CompareOperator op, final byte[] value, final Delete delete) throws IOException {
-    return doCheckAndDelete(row, family, qualifier, op.name(), value, null, delete);
-  }
-
   private boolean doCheckAndDelete(final byte[] row, final byte[] family, final byte[] qualifier,
     final String opName, final byte[] value, final TimeRange timeRange, final Delete delete)
     throws IOException {
@@ -823,21 +765,6 @@ public class HTable implements Table {
     return ((Result)results[0]).getExists();
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(final byte [] row, final byte [] family, final byte [] qualifier,
-    final CompareOp compareOp, final byte [] value, final RowMutations rm)
-  throws IOException {
-    return doCheckAndMutate(row, family, qualifier, compareOp.name(), value, null, rm);
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(final byte [] row, final byte [] family, final byte [] qualifier,
-      final CompareOperator op, final byte [] value, final RowMutations rm) throws IOException {
-    return doCheckAndMutate(row, family, qualifier, op.name(), value, null, rm);
-  }
-
   @Override
   public boolean exists(final Get get) throws IOException {
     Result r = get(get, true);
@@ -939,23 +866,6 @@ public class HTable implements Table {
     return new RegionCoprocessorRpcChannel(connection, tableName, row);
   }
 
-  @Override
-  public <T extends Service, R> Map<byte[],R> coprocessorService(final Class<T> service,
-      byte[] startKey, byte[] endKey, final Batch.Call<T,R> callable)
-      throws ServiceException, Throwable {
-    final Map<byte[],R> results =  Collections.synchronizedMap(
-        new TreeMap<byte[], R>(Bytes.BYTES_COMPARATOR));
-    coprocessorService(service, startKey, endKey, callable, new Batch.Callback<R>() {
-      @Override
-      public void update(byte[] region, byte[] row, R value) {
-        if (region != null) {
-          results.put(region, value);
-        }
-      }
-    });
-    return results;
-  }
-
   @Override
   public <T extends Service, R> void coprocessorService(final Class<T> service,
       byte[] startKey, byte[] endKey, final Batch.Call<T,R> callable,
@@ -1011,93 +921,26 @@ public class HTable implements Table {
     return unit.convert(rpcTimeoutMs, TimeUnit.MILLISECONDS);
   }
 
-  @Override
-  @Deprecated
-  public int getRpcTimeout() {
-    return rpcTimeoutMs;
-  }
-
-  @Override
-  @Deprecated
-  public void setRpcTimeout(int rpcTimeout) {
-    setReadRpcTimeout(rpcTimeout);
-    setWriteRpcTimeout(rpcTimeout);
-  }
-
   @Override
   public long getReadRpcTimeout(TimeUnit unit) {
     return unit.convert(readRpcTimeoutMs, TimeUnit.MILLISECONDS);
   }
 
-  @Override
-  @Deprecated
-  public int getReadRpcTimeout() {
-    return readRpcTimeoutMs;
-  }
-
-  @Override
-  @Deprecated
-  public void setReadRpcTimeout(int readRpcTimeout) {
-    this.readRpcTimeoutMs = readRpcTimeout;
-  }
-
   @Override
   public long getWriteRpcTimeout(TimeUnit unit) {
     return unit.convert(writeRpcTimeoutMs, TimeUnit.MILLISECONDS);
   }
 
-  @Override
-  @Deprecated
-  public int getWriteRpcTimeout() {
-    return writeRpcTimeoutMs;
-  }
-
-  @Override
-  @Deprecated
-  public void setWriteRpcTimeout(int writeRpcTimeout) {
-    this.writeRpcTimeoutMs = writeRpcTimeout;
-  }
-
   @Override
   public long getOperationTimeout(TimeUnit unit) {
     return unit.convert(operationTimeoutMs, TimeUnit.MILLISECONDS);
   }
 
-  @Override
-  @Deprecated
-  public int getOperationTimeout() {
-    return operationTimeoutMs;
-  }
-
-  @Override
-  @Deprecated
-  public void setOperationTimeout(int operationTimeout) {
-    this.operationTimeoutMs = operationTimeout;
-  }
-
   @Override
   public String toString() {
     return tableName + ";" + connection;
   }
 
-  @Override
-  public <R extends Message> Map<byte[], R> batchCoprocessorService(
-      Descriptors.MethodDescriptor methodDescriptor, Message request,
-      byte[] startKey, byte[] endKey, R responsePrototype) throws ServiceException, Throwable {
-    final Map<byte[], R> results = Collections.synchronizedMap(new TreeMap<byte[], R>(
-        Bytes.BYTES_COMPARATOR));
-    batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype,
-        new Callback<R>() {
-      @Override
-      public void update(byte[] region, byte[] row, R result) {
-        if (region != null) {
-          results.put(region, result);
-        }
-      }
-    });
-    return results;
-  }
-
   @Override
   public <R extends Message> void batchCoprocessorService(
       final Descriptors.MethodDescriptor methodDescriptor, final Message request,
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionCoprocessorRpcChannelImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionCoprocessorRpcChannelImpl.java
index 94e7d9a019..3c25c57347 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionCoprocessorRpcChannelImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionCoprocessorRpcChannelImpl.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
+import org.apache.hadoop.hbase.ipc.ServerRpcController;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.yetus.audience.InterfaceAudience;
 
@@ -57,6 +58,8 @@ class RegionCoprocessorRpcChannelImpl implements RpcChannel {
 
   private final long operationTimeoutNs;
 
+  private byte[] lastRegion;
+
   RegionCoprocessorRpcChannelImpl(AsyncConnectionImpl conn, TableName tableName, RegionInfo region,
       byte[] row, long rpcTimeoutNs, long operationTimeoutNs) {
     this.conn = conn;
@@ -71,15 +74,13 @@ class RegionCoprocessorRpcChannelImpl implements RpcChannel {
       Message responsePrototype, HBaseRpcController controller, HRegionLocation loc,
       ClientService.Interface stub) {
     CompletableFuture<Message> future = new CompletableFuture<>();
-    if (region != null
-        && !Bytes.equals(loc.getRegionInfo().getRegionName(), region.getRegionName())) {
-      future.completeExceptionally(new DoNotRetryIOException(
-          "Region name is changed, expected " + region.getRegionNameAsString() + ", actual "
-              + loc.getRegionInfo().getRegionNameAsString()));
+    if (region != null && !Bytes.equals(loc.getRegion().getRegionName(), region.getRegionName())) {
+      future.completeExceptionally(new DoNotRetryIOException("Region name is changed, expected " +
+        region.getRegionNameAsString() + ", actual " + loc.getRegion().getRegionNameAsString()));
       return future;
     }
     CoprocessorServiceRequest csr = CoprocessorRpcUtils.getCoprocessorServiceRequest(method,
-      request, row, loc.getRegionInfo().getRegionName());
+      request, row, loc.getRegion().getRegionName());
     stub.execService(controller, csr,
       new org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<CoprocessorServiceResponse>() {
 
@@ -88,6 +89,7 @@ class RegionCoprocessorRpcChannelImpl implements RpcChannel {
           if (controller.failed()) {
             future.completeExceptionally(controller.getFailed());
           } else {
+            lastRegion = resp.getRegion().getValue().toByteArray();
             try {
               future.complete(CoprocessorRpcUtils.getResponse(resp, responsePrototype));
             } catch (IOException e) {
@@ -99,6 +101,23 @@ class RegionCoprocessorRpcChannelImpl implements RpcChannel {
     return future;
   }
 
+  protected final void setError(RpcController controller, Throwable error) {
+    if (controller == null) {
+      return;
+    }
+    if (controller instanceof ServerRpcController) {
+      if (error instanceof IOException) {
+        ((ServerRpcController) controller).setFailedOn((IOException) error);
+      } else {
+        ((ServerRpcController) controller).setFailedOn(new IOException(error));
+      }
+    } else if (controller instanceof ClientCoprocessorRpcController) {
+      ((ClientCoprocessorRpcController) controller).setFailed(error);
+    } else {
+      controller.setFailed(error.toString());
+    }
+  }
+
   @Override
   public void callMethod(MethodDescriptor method, RpcController controller, Message request,
       Message responsePrototype, RpcCallback<Message> done) {
@@ -109,9 +128,13 @@ class RegionCoprocessorRpcChannelImpl implements RpcChannel {
         .action((c, l, s) -> rpcCall(method, request, responsePrototype, c, l, s)).call(),
       (r, e) -> {
         if (e != null) {
-          ((ClientCoprocessorRpcController) controller).setFailed(e);
+          setError(controller, e);
         }
         done.run(r);
       });
   }
+
+  public byte[] getLastRegion() {
+    return lastRegion;
+  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java
index 6908424781..5da179dc99 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java
@@ -18,30 +18,28 @@
  */
 package org.apache.hadoop.hbase.client;
 
+import com.google.protobuf.Descriptors;
+import com.google.protobuf.Message;
+import com.google.protobuf.Service;
+import com.google.protobuf.ServiceException;
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
+import java.util.TreeMap;
 import java.util.concurrent.TimeUnit;
-
 import org.apache.commons.lang3.NotImplementedException;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CompareOperator;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.io.TimeRange;
-import org.apache.yetus.audience.InterfaceAudience;
-
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
-import org.apache.hadoop.hbase.filter.CompareFilter;
+import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
+import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
 import org.apache.hadoop.hbase.util.Bytes;
-import com.google.protobuf.Descriptors;
-import com.google.protobuf.Message;
-import com.google.protobuf.Service;
-import com.google.protobuf.ServiceException;
+import org.apache.yetus.audience.InterfaceAudience;
 
 /**
  * Used to communicate with a single HBase table.
@@ -69,23 +67,6 @@ public interface Table extends Closeable {
    */
   Configuration getConfiguration();
 
-  /**
-   * Gets the {@link org.apache.hadoop.hbase.HTableDescriptor table descriptor} for this table.
-   * @throws java.io.IOException if a remote or network exception occurs.
-   * @deprecated since 2.0 version and will be removed in 3.0 version.
-   *             use {@link #getDescriptor()}
-   */
-  @Deprecated
-  default HTableDescriptor getTableDescriptor() throws IOException {
-    TableDescriptor descriptor = getDescriptor();
-
-    if (descriptor instanceof HTableDescriptor) {
-      return (HTableDescriptor)descriptor;
-    } else {
-      return new HTableDescriptor(descriptor);
-    }
-  }
-
   /**
    * Gets the {@link org.apache.hadoop.hbase.client.TableDescriptor table descriptor} for this table.
    * @throws java.io.IOException if a remote or network exception occurs.
@@ -129,24 +110,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Test for the existence of columns in the table, as specified by the Gets.
-   * This will return an array of booleans. Each value will be true if the related Get matches
-   * one or more keys, false if not.
-   * This is a server-side call so it prevents any data from being transferred to
-   * the client.
-   *
-   * @param gets the Gets
-   * @return Array of boolean.  True if the specified Get matches one or more keys, false if not.
-   * @throws IOException e
-   * @deprecated since 2.0 version and will be removed in 3.0 version.
-   *             use {@link #exists(List)}
-   */
-  @Deprecated
-  default boolean[] existsAll(List<Get> gets) throws IOException {
-    return exists(gets);
-  }
-
   /**
    * Method that does a batch call on Deletes, Gets, Puts, Increments, Appends, RowMutations.
    * The ordering of execution of the actions is not defined. Meaning if you do a Put and a
@@ -169,10 +132,14 @@ public interface Table extends Closeable {
   /**
    * Same as {@link #batch(List, Object[])}, but with a callback.
    * @since 0.96.0
+   * @deprecated Please use the batch related methods in {@link AsyncTable} directly if you want to
+   *             use callback. We reuse the callback for coprocessor here, and the problem is that
+   *             for batch operation, the {@link AsyncTable} does not tell us the region, so in this
+   *             method we need an extra locating after we get the result, which is not good.
    */
-  default <R> void batchCallback(
-    final List<? extends Row> actions, final Object[] results, final Batch.Callback<R> callback)
-      throws IOException, InterruptedException {
+  @Deprecated
+  default <R> void batchCallback(final List<? extends Row> actions, final Object[] results,
+      final Batch.Callback<R> callback) throws IOException, InterruptedException {
     throw new NotImplementedException("Add an implementation!");
   }
 
@@ -282,84 +249,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the put.  If the passed value is null, the check
-   * is for the lack of column (ie: non-existance)
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param value the expected value
-   * @param put data to put if check succeeds
-   * @throws IOException e
-   * @return true if the new put was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier, byte[] value, Put put)
-      throws IOException {
-    return checkAndPut(row, family, qualifier, CompareOperator.EQUAL, value, put);
-  }
-
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the put.  If the passed value is null, the check
-   * is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> add the put.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param compareOp comparison operator to use
-   * @param value the expected value
-   * @param put data to put if check succeeds
-   * @throws IOException e
-   * @return true if the new put was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      CompareFilter.CompareOp compareOp, byte[] value, Put put) throws IOException {
-    RowMutations mutations = new RowMutations(put.getRow(), 1);
-    mutations.add(put);
-
-    return checkAndMutate(row, family, qualifier, compareOp, value, mutations);
-  }
-
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the put.  If the passed value is null, the check
-   * is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> add the put.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param op comparison operator to use
-   * @param value the expected value
-   * @param put data to put if check succeeds
-   * @throws IOException e
-   * @return true if the new put was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier, CompareOperator op,
-      byte[] value, Put put) throws IOException {
-    RowMutations mutations = new RowMutations(put.getRow(), 1);
-    mutations.add(put);
-
-    return checkAndMutate(row, family, qualifier, op, value, mutations);
-  }
-
   /**
    * Deletes the specified cells/row.
    *
@@ -398,84 +287,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the delete.  If the passed value is null, the
-   * check is for the lack of column (ie: non-existance)
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param value the expected value
-   * @param delete data to delete if check succeeds
-   * @throws IOException e
-   * @return true if the new delete was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-    byte[] value, Delete delete) throws IOException {
-    return checkAndDelete(row, family, qualifier, CompareOperator.EQUAL, value, delete);
-  }
-
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the delete.  If the passed value is null, the
-   * check is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> add the delete.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param compareOp comparison operator to use
-   * @param value the expected value
-   * @param delete data to delete if check succeeds
-   * @throws IOException e
-   * @return true if the new delete was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-    CompareFilter.CompareOp compareOp, byte[] value, Delete delete) throws IOException {
-    RowMutations mutations = new RowMutations(delete.getRow(), 1);
-    mutations.add(delete);
-
-    return checkAndMutate(row, family, qualifier, compareOp, value, mutations);
-  }
-
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected
-   * value. If it does, it adds the delete.  If the passed value is null, the
-   * check is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> add the delete.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param op comparison operator to use
-   * @param value the expected value
-   * @param delete data to delete if check succeeds
-   * @throws IOException e
-   * @return true if the new delete was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-                         CompareOperator op, byte[] value, Delete delete) throws IOException {
-    RowMutations mutations = new RowMutations(delete.getRow(), 1);
-    mutations.add(delete);
-
-    return checkAndMutate(row, family, qualifier, op, value, mutations);
-  }
-
   /**
    * Atomically checks if a row/family/qualifier value matches the expected value. If it does, it
    * adds the Put/Delete/RowMutations.
@@ -643,32 +454,35 @@ public interface Table extends Closeable {
   }
 
   /**
-   * Creates and returns a {@link com.google.protobuf.RpcChannel} instance connected to the
-   * table region containing the specified row.  The row given does not actually have
-   * to exist.  Whichever region would contain the row based on start and end keys will
-   * be used.  Note that the {@code row} parameter is also not passed to the
-   * coprocessor handler registered for this protocol, unless the {@code row}
-   * is separately passed as an argument in the service request.  The parameter
-   * here is only used to locate the region used to handle the call.
-   *
+   * Creates and returns a {@link com.google.protobuf.RpcChannel} instance connected to the table
+   * region containing the specified row. The row given does not actually have to exist. Whichever
+   * region would contain the row based on start and end keys will be used. Note that the
+   * {@code row} parameter is also not passed to the coprocessor handler registered for this
+   * protocol, unless the {@code row} is separately passed as an argument in the service request.
+   * The parameter here is only used to locate the region used to handle the call.
    * <p>
    * The obtained {@link com.google.protobuf.RpcChannel} instance can be used to access a published
    * coprocessor {@link com.google.protobuf.Service} using standard protobuf service invocations:
    * </p>
+   * <div style="background-color: #cccccc; padding: 2px"> <blockquote>
    *
-   * <div style="background-color: #cccccc; padding: 2px">
-   * <blockquote><pre>
+   * <pre>
    * CoprocessorRpcChannel channel = myTable.coprocessorService(rowkey);
    * MyService.BlockingInterface service = MyService.newBlockingStub(channel);
    * MyCallRequest request = MyCallRequest.newBuilder()
    *     ...
    *     .build();
    * MyCallResponse response = service.myCall(null, request);
-   * </pre></blockquote></div>
+   * </pre>
    *
+   * </blockquote></div>
    * @param row The row key used to identify the remote region location
    * @return A CoprocessorRpcChannel instance
+   * @deprecated This is too low level, please stop using it any more. Use the coprocessorService
+   *             methods in {@link AsyncTable} instead.
+   * @see Connection#toAsyncConnection()
    */
+  @Deprecated
   default CoprocessorRpcChannel coprocessorService(byte[] row) {
     throw new NotImplementedException("Add an implementation!");
   }
@@ -678,25 +492,40 @@ public interface Table extends Closeable {
    * region spanning the range from the {@code startKey} row to {@code endKey} row (inclusive), and
    * invokes the passed {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
    * with each {@link com.google.protobuf.Service} instance.
-   *
    * @param service the protocol buffer {@code Service} implementation to call
-   * @param startKey start region selection with region containing this row.  If {@code null}, the
-   *   selection will start with the first table region.
+   * @param startKey start region selection with region containing this row. If {@code null}, the
+   *          selection will start with the first table region.
    * @param endKey select regions up to and including the region containing this row. If
-   *   {@code null}, selection will continue through the last table region.
+   *          {@code null}, selection will continue through the last table region.
    * @param callable this instance's
-   *   {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call}
-   *   method will be invoked once per table region, using the {@link com.google.protobuf.Service}
-   *   instance connected to that region.
+   *          {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method will be
+   *          invoked once per table region, using the {@link com.google.protobuf.Service} instance
+   *          connected to that region.
    * @param <T> the {@link com.google.protobuf.Service} subclass to connect to
-   * @param <R> Return type for the {@code callable} parameter's {@link
-   * org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
+   * @param <R> Return type for the {@code callable} parameter's
+   *          {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
    * @return a map of result values keyed by region name
+   * @deprecated The batch call here references the blocking interface for of a protobuf stub, so it
+   *             is not possible to do it in an asynchronous way, even if now we are building the
+   *             {@link Table} implementation based on the {@link AsyncTable}, which is not good.
+   *             Use the coprocessorService methods in {@link AsyncTable} directly instead.
+   * @see Connection#toAsyncConnection()
    */
-  default <T extends Service, R> Map<byte[],R> coprocessorService(final Class<T> service,
-    byte[] startKey, byte[] endKey, final Batch.Call<T,R> callable)
-    throws ServiceException, Throwable {
-    throw new NotImplementedException("Add an implementation!");
+  @Deprecated
+  default <T extends Service, R> Map<byte[], R> coprocessorService(final Class<T> service,
+      byte[] startKey, byte[] endKey, final Batch.Call<T, R> callable)
+      throws ServiceException, Throwable {
+    Map<byte[], R> results =
+      Collections.synchronizedMap(new TreeMap<byte[], R>(Bytes.BYTES_COMPARATOR));
+    coprocessorService(service, startKey, endKey, callable, new Batch.Callback<R>() {
+      @Override
+      public void update(byte[] region, byte[] row, R value) {
+        if (region != null) {
+          results.put(region, value);
+        }
+      }
+    });
+    return results;
   }
 
   /**
@@ -704,28 +533,34 @@ public interface Table extends Closeable {
    * region spanning the range from the {@code startKey} row to {@code endKey} row (inclusive), and
    * invokes the passed {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
    * with each {@link Service} instance.
-   *
-   * <p> The given
+   * <p>
+   * The given
    * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Callback#update(byte[],byte[],Object)}
    * method will be called with the return value from each region's
-   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} invocation. </p>
-   *
+   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} invocation.
+   * </p>
    * @param service the protocol buffer {@code Service} implementation to call
-   * @param startKey start region selection with region containing this row.  If {@code null}, the
-   *   selection will start with the first table region.
+   * @param startKey start region selection with region containing this row. If {@code null}, the
+   *          selection will start with the first table region.
    * @param endKey select regions up to and including the region containing this row. If
-   *   {@code null}, selection will continue through the last table region.
+   *          {@code null}, selection will continue through the last table region.
    * @param callable this instance's
-   *   {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call}
-   *   method will be invoked once per table region, using the {@link Service} instance connected to
-   *   that region.
+   *          {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method will be
+   *          invoked once per table region, using the {@link Service} instance connected to that
+   *          region.
    * @param <T> the {@link Service} subclass to connect to
-   * @param <R> Return type for the {@code callable} parameter's {@link
-   * org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
+   * @param <R> Return type for the {@code callable} parameter's
+   *          {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
+   * @deprecated The batch call here references the blocking interface for of a protobuf stub, so it
+   *             is not possible to do it in an asynchronous way, even if now we are building the
+   *             {@link Table} implementation based on the {@link AsyncTable}, which is not good.
+   *             Use the coprocessorService methods in {@link AsyncTable} directly instead.
+   * @see Connection#toAsyncConnection()
    */
-  default <T extends Service, R> void coprocessorService(final Class<T> service,
-    byte[] startKey, byte[] endKey, final Batch.Call<T,R> callable,
-    final Batch.Callback<R> callback) throws ServiceException, Throwable {
+  @Deprecated
+  default <T extends Service, R> void coprocessorService(final Class<T> service, byte[] startKey,
+      byte[] endKey, final Batch.Call<T, R> callable, final Batch.Callback<R> callback)
+      throws ServiceException, Throwable {
     throw new NotImplementedException("Add an implementation!");
   }
 
@@ -734,27 +569,37 @@ public interface Table extends Closeable {
    * region spanning the range from the {@code startKey} row to {@code endKey} row (inclusive), all
    * the invocations to the same region server will be batched into one call. The coprocessor
    * service is invoked according to the service instance, method name and parameters.
-   *
-   * @param methodDescriptor
-   *          the descriptor for the protobuf service method to call.
-   * @param request
-   *          the method call parameters
-   * @param startKey
-   *          start region selection with region containing this row. If {@code null}, the
+   * @param methodDescriptor the descriptor for the protobuf service method to call.
+   * @param request the method call parameters
+   * @param startKey start region selection with region containing this row. If {@code null}, the
    *          selection will start with the first table region.
-   * @param endKey
-   *          select regions up to and including the region containing this row. If {@code null},
-   *          selection will continue through the last table region.
-   * @param responsePrototype
-   *          the proto type of the response of the method in Service.
-   * @param <R>
-   *          the response type for the coprocessor Service method
+   * @param endKey select regions up to and including the region containing this row. If
+   *          {@code null}, selection will continue through the last table region.
+   * @param responsePrototype the proto type of the response of the method in Service.
+   * @param <R> the response type for the coprocessor Service method
    * @return a map of result values keyed by region name
+   * @deprecated The batch call here references the blocking interface for of a protobuf stub, so it
+   *             is not possible to do it in an asynchronous way, even if now we are building the
+   *             {@link Table} implementation based on the {@link AsyncTable}, which is not good.
+   *             Use the coprocessorService methods in {@link AsyncTable} directly instead.
+   * @see Connection#toAsyncConnection()
    */
+  @Deprecated
   default <R extends Message> Map<byte[], R> batchCoprocessorService(
-    Descriptors.MethodDescriptor methodDescriptor, Message request,
-    byte[] startKey, byte[] endKey, R responsePrototype) throws ServiceException, Throwable {
-    throw new NotImplementedException("Add an implementation!");
+      Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey,
+      byte[] endKey, R responsePrototype) throws ServiceException, Throwable {
+    final Map<byte[], R> results =
+      Collections.synchronizedMap(new TreeMap<byte[], R>(Bytes.BYTES_COMPARATOR));
+    batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype,
+      new Callback<R>() {
+        @Override
+        public void update(byte[] region, byte[] row, R result) {
+          if (region != null) {
+            results.put(region, result);
+          }
+        }
+      });
+    return results;
   }
 
   /**
@@ -762,24 +607,27 @@ public interface Table extends Closeable {
    * region spanning the range from the {@code startKey} row to {@code endKey} row (inclusive), all
    * the invocations to the same region server will be batched into one call. The coprocessor
    * service is invoked according to the service instance, method name and parameters.
-   *
    * <p>
    * The given
    * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Callback#update(byte[],byte[],Object)}
    * method will be called with the return value from each region's invocation.
    * </p>
-   *
    * @param methodDescriptor the descriptor for the protobuf service method to call.
    * @param request the method call parameters
-   * @param startKey start region selection with region containing this row.
-   *   If {@code null}, the selection will start with the first table region.
-   * @param endKey select regions up to and including the region containing this row.
-   *   If {@code null}, selection will continue through the last table region.
+   * @param startKey start region selection with region containing this row. If {@code null}, the
+   *          selection will start with the first table region.
+   * @param endKey select regions up to and including the region containing this row. If
+   *          {@code null}, selection will continue through the last table region.
    * @param responsePrototype the proto type of the response of the method in Service.
    * @param callback callback to invoke with the response for each region
-   * @param <R>
-   *          the response type for the coprocessor Service method
+   * @param <R> the response type for the coprocessor Service method
+   * @deprecated The batch call here references the blocking interface for of a protobuf stub, so it
+   *             is not possible to do it in an asynchronous way, even if now we are building the
+   *             {@link Table} implementation based on the {@link AsyncTable}, which is not good.
+   *             Use the coprocessorService methods in {@link AsyncTable} directly instead.
+   * @see Connection#toAsyncConnection()
    */
+  @Deprecated
   default <R extends Message> void batchCoprocessorService(
       Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey,
       byte[] endKey, R responsePrototype, Batch.Callback<R> callback)
@@ -787,58 +635,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected value.
-   * If it does, it performs the row mutations.  If the passed value is null, the check
-   * is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> perform row mutations.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param compareOp the comparison operator
-   * @param value the expected value
-   * @param mutation  mutations to perform if check succeeds
-   * @throws IOException e
-   * @return true if the new put was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier,
-      CompareFilter.CompareOp compareOp, byte[] value, RowMutations mutation) throws IOException {
-    throw new NotImplementedException("Add an implementation!");
-  }
-
-  /**
-   * Atomically checks if a row/family/qualifier value matches the expected value.
-   * If it does, it performs the row mutations.  If the passed value is null, the check
-   * is for the lack of column (ie: non-existence)
-   *
-   * The expected value argument of this call is on the left and the current
-   * value of the cell is on the right side of the comparison operator.
-   *
-   * Ie. eg. GREATER operator means expected value > existing <=> perform row mutations.
-   *
-   * @param row to check
-   * @param family column family to check
-   * @param qualifier column qualifier to check
-   * @param op the comparison operator
-   * @param value the expected value
-   * @param mutation  mutations to perform if check succeeds
-   * @throws IOException e
-   * @return true if the new put was executed, false otherwise
-   * @deprecated Since 2.0.0. Will be removed in 3.0.0. Use {@link #checkAndMutate(byte[], byte[])}
-   */
-  @Deprecated
-  default boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier, CompareOperator op,
-                         byte[] value, RowMutations mutation) throws IOException {
-    throw new NotImplementedException("Add an implementation!");
-  }
-
   /**
    * Get timeout of each rpc request in this Table instance. It will be overridden by a more
    * specific rpc timeout config such as readRpcTimeout or writeRpcTimeout.
@@ -851,36 +647,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Get timeout (millisecond) of each rpc request in this Table instance.
-   *
-   * @return Currently configured read timeout
-   * @deprecated use {@link #getReadRpcTimeout(TimeUnit)} or
-   *             {@link #getWriteRpcTimeout(TimeUnit)} instead
-   */
-  @Deprecated
-  default int getRpcTimeout() {
-    return (int)getRpcTimeout(TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Set timeout (millisecond) of each rpc request in operations of this Table instance, will
-   * override the value of hbase.rpc.timeout in configuration.
-   * If a rpc request waiting too long, it will stop waiting and send a new request to retry until
-   * retries exhausted or operation timeout reached.
-   * <p>
-   * NOTE: This will set both the read and write timeout settings to the provided value.
-   *
-   * @param rpcTimeout the timeout of each rpc request in millisecond.
-   *
-   * @deprecated Use setReadRpcTimeout or setWriteRpcTimeout instead
-   */
-  @Deprecated
-  default void setRpcTimeout(int rpcTimeout) {
-    setReadRpcTimeout(rpcTimeout);
-    setWriteRpcTimeout(rpcTimeout);
-  }
-
   /**
    * Get timeout of each rpc read request in this Table instance.
    * @param unit the unit of time the timeout to be represented in
@@ -890,30 +656,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Get timeout (millisecond) of each rpc read request in this Table instance.
-   * @deprecated since 2.0 and will be removed in 3.0 version
-   *             use {@link #getReadRpcTimeout(TimeUnit)} instead
-   */
-  @Deprecated
-  default int getReadRpcTimeout() {
-    return (int)getReadRpcTimeout(TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Set timeout (millisecond) of each rpc read request in operations of this Table instance, will
-   * override the value of hbase.rpc.read.timeout in configuration.
-   * If a rpc read request waiting too long, it will stop waiting and send a new request to retry
-   * until retries exhausted or operation timeout reached.
-   *
-   * @param readRpcTimeout the timeout for read rpc request in milliseconds
-   * @deprecated since 2.0.0, use {@link TableBuilder#setReadRpcTimeout} instead
-   */
-  @Deprecated
-  default void setReadRpcTimeout(int readRpcTimeout) {
-    throw new NotImplementedException("Add an implementation!");
-  }
-
   /**
    * Get timeout of each rpc write request in this Table instance.
    * @param unit the unit of time the timeout to be represented in
@@ -923,30 +665,6 @@ public interface Table extends Closeable {
     throw new NotImplementedException("Add an implementation!");
   }
 
-  /**
-   * Get timeout (millisecond) of each rpc write request in this Table instance.
-   * @deprecated since 2.0 and will be removed in 3.0 version
-   *             use {@link #getWriteRpcTimeout(TimeUnit)} instead
-   */
-  @Deprecated
-  default int getWriteRpcTimeout() {
-    return (int)getWriteRpcTimeout(TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Set timeout (millisecond) of each rpc write request in operations of this Table instance, will
-   * override the value of hbase.rpc.write.timeout in configuration.
-   * If a rpc write request waiting too long, it will stop waiting and send a new request to retry
-   * until retries exhausted or operation timeout reached.
-   *
-   * @param writeRpcTimeout the timeout for write rpc request in milliseconds
-   * @deprecated since 2.0.0, use {@link TableBuilder#setWriteRpcTimeout} instead
-   */
-  @Deprecated
-  default void setWriteRpcTimeout(int writeRpcTimeout) {
-    throw new NotImplementedException("Add an implementation!");
-  }
-
   /**
    * Get timeout of each operation in Table instance.
    * @param unit the unit of time the timeout to be represented in
@@ -955,30 +673,4 @@ public interface Table extends Closeable {
   default long getOperationTimeout(TimeUnit unit) {
     throw new NotImplementedException("Add an implementation!");
   }
-
-  /**
-   * Get timeout (millisecond) of each operation for in Table instance.
-   * @deprecated since 2.0 and will be removed in 3.0 version
-   *             use {@link #getOperationTimeout(TimeUnit)} instead
-   */
-  @Deprecated
-  default int getOperationTimeout() {
-    return (int)getOperationTimeout(TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Set timeout (millisecond) of each operation in this Table instance, will override the value
-   * of hbase.client.operation.timeout in configuration.
-   * Operation timeout is a top-level restriction that makes sure a blocking method will not be
-   * blocked more than this. In each operation, if rpc request fails because of timeout or
-   * other reason, it will retry until success or throw a RetriesExhaustedException. But if the
-   * total time being blocking reach the operation timeout before retries exhausted, it will break
-   * early and throw SocketTimeoutException.
-   * @param operationTimeout the total timeout of each operation in millisecond.
-   * @deprecated since 2.0.0, use {@link TableBuilder#setOperationTimeout} instead
-   */
-  @Deprecated
-  default void setOperationTimeout(int operationTimeout) {
-    throw new NotImplementedException("Add an implementation!");
-  }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java
new file mode 100644
index 0000000000..71462120d1
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java
@@ -0,0 +1,527 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import com.google.protobuf.Descriptors.MethodDescriptor;
+import com.google.protobuf.Message;
+import com.google.protobuf.RpcCallback;
+import com.google.protobuf.RpcController;
+import com.google.protobuf.Service;
+import com.google.protobuf.ServiceException;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+import org.apache.commons.lang3.ArrayUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.CompareOperator;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionLocation;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.RetriesExhaustedException.ThrowableWithExtraContext;
+import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
+import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
+import org.apache.hadoop.hbase.io.TimeRange;
+import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FutureUtils;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hbase.thirdparty.com.google.common.primitives.Booleans;
+
+/**
+ * The table implementation based on {@link AsyncTable}.
+ */
+@InterfaceAudience.Private
+class TableOverAsyncTable implements Table {
+
+  private static final Logger LOG = LoggerFactory.getLogger(TableOverAsyncTable.class);
+
+  private final AsyncConnectionImpl conn;
+
+  private final AsyncTable<?> table;
+
+  private final ExecutorService pool;
+
+  TableOverAsyncTable(AsyncConnectionImpl conn, AsyncTable<?> table, ExecutorService pool) {
+    this.conn = conn;
+    this.table = table;
+    this.pool = pool;
+  }
+
+  @Override
+  public TableName getName() {
+    return table.getName();
+  }
+
+  @Override
+  public Configuration getConfiguration() {
+    return table.getConfiguration();
+  }
+
+  @Override
+  public TableDescriptor getDescriptor() throws IOException {
+    return FutureUtils.get(conn.getAdmin().getDescriptor(getName()));
+  }
+
+  @Override
+  public boolean exists(Get get) throws IOException {
+    return FutureUtils.get(table.exists(get));
+  }
+
+  @Override
+  public boolean[] exists(List<Get> gets) throws IOException {
+    return Booleans.toArray(FutureUtils.get(table.existsAll(gets)));
+  }
+
+  @Override
+  public void batch(List<? extends Row> actions, Object[] results) throws IOException {
+    if (ArrayUtils.isEmpty(results)) {
+      FutureUtils.get(table.batchAll(actions));
+      return;
+    }
+    List<ThrowableWithExtraContext> errors = new ArrayList<>();
+    List<CompletableFuture<Object>> futures = table.batch(actions);
+    for (int i = 0, n = results.length; i < n; i++) {
+      try {
+        results[i] = FutureUtils.get(futures.get(i));
+      } catch (IOException e) {
+        results[i] = e;
+        errors.add(new ThrowableWithExtraContext(e, EnvironmentEdgeManager.currentTime(),
+          "Error when processing " + actions.get(i)));
+      }
+    }
+    if (!errors.isEmpty()) {
+      throw new RetriesExhaustedException(errors.size(), errors);
+    }
+  }
+
+  @Override
+  public <R> void batchCallback(List<? extends Row> actions, Object[] results, Callback<R> callback)
+      throws IOException, InterruptedException {
+    ConcurrentLinkedQueue<ThrowableWithExtraContext> errors = new ConcurrentLinkedQueue<>();
+    CountDownLatch latch = new CountDownLatch(actions.size());
+    AsyncTableRegionLocator locator = conn.getRegionLocator(getName());
+    List<CompletableFuture<R>> futures = table.<R> batch(actions);
+    for (int i = 0, n = futures.size(); i < n; i++) {
+      final int index = i;
+      FutureUtils.addListener(futures.get(i), (r, e) -> {
+        if (e != null) {
+          errors.add(new ThrowableWithExtraContext(e, EnvironmentEdgeManager.currentTime(),
+            "Error when processing " + actions.get(index)));
+          if (!ArrayUtils.isEmpty(results)) {
+            results[index] = e;
+          }
+          latch.countDown();
+        } else {
+          if (!ArrayUtils.isEmpty(results)) {
+            results[index] = r;
+          }
+          FutureUtils.addListener(locator.getRegionLocation(actions.get(index).getRow()),
+            (l, le) -> {
+              if (le != null) {
+                errors.add(new ThrowableWithExtraContext(le, EnvironmentEdgeManager.currentTime(),
+                  "Error when finding the region for row " +
+                    Bytes.toStringBinary(actions.get(index).getRow())));
+              } else {
+                callback.update(l.getRegion().getRegionName(), actions.get(index).getRow(), r);
+              }
+              latch.countDown();
+            });
+        }
+      });
+    }
+    latch.await();
+    if (!errors.isEmpty()) {
+      throw new RetriesExhaustedException(errors.size(),
+        errors.stream().collect(Collectors.toList()));
+    }
+  }
+
+  @Override
+  public Result get(Get get) throws IOException {
+    return FutureUtils.get(table.get(get));
+  }
+
+  @Override
+  public Result[] get(List<Get> gets) throws IOException {
+    return FutureUtils.get(table.getAll(gets)).toArray(new Result[0]);
+  }
+
+  @Override
+  public ResultScanner getScanner(Scan scan) throws IOException {
+    return table.getScanner(scan);
+  }
+
+  @Override
+  public ResultScanner getScanner(byte[] family) throws IOException {
+    return table.getScanner(family);
+  }
+
+  @Override
+  public ResultScanner getScanner(byte[] family, byte[] qualifier) throws IOException {
+    return table.getScanner(family, qualifier);
+  }
+
+  @Override
+  public void put(Put put) throws IOException {
+    FutureUtils.get(table.put(put));
+  }
+
+  @Override
+  public void put(List<Put> puts) throws IOException {
+    FutureUtils.get(table.putAll(puts));
+  }
+
+  @Override
+  public void delete(Delete delete) throws IOException {
+    FutureUtils.get(table.delete(delete));
+  }
+
+  @Override
+  public void delete(List<Delete> deletes) throws IOException {
+    FutureUtils.get(table.deleteAll(deletes));
+  }
+
+  private static final class CheckAndMutateBuilderImpl implements CheckAndMutateBuilder {
+
+    private final AsyncTable.CheckAndMutateBuilder builder;
+
+    public CheckAndMutateBuilderImpl(
+        org.apache.hadoop.hbase.client.AsyncTable.CheckAndMutateBuilder builder) {
+      this.builder = builder;
+    }
+
+    @Override
+    public CheckAndMutateBuilder qualifier(byte[] qualifier) {
+      builder.qualifier(qualifier);
+      return this;
+    }
+
+    @Override
+    public CheckAndMutateBuilder timeRange(TimeRange timeRange) {
+      builder.timeRange(timeRange);
+      return this;
+    }
+
+    @Override
+    public CheckAndMutateBuilder ifNotExists() {
+      builder.ifNotExists();
+      return this;
+    }
+
+    @Override
+    public CheckAndMutateBuilder ifMatches(CompareOperator compareOp, byte[] value) {
+      builder.ifMatches(compareOp, value);
+      return this;
+    }
+
+    @Override
+    public boolean thenPut(Put put) throws IOException {
+      return FutureUtils.get(builder.thenPut(put));
+    }
+
+    @Override
+    public boolean thenDelete(Delete delete) throws IOException {
+      return FutureUtils.get(builder.thenDelete(delete));
+    }
+
+    @Override
+    public boolean thenMutate(RowMutations mutation) throws IOException {
+      return FutureUtils.get(builder.thenMutate(mutation));
+    }
+  }
+
+  @Override
+  public CheckAndMutateBuilder checkAndMutate(byte[] row, byte[] family) {
+    return new CheckAndMutateBuilderImpl(table.checkAndMutate(row, family));
+  }
+
+  @Override
+  public void mutateRow(RowMutations rm) throws IOException {
+    FutureUtils.get(table.mutateRow(rm));
+  }
+
+  @Override
+  public Result append(Append append) throws IOException {
+    return FutureUtils.get(table.append(append));
+  }
+
+  @Override
+  public Result increment(Increment increment) throws IOException {
+    return FutureUtils.get(table.increment(increment));
+  }
+
+  @Override
+  public long incrementColumnValue(byte[] row, byte[] family, byte[] qualifier, long amount)
+      throws IOException {
+    return FutureUtils.get(table.incrementColumnValue(row, family, qualifier, amount));
+  }
+
+  @Override
+  public long incrementColumnValue(byte[] row, byte[] family, byte[] qualifier, long amount,
+      Durability durability) throws IOException {
+    return FutureUtils.get(table.incrementColumnValue(row, family, qualifier, amount, durability));
+  }
+
+  @Override
+  public void close() {
+  }
+
+  private static final class BlockingRpcCallback<R> implements RpcCallback<R> {
+    private R result;
+    private boolean resultSet = false;
+
+    /**
+     * Called on completion of the RPC call with the response object, or {@code null} in the case of
+     * an error.
+     * @param parameter the response object or {@code null} if an error occurred
+     */
+    @Override
+    public void run(R parameter) {
+      synchronized (this) {
+        result = parameter;
+        resultSet = true;
+        this.notifyAll();
+      }
+    }
+
+    /**
+     * Returns the parameter passed to {@link #run(Object)} or {@code null} if a null value was
+     * passed. When used asynchronously, this method will block until the {@link #run(Object)}
+     * method has been called.
+     * @return the response object or {@code null} if no response was passed
+     */
+    public synchronized R get() throws IOException {
+      while (!resultSet) {
+        try {
+          this.wait();
+        } catch (InterruptedException ie) {
+          InterruptedIOException exception = new InterruptedIOException(ie.getMessage());
+          exception.initCause(ie);
+          throw exception;
+        }
+      }
+      return result;
+    }
+  }
+
+  private static final class RegionCoprocessorRpcChannel extends RegionCoprocessorRpcChannelImpl
+      implements CoprocessorRpcChannel {
+
+    RegionCoprocessorRpcChannel(AsyncConnectionImpl conn, TableName tableName, RegionInfo region,
+        byte[] row, long rpcTimeoutNs, long operationTimeoutNs) {
+      super(conn, tableName, region, row, rpcTimeoutNs, operationTimeoutNs);
+    }
+
+    @Override
+    public void callMethod(MethodDescriptor method, RpcController controller, Message request,
+        Message responsePrototype, RpcCallback<Message> done) {
+      ClientCoprocessorRpcController c = new ClientCoprocessorRpcController();
+      BlockingRpcCallback<Message> callback = new BlockingRpcCallback<>();
+      super.callMethod(method, c, request, responsePrototype, callback);
+      Message ret;
+      try {
+        ret = callback.get();
+      } catch (IOException e) {
+        setError(controller, e);
+        return;
+      }
+      if (c.failed()) {
+        setError(controller, c.getFailed());
+      }
+      done.run(ret);
+    }
+
+    @Override
+    public Message callBlockingMethod(MethodDescriptor method, RpcController controller,
+        Message request, Message responsePrototype) throws ServiceException {
+      ClientCoprocessorRpcController c = new ClientCoprocessorRpcController();
+      BlockingRpcCallback<Message> done = new BlockingRpcCallback<>();
+      callMethod(method, c, request, responsePrototype, done);
+      Message ret;
+      try {
+        ret = done.get();
+      } catch (IOException e) {
+        throw new ServiceException(e);
+      }
+      if (c.failed()) {
+        setError(controller, c.getFailed());
+        throw new ServiceException(c.getFailed());
+      }
+      return ret;
+    }
+  }
+
+  @Override
+  public RegionCoprocessorRpcChannel coprocessorService(byte[] row) {
+    return new RegionCoprocessorRpcChannel(conn, getName(), null, row,
+      getRpcTimeout(TimeUnit.NANOSECONDS), getOperationTimeout(TimeUnit.NANOSECONDS));
+  }
+
+  /**
+   * Get the corresponding start keys and regions for an arbitrary range of keys.
+   * <p>
+   * @param startKey Starting row in range, inclusive
+   * @param endKey Ending row in range
+   * @param includeEndKey true if endRow is inclusive, false if exclusive
+   * @return A pair of list of start keys and list of HRegionLocations that contain the specified
+   *         range
+   * @throws IOException if a remote or network exception occurs
+   */
+  private Pair<List<byte[]>, List<HRegionLocation>> getKeysAndRegionsInRange(final byte[] startKey,
+      final byte[] endKey, final boolean includeEndKey) throws IOException {
+    return getKeysAndRegionsInRange(startKey, endKey, includeEndKey, false);
+  }
+
+  /**
+   * Get the corresponding start keys and regions for an arbitrary range of keys.
+   * <p>
+   * @param startKey Starting row in range, inclusive
+   * @param endKey Ending row in range
+   * @param includeEndKey true if endRow is inclusive, false if exclusive
+   * @param reload true to reload information or false to use cached information
+   * @return A pair of list of start keys and list of HRegionLocations that contain the specified
+   *         range
+   * @throws IOException if a remote or network exception occurs
+   */
+  private Pair<List<byte[]>, List<HRegionLocation>> getKeysAndRegionsInRange(final byte[] startKey,
+      final byte[] endKey, final boolean includeEndKey, final boolean reload) throws IOException {
+    final boolean endKeyIsEndOfTable = Bytes.equals(endKey, HConstants.EMPTY_END_ROW);
+    if ((Bytes.compareTo(startKey, endKey) > 0) && !endKeyIsEndOfTable) {
+      throw new IllegalArgumentException(
+        "Invalid range: " + Bytes.toStringBinary(startKey) + " > " + Bytes.toStringBinary(endKey));
+    }
+    List<byte[]> keysInRange = new ArrayList<>();
+    List<HRegionLocation> regionsInRange = new ArrayList<>();
+    byte[] currentKey = startKey;
+    do {
+      HRegionLocation regionLocation =
+        FutureUtils.get(conn.getRegionLocator(getName()).getRegionLocation(currentKey, reload));
+      keysInRange.add(currentKey);
+      regionsInRange.add(regionLocation);
+      currentKey = regionLocation.getRegion().getEndKey();
+    } while (!Bytes.equals(currentKey, HConstants.EMPTY_END_ROW) &&
+      (endKeyIsEndOfTable || Bytes.compareTo(currentKey, endKey) < 0 ||
+        (includeEndKey && Bytes.compareTo(currentKey, endKey) == 0)));
+    return new Pair<>(keysInRange, regionsInRange);
+  }
+
+  private List<byte[]> getStartKeysInRange(byte[] start, byte[] end) throws IOException {
+    if (start == null) {
+      start = HConstants.EMPTY_START_ROW;
+    }
+    if (end == null) {
+      end = HConstants.EMPTY_END_ROW;
+    }
+    return getKeysAndRegionsInRange(start, end, true).getFirst();
+  }
+
+  @FunctionalInterface
+  private interface StubCall<R> {
+    R call(RegionCoprocessorRpcChannel channel) throws Exception;
+  }
+
+  private <R> void coprocssorService(String serviceName, byte[] startKey, byte[] endKey,
+      Callback<R> callback, StubCall<R> call) throws Throwable {
+    // get regions covered by the row range
+    List<byte[]> keys = getStartKeysInRange(startKey, endKey);
+    Map<byte[], Future<R>> futures = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    for (byte[] r : keys) {
+      RegionCoprocessorRpcChannel channel = coprocessorService(r);
+      Future<R> future = pool.submit(new Callable<R>() {
+        @Override
+        public R call() throws Exception {
+          R result = call.call(channel);
+          byte[] region = channel.getLastRegion();
+          if (callback != null) {
+            callback.update(region, r, result);
+          }
+          return result;
+        }
+      });
+      futures.put(r, future);
+    }
+    for (Map.Entry<byte[], Future<R>> e : futures.entrySet()) {
+      try {
+        e.getValue().get();
+      } catch (ExecutionException ee) {
+        LOG.warn("Error calling coprocessor service " + serviceName + " for row " +
+          Bytes.toStringBinary(e.getKey()), ee);
+        throw ee.getCause();
+      } catch (InterruptedException ie) {
+        throw new InterruptedIOException("Interrupted calling coprocessor service " + serviceName +
+          " for row " + Bytes.toStringBinary(e.getKey())).initCause(ie);
+      }
+    }
+  }
+
+  @Override
+  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
+      byte[] endKey, Call<T, R> callable, Callback<R> callback) throws ServiceException, Throwable {
+    coprocssorService(service.getName(), startKey, endKey, callback, channel -> {
+      T instance = org.apache.hadoop.hbase.protobuf.ProtobufUtil.newServiceStub(service, channel);
+      return callable.call(instance);
+    });
+  }
+
+  @SuppressWarnings("unchecked")
+  @Override
+  public <R extends Message> void batchCoprocessorService(MethodDescriptor methodDescriptor,
+      Message request, byte[] startKey, byte[] endKey, R responsePrototype, Callback<R> callback)
+      throws ServiceException, Throwable {
+    coprocssorService(methodDescriptor.getFullName(), startKey, endKey, callback, channel -> {
+      return (R) channel.callBlockingMethod(methodDescriptor, null, request, responsePrototype);
+    });
+  }
+
+  @Override
+  public long getRpcTimeout(TimeUnit unit) {
+    return table.getRpcTimeout(unit);
+  }
+
+  @Override
+  public long getReadRpcTimeout(TimeUnit unit) {
+    return table.getReadRpcTimeout(unit);
+  }
+
+  @Override
+  public long getWriteRpcTimeout(TimeUnit unit) {
+    return table.getWriteRpcTimeout(unit);
+  }
+
+  @Override
+  public long getOperationTimeout(TimeUnit unit) {
+    return table.getOperationTimeout(unit);
+  }
+}
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcChannel.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcChannel.java
index 6ae7027894..43d135bcaf 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcChannel.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcChannel.java
@@ -22,12 +22,18 @@ import com.google.protobuf.RpcChannel;
 import org.apache.yetus.audience.InterfaceAudience;
 
 /**
- * Base interface which provides clients with an RPC connection to
- * call coprocessor endpoint {@link com.google.protobuf.Service}s.
+ * Base interface which provides clients with an RPC connection to call coprocessor endpoint
+ * {@link com.google.protobuf.Service}s.
+ * <p/>
  * Note that clients should not use this class directly, except through
  * {@link org.apache.hadoop.hbase.client.Table#coprocessorService(byte[])}.
+ * <p/>
+ * @deprecated Please stop using this class again, as it is too low level, which is part of the rpc
+ *             framework for HBase. Will be deleted in 4.0.0.
  */
+@Deprecated
 @InterfaceAudience.Public
-public interface CoprocessorRpcChannel extends RpcChannel, BlockingRpcChannel {}
+public interface CoprocessorRpcChannel extends RpcChannel, BlockingRpcChannel {
+}
 // This Interface is part of our public, client-facing API!!!
 // This belongs in client package but it is exposed in our public API so we cannot relocate.
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/SimpleRegistry.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/SimpleRegistry.java
new file mode 100644
index 0000000000..4d4d62043f
--- /dev/null
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/SimpleRegistry.java
@@ -0,0 +1,83 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.util.concurrent.CompletableFuture;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionLocation;
+import org.apache.hadoop.hbase.RegionLocations;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.util.FutureUtils;
+
+/**
+ * Simple cluster registry inserted in place of our usual zookeeper based one.
+ */
+class SimpleRegistry extends DoNothingAsyncRegistry {
+
+  private final ServerName metaHost;
+
+  volatile boolean closed = false;
+
+  private static final String META_HOST_CONFIG_NAME = "hbase.client.simple-registry.meta.host";
+
+  private static final String DEFAULT_META_HOST = "meta.example.org.16010,12345";
+
+  public static void setMetaHost(Configuration conf, ServerName metaHost) {
+    conf.set(META_HOST_CONFIG_NAME, metaHost.getServerName());
+  }
+
+  public SimpleRegistry(Configuration conf) {
+    super(conf);
+    this.metaHost = ServerName.valueOf(conf.get(META_HOST_CONFIG_NAME, DEFAULT_META_HOST));
+  }
+
+  @Override
+  public CompletableFuture<RegionLocations> getMetaRegionLocation() {
+    if (closed) {
+      return FutureUtils.failedFuture(new DoNotRetryIOException("Client already closed"));
+    } else {
+      return CompletableFuture.completedFuture(new RegionLocations(
+        new HRegionLocation(RegionInfoBuilder.FIRST_META_REGIONINFO, metaHost)));
+    }
+  }
+
+  @Override
+  public CompletableFuture<String> getClusterId() {
+    if (closed) {
+      return FutureUtils.failedFuture(new DoNotRetryIOException("Client already closed"));
+    } else {
+      return CompletableFuture.completedFuture(HConstants.CLUSTER_ID_DEFAULT);
+    }
+  }
+
+  @Override
+  public CompletableFuture<Integer> getCurrentNrHRS() {
+    if (closed) {
+      return FutureUtils.failedFuture(new DoNotRetryIOException("Client already closed"));
+    } else {
+      return CompletableFuture.completedFuture(1);
+    }
+  }
+
+  @Override
+  public void close() {
+    closed = true;
+  }
+}
\ No newline at end of file
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
index d4781d12ed..0899fa1ed0 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
@@ -1351,7 +1351,7 @@ public class TestAsyncProcess {
       ap.previousTimeout = -1;
 
       try {
-        ht.existsAll(gets);
+        ht.exists(gets);
       } catch (ClassCastException e) {
         // No result response on this test.
       }
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestBufferedMutator.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestBufferedMutator.java
index 647ea32a5e..96bb846128 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestBufferedMutator.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestBufferedMutator.java
@@ -25,6 +25,7 @@ import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ipc.RpcControllerFactory;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.junit.ClassRule;
@@ -33,12 +34,12 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
-@Category({SmallTests.class, ClientTests.class})
+@Category({ SmallTests.class, ClientTests.class })
 public class TestBufferedMutator {
 
   @ClassRule
   public static final HBaseClassTestRule CLASS_RULE =
-      HBaseClassTestRule.forClass(TestBufferedMutator.class);
+    HBaseClassTestRule.forClass(TestBufferedMutator.class);
 
   @Rule
   public TestName name = new TestName();
@@ -55,10 +56,12 @@ public class TestBufferedMutator {
 
   @Test
   public void testAlternateBufferedMutatorImpl() throws IOException {
-    BufferedMutatorParams params =  new BufferedMutatorParams(TableName.valueOf(name.getMethodName()));
+    BufferedMutatorParams params =
+      new BufferedMutatorParams(TableName.valueOf(name.getMethodName()));
     Configuration conf = HBaseConfiguration.create();
     conf.set(AsyncRegistryFactory.REGISTRY_IMPL_CONF_KEY, DoNothingAsyncRegistry.class.getName());
-    try (Connection connection = ConnectionFactory.createConnection(conf)) {
+    try (ConnectionImplementation connection = ConnectionFactory.createConnectionImpl(conf, null,
+      UserProvider.instantiate(conf).getCurrent())) {
       BufferedMutator bm = connection.getBufferedMutator(params);
       // Assert we get default BM if nothing specified.
       assertTrue(bm instanceof BufferedMutatorImpl);
@@ -70,7 +73,8 @@ public class TestBufferedMutator {
     // Now try creating a Connection after setting an alterate BufferedMutator into
     // the configuration and confirm we get what was expected.
     conf.set(BufferedMutator.CLASSNAME_KEY, MyBufferedMutator.class.getName());
-    try (Connection connection = ConnectionFactory.createConnection(conf)) {
+    try (Connection connection = ConnectionFactory.createConnectionImpl(conf, null,
+      UserProvider.instantiate(conf).getCurrent())) {
       BufferedMutator bm = connection.getBufferedMutator(params);
       assertTrue(bm instanceof MyBufferedMutator);
     }
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
index 3cab09dec5..fd3a4f8deb 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
@@ -28,7 +28,6 @@ import java.util.Map;
 import java.util.Objects;
 import java.util.Random;
 import java.util.SortedMap;
-import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentSkipListMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
@@ -43,10 +42,8 @@ import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MetaTableAccessor;
-import org.apache.hadoop.hbase.RegionLocations;
 import org.apache.hadoop.hbase.RegionTooBusyException;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
@@ -118,37 +115,11 @@ public class TestClientNoCluster extends Configured implements Tool {
   @Before
   public void setUp() throws Exception {
     this.conf = HBaseConfiguration.create();
-    // Run my Connection overrides.  Use my little ConnectionImplementation below which
+    // Run my Connection overrides. Use my little ConnectionImplementation below which
     // allows me insert mocks and also use my Registry below rather than the default zk based
     // one so tests run faster and don't have zk dependency.
     this.conf.set("hbase.client.registry.impl", SimpleRegistry.class.getName());
-  }
-
-  /**
-   * Simple cluster registry inserted in place of our usual zookeeper based one.
-   */
-  static class SimpleRegistry extends DoNothingAsyncRegistry {
-    final ServerName META_HOST = META_SERVERNAME;
-
-    public SimpleRegistry(Configuration conf) {
-      super(conf);
-    }
-
-    @Override
-    public CompletableFuture<RegionLocations> getMetaRegionLocation() {
-      return CompletableFuture.completedFuture(new RegionLocations(
-          new HRegionLocation(RegionInfoBuilder.FIRST_META_REGIONINFO, META_HOST)));
-    }
-
-    @Override
-    public CompletableFuture<String> getClusterId() {
-      return CompletableFuture.completedFuture(HConstants.CLUSTER_ID_DEFAULT);
-    }
-
-    @Override
-    public CompletableFuture<Integer> getCurrentNrHRS() {
-      return CompletableFuture.completedFuture(1);
-    }
+    SimpleRegistry.setMetaHost(conf, META_SERVERNAME);
   }
 
   /**
diff --git a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
index da4c62bb14..4e4e11a133 100644
--- a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
+++ b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
@@ -75,6 +75,7 @@ import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.io.compress.Compression;
 import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;
@@ -521,7 +522,7 @@ public class TestHFileOutputFormat2  {
     RegionLocator regionLocator = Mockito.mock(RegionLocator.class);
     setupMockStartKeys(regionLocator);
     setupMockTableName(regionLocator);
-    HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);
+    HFileOutputFormat2.configureIncrementalLoad(job, table.getDescriptor(), regionLocator);
     assertEquals(job.getNumReduceTasks(), 4);
   }
 
@@ -631,7 +632,7 @@ public class TestHFileOutputFormat2  {
       assertEquals("Should make " + regionNum + " regions", numRegions, regionNum);
 
       allTables.put(tableStrSingle, table);
-      tableInfo.add(new HFileOutputFormat2.TableInfo(table.getTableDescriptor(), r));
+      tableInfo.add(new HFileOutputFormat2.TableInfo(table.getDescriptor(), r));
     }
     Path testDir = util.getDataTestDirOnTestFS("testLocalMRIncrementalLoad");
     // Generate the bulk load files
@@ -817,7 +818,7 @@ public class TestHFileOutputFormat2  {
       conf.set(HFileOutputFormat2.COMPRESSION_FAMILIES_CONF_KEY,
               HFileOutputFormat2.serializeColumnFamilyAttribute
                       (HFileOutputFormat2.compressionDetails,
-                              Arrays.asList(table.getTableDescriptor())));
+                              Arrays.asList(table.getDescriptor())));
 
       // read back family specific compression setting from the configuration
       Map<byte[], Algorithm> retrievedFamilyToCompressionMap = HFileOutputFormat2
@@ -843,7 +844,7 @@ public class TestHFileOutputFormat2  {
           .setBlockCacheEnabled(false)
           .setTimeToLive(0));
     }
-    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();
+    Mockito.doReturn(mockTableDescriptor).when(table).getDescriptor();
   }
 
   /**
@@ -889,7 +890,7 @@ public class TestHFileOutputFormat2  {
           familyToBloomType);
       conf.set(HFileOutputFormat2.BLOOM_TYPE_FAMILIES_CONF_KEY,
               HFileOutputFormat2.serializeColumnFamilyAttribute(HFileOutputFormat2.bloomTypeDetails,
-              Arrays.asList(table.getTableDescriptor())));
+              Arrays.asList(table.getDescriptor())));
 
       // read back family specific data block encoding settings from the
       // configuration
@@ -917,7 +918,7 @@ public class TestHFileOutputFormat2  {
           .setBlockCacheEnabled(false)
           .setTimeToLive(0));
     }
-    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();
+    Mockito.doReturn(mockTableDescriptor).when(table).getDescriptor();
   }
 
   /**
@@ -961,7 +962,7 @@ public class TestHFileOutputFormat2  {
       conf.set(HFileOutputFormat2.BLOCK_SIZE_FAMILIES_CONF_KEY,
               HFileOutputFormat2.serializeColumnFamilyAttribute
                       (HFileOutputFormat2.blockSizeDetails, Arrays.asList(table
-                              .getTableDescriptor())));
+                              .getDescriptor())));
 
       // read back family specific data block encoding settings from the
       // configuration
@@ -990,7 +991,7 @@ public class TestHFileOutputFormat2  {
           .setBlockCacheEnabled(false)
           .setTimeToLive(0));
     }
-    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();
+    Mockito.doReturn(mockTableDescriptor).when(table).getDescriptor();
   }
 
   /**
@@ -1035,7 +1036,7 @@ public class TestHFileOutputFormat2  {
       Table table = Mockito.mock(Table.class);
       setupMockColumnFamiliesForDataBlockEncoding(table,
           familyToDataBlockEncoding);
-      HTableDescriptor tableDescriptor = table.getTableDescriptor();
+      TableDescriptor tableDescriptor = table.getDescriptor();
       conf.set(HFileOutputFormat2.DATABLOCK_ENCODING_FAMILIES_CONF_KEY,
               HFileOutputFormat2.serializeColumnFamilyAttribute
                       (HFileOutputFormat2.dataBlockEncodingDetails, Arrays
@@ -1067,7 +1068,7 @@ public class TestHFileOutputFormat2  {
           .setBlockCacheEnabled(false)
           .setTimeToLive(0));
     }
-    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();
+    Mockito.doReturn(mockTableDescriptor).when(table).getDescriptor();
   }
 
   /**
@@ -1125,7 +1126,7 @@ public class TestHFileOutputFormat2  {
     Table table = Mockito.mock(Table.class);
     RegionLocator regionLocator = Mockito.mock(RegionLocator.class);
     HTableDescriptor htd = new HTableDescriptor(TABLE_NAMES[0]);
-    Mockito.doReturn(htd).when(table).getTableDescriptor();
+    Mockito.doReturn(htd).when(table).getDescriptor();
     for (HColumnDescriptor hcd: HBaseTestingUtility.generateColumnDescriptors()) {
       htd.addFamily(hcd);
     }
@@ -1145,7 +1146,7 @@ public class TestHFileOutputFormat2  {
       Job job = new Job(conf, "testLocalMRIncrementalLoad");
       job.setWorkingDirectory(util.getDataTestDirOnTestFS("testColumnFamilySettings"));
       setupRandomGeneratorMapper(job, false);
-      HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);
+      HFileOutputFormat2.configureIncrementalLoad(job, table.getDescriptor(), regionLocator);
       FileOutputFormat.setOutputPath(job, dir);
       context = createTestTaskAttemptContext(job);
       HFileOutputFormat2 hof = new HFileOutputFormat2();
@@ -1412,7 +1413,7 @@ public class TestHFileOutputFormat2  {
           RegionLocator regionLocator = c.getRegionLocator(tname)) {
         Path outDir = new Path("incremental-out");
         runIncrementalPELoad(conf, Arrays.asList(new HFileOutputFormat2.TableInfo(admin
-                .getTableDescriptor(tname), regionLocator)), outDir, false);
+                .getDescriptor(tname), regionLocator)), outDir, false);
       }
     } else {
       throw new RuntimeException(
diff --git a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
index eff26d7994..af97793c25 100644
--- a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
+++ b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
@@ -34,6 +34,7 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.AsyncConnection;
 import org.apache.hadoop.hbase.client.BufferedMutator;
 import org.apache.hadoop.hbase.client.BufferedMutatorParams;
 import org.apache.hadoop.hbase.client.Connection;
@@ -240,5 +241,10 @@ public class TestMultiTableInputFormatBase {
     @Override
     public void clearRegionLocationCache() {
     }
+
+    @Override
+    public AsyncConnection toAsyncConnection() {
+      return null;
+    }
   }
 }
diff --git a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatBase.java b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatBase.java
index 944bd1049b..5fd5ccff73 100644
--- a/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatBase.java
+++ b/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatBase.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.AsyncConnection;
 import org.apache.hadoop.hbase.client.BufferedMutator;
 import org.apache.hadoop.hbase.client.BufferedMutatorParams;
 import org.apache.hadoop.hbase.client.Connection;
@@ -290,5 +291,10 @@ public class TestTableInputFormatBase {
     @Override
     public void clearRegionLocationCache() {
     }
+
+    @Override
+    public AsyncConnection toAsyncConnection() {
+      throw new UnsupportedOperationException();
+    }
   }
 }
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java
index ecfe86d239..028ac6755d 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java
@@ -26,6 +26,7 @@ import javax.ws.rs.core.Response;
 
 import org.apache.yetus.audience.InterfaceAudience;
 import org.apache.hadoop.hbase.TableNotFoundException;
+import org.apache.hadoop.hbase.client.RetriesExhaustedException;
 import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;
 import org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException;
 import org.apache.hadoop.util.StringUtils;
@@ -82,10 +83,9 @@ public class ResourceBase implements Constants {
               StringUtils.stringifyException(exp) + CRLF)
             .build());
     }
-    if (exp instanceof RetriesExhaustedWithDetailsException) {
-      RetriesExhaustedWithDetailsException retryException =
-          (RetriesExhaustedWithDetailsException) exp;
-      processException(retryException.getCause(0));
+    if (exp instanceof RetriesExhaustedException) {
+      RetriesExhaustedException retryException = (RetriesExhaustedException) exp;
+      processException(retryException.getCause());
     }
     throw new WebApplicationException(
       Response.status(Response.Status.SERVICE_UNAVAILABLE)
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
index 83a70fffcb..40f8b2d004 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
@@ -21,7 +21,6 @@ package org.apache.hadoop.hbase.rest;
 
 import java.io.IOException;
 import java.util.Map;
-
 import javax.ws.rs.Consumes;
 import javax.ws.rs.DELETE;
 import javax.ws.rs.GET;
@@ -35,20 +34,19 @@ import javax.ws.rs.core.Response;
 import javax.ws.rs.core.Response.ResponseBuilder;
 import javax.ws.rs.core.UriInfo;
 import javax.xml.namespace.QName;
-
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotEnabledException;
 import org.apache.hadoop.hbase.TableNotFoundException;
-import org.apache.yetus.audience.InterfaceAudience;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.rest.model.ColumnSchemaModel;
 import org.apache.hadoop.hbase.rest.model.TableSchemaModel;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 @InterfaceAudience.Private
 public class SchemaResource extends ResourceBase {
@@ -73,13 +71,9 @@ public class SchemaResource extends ResourceBase {
     this.tableResource = tableResource;
   }
 
-  private HTableDescriptor getTableSchema() throws IOException,
-      TableNotFoundException {
-    Table table = servlet.getTable(tableResource.getName());
-    try {
-      return table.getTableDescriptor();
-    } finally {
-      table.close();
+  private HTableDescriptor getTableSchema() throws IOException, TableNotFoundException {
+    try (Table table = servlet.getTable(tableResource.getName())) {
+      return new HTableDescriptor(table.getDescriptor());
     }
   }
 
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
index 29b48e1443..4addfb4436 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
@@ -23,19 +23,26 @@ import com.google.protobuf.Descriptors;
 import com.google.protobuf.Message;
 import com.google.protobuf.Service;
 import com.google.protobuf.ServiceException;
-
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.io.UnsupportedEncodingException;
+import java.net.URLEncoder;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.concurrent.TimeUnit;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.CompareOperator;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.TableName;
-import org.apache.yetus.audience.InterfaceAudience;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Durability;
@@ -52,7 +59,6 @@ import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
 import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
 import org.apache.hadoop.hbase.client.metrics.ScanMetrics;
-import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
 import org.apache.hadoop.hbase.rest.Constants;
@@ -63,19 +69,9 @@ import org.apache.hadoop.hbase.rest.model.ScannerModel;
 import org.apache.hadoop.hbase.rest.model.TableSchemaModel;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.util.StringUtils;
-
-import java.io.IOException;
-import java.io.InterruptedIOException;
-import java.io.UnsupportedEncodingException;
-import java.net.URLEncoder;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-import java.util.concurrent.TimeUnit;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;
 
@@ -256,36 +252,6 @@ public class RemoteHTable implements Table {
     return conf;
   }
 
-  @Override
-  @Deprecated
-  public HTableDescriptor getTableDescriptor() throws IOException {
-    StringBuilder sb = new StringBuilder();
-    sb.append('/');
-    sb.append(Bytes.toString(name));
-    sb.append('/');
-    sb.append("schema");
-    for (int i = 0; i < maxRetries; i++) {
-      Response response = client.get(sb.toString(), Constants.MIMETYPE_PROTOBUF);
-      int code = response.getCode();
-      switch (code) {
-      case 200:
-        TableSchemaModel schema = new TableSchemaModel();
-        schema.getObjectFromMessage(response.getBody());
-        return schema.getTableDescriptor();
-      case 509:
-        try {
-          Thread.sleep(sleepTime);
-        } catch (InterruptedException e) {
-          throw (InterruptedIOException)new InterruptedIOException().initCause(e);
-        }
-        break;
-      default:
-        throw new IOException("schema request returned " + code);
-      }
-    }
-    throw new IOException("schema request timed out");
-  }
-
   @Override
   public void close() throws IOException {
     client.shutdown();
@@ -316,12 +282,13 @@ public class RemoteHTable implements Table {
     int maxVersions = 1;
     int count = 0;
 
-    for(Get g:gets) {
+    for (Get g : gets) {
 
-      if ( count == 0 ) {
+      if (count == 0) {
         maxVersions = g.getMaxVersions();
       } else if (g.getMaxVersions() != maxVersions) {
-        LOG.warn("MaxVersions on Gets do not match, using the first in the list ("+maxVersions+")");
+        LOG.warn(
+          "MaxVersions on Gets do not match, using the first in the list (" + maxVersions + ")");
       }
 
       if (g.getFilter() != null) {
@@ -329,7 +296,7 @@ public class RemoteHTable implements Table {
       }
 
       rows[count] = g.getRow();
-      count ++;
+      count++;
     }
 
     String spec = buildMultiRowSpec(rows, maxVersions);
@@ -346,7 +313,7 @@ public class RemoteHTable implements Table {
           CellSetModel model = new CellSetModel();
           model.getObjectFromMessage(response.getBody());
           Result[] results = buildResultFromModel(model);
-          if ( results.length > 0) {
+          if (results.length > 0) {
             return results;
           }
           // fall through
@@ -357,7 +324,7 @@ public class RemoteHTable implements Table {
           try {
             Thread.sleep(sleepTime);
           } catch (InterruptedException e) {
-            throw (InterruptedIOException)new InterruptedIOException().initCause(e);
+            throw (InterruptedIOException) new InterruptedIOException().initCause(e);
           }
           break;
         default:
@@ -393,21 +360,21 @@ public class RemoteHTable implements Table {
     sb.append('/');
     sb.append(toURLEncodedBytes(put.getRow()));
     for (int i = 0; i < maxRetries; i++) {
-      Response response = client.put(sb.toString(), Constants.MIMETYPE_PROTOBUF,
-        model.createProtobufOutput());
+      Response response =
+        client.put(sb.toString(), Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());
       int code = response.getCode();
       switch (code) {
-      case 200:
-        return;
-      case 509:
-        try {
-          Thread.sleep(sleepTime);
-        } catch (InterruptedException e) {
-          throw (InterruptedIOException)new InterruptedIOException().initCause(e);
-        }
-        break;
-      default:
-        throw new IOException("put request failed with " + code);
+        case 200:
+          return;
+        case 509:
+          try {
+            Thread.sleep(sleepTime);
+          } catch (InterruptedException e) {
+            throw (InterruptedIOException) new InterruptedIOException().initCause(e);
+          }
+          break;
+        default:
+          throw new IOException("put request failed with " + code);
       }
     }
     throw new IOException("put request timed out");
@@ -419,24 +386,24 @@ public class RemoteHTable implements Table {
     // ignores the row specification in the URI
 
     // separate puts by row
-    TreeMap<byte[],List<Cell>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
-    for (Put put: puts) {
+    TreeMap<byte[], List<Cell>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    for (Put put : puts) {
       byte[] row = put.getRow();
       List<Cell> cells = map.get(row);
       if (cells == null) {
         cells = new ArrayList<>();
         map.put(row, cells);
       }
-      for (List<Cell> l: put.getFamilyCellMap().values()) {
+      for (List<Cell> l : put.getFamilyCellMap().values()) {
         cells.addAll(l);
       }
     }
 
     // build the cell set
     CellSetModel model = new CellSetModel();
-    for (Map.Entry<byte[], List<Cell>> e: map.entrySet()) {
+    for (Map.Entry<byte[], List<Cell>> e : map.entrySet()) {
       RowModel row = new RowModel(e.getKey());
-      for (Cell cell: e.getValue()) {
+      for (Cell cell : e.getValue()) {
         row.addCell(new CellModel(cell));
       }
       model.addRow(row);
@@ -448,21 +415,21 @@ public class RemoteHTable implements Table {
     sb.append(Bytes.toString(name));
     sb.append("/$multiput"); // can be any nonexistent row
     for (int i = 0; i < maxRetries; i++) {
-      Response response = client.put(sb.toString(), Constants.MIMETYPE_PROTOBUF,
-        model.createProtobufOutput());
+      Response response =
+        client.put(sb.toString(), Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());
       int code = response.getCode();
       switch (code) {
-      case 200:
-        return;
-      case 509:
-        try {
-          Thread.sleep(sleepTime);
-        } catch (InterruptedException e) {
-          throw (InterruptedIOException)new InterruptedIOException().initCause(e);
-        }
-        break;
-      default:
-        throw new IOException("multiput request failed with " + code);
+        case 200:
+          return;
+        case 509:
+          try {
+            Thread.sleep(sleepTime);
+          } catch (InterruptedException e) {
+            throw (InterruptedIOException) new InterruptedIOException().initCause(e);
+          }
+          break;
+        default:
+          throw new IOException("multiput request failed with " + code);
       }
     }
     throw new IOException("multiput request timed out");
@@ -505,7 +472,31 @@ public class RemoteHTable implements Table {
 
   @Override
   public TableDescriptor getDescriptor() throws IOException {
-    return getTableDescriptor();
+    StringBuilder sb = new StringBuilder();
+    sb.append('/');
+    sb.append(Bytes.toString(name));
+    sb.append('/');
+    sb.append("schema");
+    for (int i = 0; i < maxRetries; i++) {
+      Response response = client.get(sb.toString(), Constants.MIMETYPE_PROTOBUF);
+      int code = response.getCode();
+      switch (code) {
+        case 200:
+          TableSchemaModel schema = new TableSchemaModel();
+          schema.getObjectFromMessage(response.getBody());
+          return schema.getTableDescriptor();
+        case 509:
+          try {
+            Thread.sleep(sleepTime);
+          } catch (InterruptedException e) {
+            throw (InterruptedIOException) new InterruptedIOException().initCause(e);
+          }
+          break;
+        default:
+          throw new IOException("schema request returned " + code);
+      }
+    }
+    throw new IOException("schema request timed out");
   }
 
   class Scanner implements ResultScanner {
@@ -671,13 +662,6 @@ public class RemoteHTable implements Table {
     return true;
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException {
-    return doCheckAndPut(row, family, qualifier, value, put);
-  }
-
   private boolean doCheckAndPut(byte[] row, byte[] family, byte[] qualifier,
       byte[] value, Put put) throws IOException {
     // column to check-the-value
@@ -714,26 +698,6 @@ public class RemoteHTable implements Table {
     throw new IOException("checkAndPut request timed out");
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      CompareOp compareOp, byte[] value, Put put) throws IOException {
-    throw new IOException("checkAndPut for non-equal comparison not implemented");
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-                             CompareOperator compareOp, byte[] value, Put put) throws IOException {
-    throw new IOException("checkAndPut for non-equal comparison not implemented");
-  }
-
-  @Override
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException {
-    return doCheckAndDelete(row, family, qualifier, value, delete);
-  }
-
   private boolean doCheckAndDelete(byte[] row, byte[] family, byte[] qualifier,
       byte[] value, Delete delete) throws IOException {
     Put put = new Put(row);
@@ -771,39 +735,11 @@ public class RemoteHTable implements Table {
     throw new IOException("checkAndDelete request timed out");
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      CompareOp compareOp, byte[] value, Delete delete) throws IOException {
-    throw new IOException("checkAndDelete for non-equal comparison not implemented");
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-                                CompareOperator compareOp, byte[] value, Delete delete) throws IOException {
-    throw new IOException("checkAndDelete for non-equal comparison not implemented");
-  }
-
   @Override
   public CheckAndMutateBuilder checkAndMutate(byte[] row, byte[] family) {
     return new CheckAndMutateBuilderImpl(row, family);
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier,
-      CompareOp compareOp, byte[] value, RowMutations rm) throws IOException {
-    throw new UnsupportedOperationException("checkAndMutate not implemented");
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier,
-      CompareOperator compareOp, byte[] value, RowMutations rm) throws IOException {
-    throw new UnsupportedOperationException("checkAndMutate not implemented");
-  }
-
   @Override
   public Result increment(Increment increment) throws IOException {
     throw new IOException("Increment not supported");
@@ -876,69 +812,21 @@ public class RemoteHTable implements Table {
     throw new UnsupportedOperationException("batchCoprocessorService not implemented");
   }
 
-  @Override
-  @Deprecated
-  public void setOperationTimeout(int operationTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public int getOperationTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public void setRpcTimeout(int rpcTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getReadRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public int getRpcTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public int getReadRpcTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public void setReadRpcTimeout(int readRpcTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getWriteRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public int getWriteRpcTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public void setWriteRpcTimeout(int writeRpcTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getOperationTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java
index da09473ced..28d941ccbc 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java
@@ -377,22 +377,20 @@ public class TestScannerResource {
     assertEquals(404, response.getCode());
   }
 
-  // performs table scan during which the underlying table is disabled
-  // assert that we get 410 (Gone)
   @Test
   public void testTableScanWithTableDisable() throws IOException {
+    TEST_UTIL.getAdmin().disableTable(TABLE_TO_BE_DISABLED);
     ScannerModel model = new ScannerModel();
     model.addColumn(Bytes.toBytes(COLUMN_1));
     model.setCaching(1);
     Response response = client.put("/" + TABLE_TO_BE_DISABLED + "/scanner",
       Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());
+    // we will see the exception when we actually want to get the result.
     assertEquals(201, response.getCode());
     String scannerURI = response.getLocation();
     assertNotNull(scannerURI);
-    TEST_UTIL.getAdmin().disableTable(TABLE_TO_BE_DISABLED);
-      response = client.get(scannerURI, Constants.MIMETYPE_PROTOBUF);
-    assertTrue("got " + response.getCode(), response.getCode() == 410);
+    response = client.get(scannerURI, Constants.MIMETYPE_PROTOBUF);
+    assertEquals(410, response.getCode());
   }
-
 }
 
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
index c6f51957ef..269dc68ae5 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.rest.HBaseRESTTestingUtility;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.testclassification.RestTests;
@@ -154,8 +155,8 @@ public class TestRemoteTable {
     Table table = null;
     try {
       table = TEST_UTIL.getConnection().getTable(TABLE);
-      HTableDescriptor local = table.getTableDescriptor();
-      assertEquals(remoteTable.getTableDescriptor(), local);
+      TableDescriptor local = table.getDescriptor();
+      assertEquals(remoteTable.getDescriptor(), new HTableDescriptor(local));
     } finally {
       if (null != table) table.close();
     }
@@ -504,7 +505,7 @@ public class TestRemoteTable {
     assertTrue(Bytes.equals(VALUE_1, value1));
     assertNull(value2);
     assertTrue(remoteTable.exists(get));
-    assertEquals(1, remoteTable.existsAll(Collections.singletonList(get)).length);
+    assertEquals(1, remoteTable.exists(Collections.singletonList(get)).length);
     Delete delete = new Delete(ROW_1);
 
     remoteTable.checkAndMutate(ROW_1, COLUMN_1).qualifier(QUALIFIER_1)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedAsyncConnection.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedAsyncConnection.java
new file mode 100644
index 0000000000..0f05b21c05
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedAsyncConnection.java
@@ -0,0 +1,112 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.io.IOException;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutorService;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.yetus.audience.InterfaceAudience;
+
+/**
+ * Wraps a {@link AsyncConnection} to make it can't be closed.
+ */
+@InterfaceAudience.Private
+public class SharedAsyncConnection implements AsyncConnection {
+
+  private final AsyncConnection conn;
+
+  public SharedAsyncConnection(AsyncConnection conn) {
+    this.conn = conn;
+  }
+
+  @Override
+  public boolean isClosed() {
+    return conn.isClosed();
+  }
+
+  @Override
+  public void close() throws IOException {
+    throw new UnsupportedOperationException("Shared connection");
+  }
+
+  @Override
+  public Configuration getConfiguration() {
+    return conn.getConfiguration();
+  }
+
+  @Override
+  public AsyncTableRegionLocator getRegionLocator(TableName tableName) {
+    return conn.getRegionLocator(tableName);
+  }
+
+  @Override
+  public void clearRegionLocationCache() {
+    conn.clearRegionLocationCache();
+  }
+
+  @Override
+  public AsyncTableBuilder<AdvancedScanResultConsumer> getTableBuilder(TableName tableName) {
+    return conn.getTableBuilder(tableName);
+  }
+
+  @Override
+  public AsyncTableBuilder<ScanResultConsumer> getTableBuilder(TableName tableName,
+      ExecutorService pool) {
+    return conn.getTableBuilder(tableName, pool);
+  }
+
+  @Override
+  public AsyncAdminBuilder getAdminBuilder() {
+    return conn.getAdminBuilder();
+  }
+
+  @Override
+  public AsyncAdminBuilder getAdminBuilder(ExecutorService pool) {
+    return conn.getAdminBuilder(pool);
+  }
+
+  @Override
+  public AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(TableName tableName) {
+    return conn.getBufferedMutatorBuilder(tableName);
+  }
+
+  @Override
+  public AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(TableName tableName,
+      ExecutorService pool) {
+    return conn.getBufferedMutatorBuilder(tableName, pool);
+  }
+
+  @Override
+  public CompletableFuture<Hbck> getHbck() {
+    return conn.getHbck();
+  }
+
+  @Override
+  public Hbck getHbck(ServerName masterServer) throws IOException {
+    return conn.getHbck(masterServer);
+  }
+
+  @Override
+  public Connection toConnection() {
+    return new SharedConnection(conn.toConnection());
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/SharedConnection.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedConnection.java
similarity index 85%
rename from hbase-server/src/main/java/org/apache/hadoop/hbase/SharedConnection.java
rename to hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedConnection.java
index de0c39b422..f189a2ada6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/SharedConnection.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/SharedConnection.java
@@ -15,22 +15,17 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hbase;
+package org.apache.hadoop.hbase.client;
 
 import java.io.IOException;
 import java.util.concurrent.ExecutorService;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.client.Admin;
-import org.apache.hadoop.hbase.client.BufferedMutator;
-import org.apache.hadoop.hbase.client.BufferedMutatorParams;
-import org.apache.hadoop.hbase.client.Connection;
-import org.apache.hadoop.hbase.client.Hbck;
-import org.apache.hadoop.hbase.client.RegionLocator;
-import org.apache.hadoop.hbase.client.TableBuilder;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.yetus.audience.InterfaceAudience;
 
 /**
- * Wraps a Connection to make it can't be closed or aborted.
+ * Wraps a {@link Connection} to make it can't be closed or aborted.
  */
 @InterfaceAudience.Private
 public class SharedConnection implements Connection {
@@ -105,4 +100,9 @@ public class SharedConnection implements Connection {
   public Hbck getHbck(ServerName masterServer) throws IOException {
     return conn.getHbck(masterServer);
   }
+
+  @Override
+  public AsyncConnection toAsyncConnection() {
+    return new SharedAsyncConnection(conn.toAsyncConnection());
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 0d603e43e3..ad5942d527 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -1852,7 +1852,7 @@ public class HMaster extends HRegionServer implements MasterServices {
         List<NormalizationPlan> plans = this.normalizer.computePlanForTable(table);
         if (plans != null) {
           for (NormalizationPlan plan : plans) {
-            plan.execute(connection.getAdmin());
+            plan.execute(asyncClusterConnection.toConnection().getAdmin());
             if (plan.getType() == PlanType.SPLIT) {
               splitPlanCount++;
             } else if (plan.getType() == PlanType.MERGE) {
@@ -3049,9 +3049,6 @@ public class HMaster extends HRegionServer implements MasterServices {
     // this is what we want especially if the Master is in startup phase doing call outs to
     // hbase:meta, etc. when cluster is down. Without ths connection close, we'd have to wait on
     // the rpc to timeout.
-    if (this.connection != null) {
-      this.connection.close();
-    }
     if (this.asyncClusterConnection != null) {
       this.asyncClusterConnection.close();
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
index 8764143d1b..b49c04f6e2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
@@ -28,12 +28,12 @@ import org.apache.hadoop.hbase.ClusterMetrics;
 import org.apache.hadoop.hbase.MetaMutationAnnotation;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.SharedConnection;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.MasterSwitchType;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.RegionInfo;
+import org.apache.hadoop.hbase.client.SharedConnection;
 import org.apache.hadoop.hbase.client.SnapshotDescription;
 import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.coprocessor.BaseEnvironment;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index d15a11c35e..db468ddd4a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -85,6 +85,7 @@ import org.apache.hadoop.hbase.ZNodeClearer;
 import org.apache.hadoop.hbase.client.AsyncClusterConnection;
 import org.apache.hadoop.hbase.client.ClusterConnectionFactory;
 import org.apache.hadoop.hbase.client.Connection;
+import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.ConnectionUtils;
 import org.apache.hadoop.hbase.client.RegionInfo;
 import org.apache.hadoop.hbase.client.RegionInfoBuilder;
@@ -153,6 +154,7 @@ import org.apache.hadoop.hbase.util.CompressionTest;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.FutureUtils;
 import org.apache.hadoop.hbase.util.HasThread;
 import org.apache.hadoop.hbase.util.JvmPauseMonitor;
 import org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig;
@@ -257,19 +259,6 @@ public class HRegionServer extends HasThread implements
 
   protected HeapMemoryManager hMemManager;
 
-  /**
-   * Connection to be shared by services.
-   * <p/>
-   * Initialized at server startup and closed when server shuts down.
-   * <p/>
-   * Clients must never close it explicitly.
-   * <p/>
-   * Clients hosted by this Server should make use of this connection rather than create their own;
-   * if they create their own, there is no way for the hosting server to shutdown ongoing client
-   * RPCs.
-   */
-  protected Connection connection;
-
   /**
    * The asynchronous cluster connection to be shared by services.
    */
@@ -803,30 +792,8 @@ public class HRegionServer extends HasThread implements
     return conf;
   }
 
-  /**
-   * Create a 'smarter' Connection, one that is capable of by-passing RPC if the request is to the
-   * local server; i.e. a short-circuit Connection. Safe to use going to local or remote server.
-   */
-  private Connection createConnection() throws IOException {
-    // Create a cluster connection that when appropriate, can short-circuit and go directly to the
-    // local server if the request is to the local server bypassing RPC. Can be used for both local
-    // and remote invocations.
-    Connection conn =
-      ConnectionUtils.createShortCircuitConnection(unsetClientZookeeperQuorum(), null,
-        userProvider.getCurrent(), serverName, rpcServices, rpcServices);
-    // This is used to initialize the batch thread pool inside the connection implementation.
-    // When deploy a fresh cluster, we may first use the cluster connection in InitMetaProcedure,
-    // which will be executed inside the PEWorker, and then the batch thread pool will inherit the
-    // thread group of PEWorker, which will be destroy when shutting down the ProcedureExecutor. It
-    // will cause lots of procedure related UTs to fail, so here let's initialize it first, no harm.
-    conn.getTable(TableName.META_TABLE_NAME).close();
-    return conn;
-  }
-
   /**
    * Run test on configured codecs to make sure supporting libs are in place.
-   * @param c
-   * @throws IOException
    */
   private static void checkCodecs(final Configuration c) throws IOException {
     // check to see if the codec list is available:
@@ -848,11 +815,12 @@ public class HRegionServer extends HasThread implements
    * Setup our cluster connection if not already initialized.
    */
   protected final synchronized void setupClusterConnection() throws IOException {
-    if (connection == null) {
-      connection = createConnection();
+    if (asyncClusterConnection == null) {
+      Configuration conf = unsetClientZookeeperQuorum();
+      InetSocketAddress localAddress = new InetSocketAddress(this.rpcServices.isa.getAddress(), 0);
+      User user = userProvider.getCurrent();
       asyncClusterConnection =
-        ClusterConnectionFactory.createAsyncClusterConnection(unsetClientZookeeperQuorum(),
-          new InetSocketAddress(this.rpcServices.isa.getAddress(), 0), userProvider.getCurrent());
+        ClusterConnectionFactory.createAsyncClusterConnection(conf, localAddress, user);
     }
   }
 
@@ -1121,15 +1089,6 @@ public class HRegionServer extends HasThread implements
       LOG.info("stopping server " + this.serverName);
     }
 
-    if (this.connection != null && !connection.isClosed()) {
-      try {
-        this.connection.close();
-      } catch (IOException e) {
-        // Although the {@link Closeable} interface throws an {@link
-        // IOException}, in reality, the implementation would never do that.
-        LOG.warn("Attempt to close server's short circuit ClusterConnection failed.", e);
-      }
-    }
     if (this.asyncClusterConnection != null) {
       try {
         this.asyncClusterConnection.close();
@@ -2195,7 +2154,7 @@ public class HRegionServer extends HasThread implements
 
   @Override
   public Connection getConnection() {
-    return this.connection;
+    return getAsyncConnection().toConnection();
   }
 
   @Override
@@ -2298,8 +2257,8 @@ public class HRegionServer extends HasThread implements
           }
         } else {
           try {
-            MetaTableAccessor.updateRegionLocation(connection,
-              hris[0], serverName, openSeqNum, masterSystemTime);
+            MetaTableAccessor.updateRegionLocation(asyncClusterConnection.toConnection(), hris[0],
+              serverName, openSeqNum, masterSystemTime);
           } catch (IOException e) {
             LOG.info("Failed to update meta", e);
             return false;
@@ -2326,7 +2285,7 @@ public class HRegionServer extends HasThread implements
     // Keep looping till we get an error. We want to send reports even though server is going down.
     // Only go down if clusterConnection is null. It is set to null almost as last thing as the
     // HRegionServer does down.
-    while (this.connection != null && !this.connection.isClosed()) {
+    while (this.asyncClusterConnection != null && !this.asyncClusterConnection.isClosed()) {
       RegionServerStatusService.BlockingInterface rss = rssStub;
       try {
         if (rss == null) {
@@ -3779,7 +3738,7 @@ public class HRegionServer extends HasThread implements
 
   @Override
   public void unassign(byte[] regionName) throws IOException {
-    connection.getAdmin().unassign(regionName, false);
+    FutureUtils.get(asyncClusterConnection.getAdmin().unassign(regionName, false));
   }
 
   @Override
@@ -3828,8 +3787,7 @@ public class HRegionServer extends HasThread implements
   @Override
   public Connection createConnection(Configuration conf) throws IOException {
     User user = UserProvider.instantiate(conf).getCurrent();
-    return ConnectionUtils.createShortCircuitConnection(conf, null, user, this.serverName,
-        this.rpcServices, this.rpcServices);
+    return ConnectionFactory.createConnection(conf, null, user);
   }
 
   public void executeProcedure(long procId, RSProcedureCallable callable) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
index 16fd33201d..1506ed5815 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
@@ -40,7 +40,6 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.RawCellBuilder;
 import org.apache.hadoop.hbase.RawCellBuilderFactory;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.SharedConnection;
 import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.Delete;
@@ -52,6 +51,7 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.RegionInfo;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.SharedConnection;
 import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.coprocessor.BaseEnvironment;
 import org.apache.hadoop.hbase.coprocessor.BulkLoadObserver;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
index 42a4e00e8e..f15312ae10 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
@@ -25,8 +25,8 @@ import com.google.protobuf.Service;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.SharedConnection;
 import org.apache.hadoop.hbase.client.Connection;
+import org.apache.hadoop.hbase.client.SharedConnection;
 import org.apache.hadoop.hbase.coprocessor.BaseEnvironment;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorServiceBackwardCompatiblity;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MultiHConnection.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MultiHConnection.java
deleted file mode 100644
index d095fa361d..0000000000
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MultiHConnection.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.util;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.client.Connection;
-import org.apache.hadoop.hbase.client.ConnectionFactory;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.coprocessor.Batch;
-import org.apache.yetus.audience.InterfaceAudience;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Provides ability to create multiple Connection instances and allows to process a batch of
- * actions using CHTable.doBatchWithCallback()
- */
-@InterfaceAudience.Private
-public class MultiHConnection {
-  private static final Logger LOG = LoggerFactory.getLogger(MultiHConnection.class);
-  private Connection[] connections;
-  private final Object connectionsLock =  new Object();
-  private final int noOfConnections;
-  private ExecutorService batchPool;
-
-  /**
-   * Create multiple Connection instances and initialize a thread pool executor
-   * @param conf configuration
-   * @param noOfConnections total no of Connections to create
-   * @throws IOException if IO failure occurs
-   */
-  public MultiHConnection(Configuration conf, int noOfConnections)
-      throws IOException {
-    this.noOfConnections = noOfConnections;
-    synchronized (this.connectionsLock) {
-      connections = new Connection[noOfConnections];
-      for (int i = 0; i < noOfConnections; i++) {
-        Connection conn = ConnectionFactory.createConnection(conf);
-        connections[i] = conn;
-      }
-    }
-    createBatchPool(conf);
-  }
-
-  /**
-   * Close the open connections and shutdown the batchpool
-   */
-  public void close() {
-    synchronized (connectionsLock) {
-      if (connections != null) {
-        for (Connection conn : connections) {
-          if (conn != null) {
-            try {
-              conn.close();
-            } catch (IOException e) {
-              LOG.info("Got exception in closing connection", e);
-            } finally {
-              conn = null;
-            }
-          }
-        }
-        connections = null;
-      }
-    }
-    if (this.batchPool != null && !this.batchPool.isShutdown()) {
-      this.batchPool.shutdown();
-      try {
-        if (!this.batchPool.awaitTermination(10, TimeUnit.SECONDS)) {
-          this.batchPool.shutdownNow();
-        }
-      } catch (InterruptedException e) {
-        this.batchPool.shutdownNow();
-      }
-    }
-
-  }
-
-  /**
-   * Randomly pick a connection and process the batch of actions for a given table
-   * @param actions the actions
-   * @param tableName table name
-   * @param results the results array
-   * @param callback to run when results are in
-   * @throws IOException If IO failure occurs
-   */
-  public <R> void processBatchCallback(List<? extends Row> actions, TableName tableName,
-      Object[] results, Batch.Callback<R> callback) throws IOException {
-    // Currently used by RegionStateStore
-    HTable.doBatchWithCallback(actions, results, callback,
-      connections[ThreadLocalRandom.current().nextInt(noOfConnections)], batchPool, tableName);
-  }
-
-  // Copied from ConnectionImplementation.getBatchPool()
-  // We should get rid of this when Connection.processBatchCallback is un-deprecated and provides
-  // an API to manage a batch pool
-  private void createBatchPool(Configuration conf) {
-    // Use the same config for keep alive as in ConnectionImplementation.getBatchPool();
-    int maxThreads = conf.getInt("hbase.multihconnection.threads.max", 256);
-    if (maxThreads == 0) {
-      maxThreads = Runtime.getRuntime().availableProcessors() * 8;
-    }
-    long keepAliveTime = conf.getLong("hbase.multihconnection.threads.keepalivetime", 60);
-    LinkedBlockingQueue<Runnable> workQueue =
-        new LinkedBlockingQueue<>(maxThreads
-            * conf.getInt(HConstants.HBASE_CLIENT_MAX_TOTAL_TASKS,
-              HConstants.DEFAULT_HBASE_CLIENT_MAX_TOTAL_TASKS));
-    ThreadPoolExecutor tpe =
-        new ThreadPoolExecutor(maxThreads, maxThreads, keepAliveTime, TimeUnit.SECONDS, workQueue,
-            Threads.newDaemonThreadFactory("MultiHConnection" + "-shared-"));
-    tpe.allowCoreThreadTimeOut(true);
-    this.batchPool = tpe;
-  }
-  
-}
diff --git a/hbase-server/src/main/resources/hbase-webapps/master/table.jsp b/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
index 4e2688671b..bec374c6d9 100644
--- a/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
+++ b/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
@@ -31,6 +31,7 @@
   import="java.util.TreeMap"
   import="org.apache.commons.lang3.StringEscapeUtils"
   import="org.apache.hadoop.conf.Configuration"
+  import="org.apache.hadoop.hbase.HTableDescriptor"
   import="org.apache.hadoop.hbase.HColumnDescriptor"
   import="org.apache.hadoop.hbase.HConstants"
   import="org.apache.hadoop.hbase.HRegionLocation"
@@ -131,7 +132,7 @@
 if ( fqtn != null ) {
   try {
   table = master.getConnection().getTable(TableName.valueOf(fqtn));
-  if (table.getTableDescriptor().getRegionReplication() > 1) {
+  if (table.getDescriptor().getRegionReplication() > 1) {
     tableHeader = "<h2>Table Regions</h2><table id=\"tableRegionTable\" class=\"tablesorter table table-striped\" style=\"table-layout: fixed; word-wrap: break-word;\"><thead><tr><th>Name</th><th>Region Server</th><th>ReadRequests</th><th>WriteRequests</th><th>StorefileSize</th><th>Num.Storefiles</th><th>MemSize</th><th>Locality</th><th>Start Key</th><th>End Key</th><th>ReplicaID</th></tr></thead>";
     withReplica = true;
   } else {
@@ -365,7 +366,7 @@ if ( fqtn != null ) {
       <th></th>
   </tr>
   <%
-    Collection<HColumnDescriptor> families = table.getTableDescriptor().getFamilies();
+    Collection<HColumnDescriptor> families = new HTableDescriptor(table.getDescriptor()).getFamilies();
     for (HColumnDescriptor family: families) {
   %>
   <tr>
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 74d168cc49..c3c15a8871 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -63,7 +63,9 @@ import org.apache.hadoop.hbase.ClusterMetrics.Option;
 import org.apache.hadoop.hbase.Waiter.ExplainingPredicate;
 import org.apache.hadoop.hbase.Waiter.Predicate;
 import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.AsyncClusterConnection;
 import org.apache.hadoop.hbase.client.BufferedMutator;
+import org.apache.hadoop.hbase.client.ClusterConnectionFactory;
 import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;
 import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;
 import org.apache.hadoop.hbase.client.Connection;
@@ -118,6 +120,7 @@ import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.RegionServerStoppedException;
 import org.apache.hadoop.hbase.security.HBaseKerberosUtils;
 import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache;
 import org.apache.hadoop.hbase.trace.TraceUtil;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -154,6 +157,8 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.slf4j.impl.Log4jLoggerAdapter;
 
+import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
+
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 
 /**
@@ -211,10 +216,7 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
     * HBaseTestingUtility*/
   private Path dataTestDirOnTestFS = null;
 
-  /**
-   * Shared cluster connection.
-   */
-  private volatile Connection connection;
+  private volatile AsyncClusterConnection asyncConnection;
 
   /** Filesystem URI used for map-reduce mini-cluster setup */
   private static String FS_URI;
@@ -1206,9 +1208,9 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
       hbaseAdmin.close();
       hbaseAdmin = null;
     }
-    if (this.connection != null) {
-      this.connection.close();
-      this.connection = null;
+    if (this.asyncConnection != null) {
+      this.asyncConnection.close();
+      this.asyncConnection = null;
     }
     this.hbaseCluster = new MiniHBaseCluster(this.conf, 1, servers, ports, null, null);
     // Don't leave here till we've done a successful scan of the hbase:meta
@@ -1289,14 +1291,7 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
 
   // close hbase admin, close current connection and reset MIN MAX configs for RS.
   private void cleanup() throws IOException {
-    if (hbaseAdmin != null) {
-      hbaseAdmin.close();
-      hbaseAdmin = null;
-    }
-    if (this.connection != null) {
-      this.connection.close();
-      this.connection = null;
-    }
+    closeConnection();
     // unset the configuration for MIN and MAX RS to start
     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1);
     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, -1);
@@ -3021,17 +3016,35 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
     return hbaseCluster;
   }
 
+  private void initConnection() throws IOException {
+    User user = UserProvider.instantiate(conf).getCurrent();
+    this.asyncConnection = ClusterConnectionFactory.createAsyncClusterConnection(conf, null, user);
+  }
+
   /**
-   * Get a Connection to the cluster.
-   * Not thread-safe (This class needs a lot of work to make it thread-safe).
+   * Get a Connection to the cluster. Not thread-safe (This class needs a lot of work to make it
+   * thread-safe).
    * @return A Connection that can be shared. Don't close. Will be closed on shutdown of cluster.
-   * @throws IOException
    */
   public Connection getConnection() throws IOException {
-    if (this.connection == null) {
-      this.connection = ConnectionFactory.createConnection(this.conf);
+    if (this.asyncConnection == null) {
+      initConnection();
     }
-    return this.connection;
+    return this.asyncConnection.toConnection();
+  }
+
+  public AsyncClusterConnection getAsyncConnection() throws IOException {
+    if (this.asyncConnection == null) {
+      initConnection();
+    }
+    return this.asyncConnection;
+  }
+
+  public void closeConnection() throws IOException {
+    Closeables.close(hbaseAdmin, true);
+    Closeables.close(asyncConnection, true);
+    this.hbaseAdmin = null;
+    this.asyncConnection = null;
   }
 
   /**
@@ -3203,36 +3216,30 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
    * Wait until all regions in a table have been assigned
    * @param table Table to wait on.
    * @param timeoutMillis Timeout.
-   * @throws InterruptedException
-   * @throws IOException
    */
   public void waitTableAvailable(byte[] table, long timeoutMillis)
-  throws InterruptedException, IOException {
+      throws InterruptedException, IOException {
     waitFor(timeoutMillis, predicateTableAvailable(TableName.valueOf(table)));
   }
 
   public String explainTableAvailability(TableName tableName) throws IOException {
     String msg = explainTableState(tableName, TableState.State.ENABLED) + ", ";
     if (getHBaseCluster().getMaster().isAlive()) {
-      Map<RegionInfo, ServerName> assignments =
-          getHBaseCluster().getMaster().getAssignmentManager().getRegionStates()
-              .getRegionAssignments();
+      Map<RegionInfo, ServerName> assignments = getHBaseCluster().getMaster().getAssignmentManager()
+        .getRegionStates().getRegionAssignments();
       final List<Pair<RegionInfo, ServerName>> metaLocations =
-          MetaTableAccessor.getTableRegionsAndLocations(connection, tableName);
+        MetaTableAccessor.getTableRegionsAndLocations(asyncConnection.toConnection(), tableName);
       for (Pair<RegionInfo, ServerName> metaLocation : metaLocations) {
         RegionInfo hri = metaLocation.getFirst();
         ServerName sn = metaLocation.getSecond();
         if (!assignments.containsKey(hri)) {
-          msg += ", region " + hri
-              + " not assigned, but found in meta, it expected to be on " + sn;
+          msg += ", region " + hri + " not assigned, but found in meta, it expected to be on " + sn;
 
         } else if (sn == null) {
-          msg += ",  region " + hri
-              + " assigned,  but has no server in meta";
+          msg += ",  region " + hri + " assigned,  but has no server in meta";
         } else if (!sn.equals(assignments.get(hri))) {
-          msg += ",  region " + hri
-              + " assigned,  but has different servers in meta and AM ( " +
-              sn + " <> " + assignments.get(hri);
+          msg += ",  region " + hri + " assigned,  but has different servers in meta and AM ( " +
+            sn + " <> " + assignments.get(hri);
         }
       }
     }
@@ -3241,10 +3248,10 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
 
   public String explainTableState(final TableName table, TableState.State state)
       throws IOException {
-    TableState tableState = MetaTableAccessor.getTableState(connection, table);
+    TableState tableState = MetaTableAccessor.getTableState(asyncConnection.toConnection(), table);
     if (tableState == null) {
-      return "TableState in META: No table state in META for table " + table
-          + " last state in meta (including deleted is " + findLastTableState(table) + ")";
+      return "TableState in META: No table state in META for table " + table +
+        " last state in meta (including deleted is " + findLastTableState(table) + ")";
     } else if (!tableState.inStates(state)) {
       return "TableState in META: Not " + state + " state, but " + tableState;
     } else {
@@ -3258,18 +3265,18 @@ public class HBaseTestingUtility extends HBaseZKTestingUtility {
     MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {
       @Override
       public boolean visit(Result r) throws IOException {
-        if (!Arrays.equals(r.getRow(), table.getName()))
+        if (!Arrays.equals(r.getRow(), table.getName())) {
           return false;
+        }
         TableState state = MetaTableAccessor.getTableState(r);
-        if (state != null)
+        if (state != null) {
           lastTableState.set(state);
+        }
         return true;
       }
     };
-    MetaTableAccessor
-        .scanMeta(connection, null, null,
-            MetaTableAccessor.QueryType.TABLE,
-            Integer.MAX_VALUE, visitor);
+    MetaTableAccessor.scanMeta(asyncConnection.toConnection(), null, null,
+      MetaTableAccessor.QueryType.TABLE, Integer.MAX_VALUE, visitor);
     return lastTableState.get();
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
index cc62cbb239..9cde97aecf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
@@ -29,7 +29,6 @@ import java.util.ArrayList;
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Set;
-import org.apache.hadoop.hbase.client.ClientScanner;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.RegionInfo;
@@ -569,7 +568,6 @@ public class TestPartialResultsFromClientSide {
   /**
    * @param resultSizeRowLimit The row limit that will be enforced through maxResultSize
    * @param cachingRowLimit The row limit that will be enforced through caching
-   * @throws Exception
    */
   public void testPartialResultsAndCaching(int resultSizeRowLimit, int cachingRowLimit)
       throws Exception {
@@ -585,19 +583,16 @@ public class TestPartialResultsFromClientSide {
     scan.setMaxResultSize(maxResultSize);
     scan.setCaching(cachingRowLimit);
 
-    ResultScanner scanner = TABLE.getScanner(scan);
-    ClientScanner clientScanner = (ClientScanner) scanner;
-    Result r = null;
-
-    // Approximate the number of rows we expect will fit into the specified max rsult size. If this
-    // approximation is less than caching, then we expect that the max result size limit will be
-    // hit before the caching limit and thus partial results may be seen
-    boolean expectToSeePartialResults = resultSizeRowLimit < cachingRowLimit;
-    while ((r = clientScanner.next()) != null) {
-      assertTrue(!r.mayHaveMoreCellsInRow() || expectToSeePartialResults);
+    try (ResultScanner scanner = TABLE.getScanner(scan)) {
+      Result r = null;
+      // Approximate the number of rows we expect will fit into the specified max rsult size. If
+      // this approximation is less than caching, then we expect that the max result size limit will
+      // be hit before the caching limit and thus partial results may be seen
+      boolean expectToSeePartialResults = resultSizeRowLimit < cachingRowLimit;
+      while ((r = scanner.next()) != null) {
+        assertTrue(!r.mayHaveMoreCellsInRow() || expectToSeePartialResults);
+      }
     }
-
-    scanner.close();
   }
 
   /**
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIOperationTimeout.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIOperationTimeout.java
index d1091084c0..d1f0e1aa1b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIOperationTimeout.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIOperationTimeout.java
@@ -28,7 +28,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
- * Based class for testing operation timeout logic for {@link ConnectionImplementation}.
+ * Based class for testing operation timeout logic.
  */
 public abstract class AbstractTestCIOperationTimeout extends AbstractTestCITimeout {
 
@@ -73,7 +73,7 @@ public abstract class AbstractTestCIOperationTimeout extends AbstractTestCITimeo
       SleepAndFailFirstTime.ct.set(0);
       execute(table);
       fail("We expect an exception here");
-    } catch (SocketTimeoutException | RetriesExhaustedWithDetailsException e) {
+    } catch (SocketTimeoutException | RetriesExhaustedException e) {
       // The client has a CallTimeout class, but it's not shared. We're not very clean today,
       // in the general case you can expect the call to stop, but the exception may vary.
       // In this test however, we're sure that it will be a socket timeout.
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIRpcTimeout.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIRpcTimeout.java
index 89696cf2ab..aedb8148dd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIRpcTimeout.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCIRpcTimeout.java
@@ -29,7 +29,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
- * Based class for testing rpc timeout logic for {@link ConnectionImplementation}.
+ * Based class for testing rpc timeout logic.
  */
 public abstract class AbstractTestCIRpcTimeout extends AbstractTestCITimeout {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCITimeout.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCITimeout.java
index 49e0f56692..33e7fe4ace 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCITimeout.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/AbstractTestCITimeout.java
@@ -38,7 +38,7 @@ import org.junit.Rule;
 import org.junit.rules.TestName;
 
 /**
- * Based class for testing timeout logic for {@link ConnectionImplementation}.
+ * Based class for testing timeout logic.
  */
 public abstract class AbstractTestCITimeout {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/DummyAsyncClusterConnection.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/DummyAsyncClusterConnection.java
index d5fc58ec4a..95afa64ac0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/DummyAsyncClusterConnection.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/DummyAsyncClusterConnection.java
@@ -152,4 +152,9 @@ public class DummyAsyncClusterConnection implements AsyncClusterConnection {
   public CompletableFuture<Void> cleanupBulkLoad(TableName tableName, String bulkToken) {
     return null;
   }
+
+  @Override
+  public Connection toConnection() {
+    return null;
+  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
index 11958872e3..09cdbd23e3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
@@ -51,6 +51,7 @@ import org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HStore;
 import org.apache.hadoop.hbase.regionserver.HStoreFile;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -59,7 +60,6 @@ import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
 import org.junit.After;
 import org.junit.AfterClass;
-import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
 import org.junit.Rule;
@@ -69,6 +69,8 @@ import org.junit.rules.TestName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
+
 import org.apache.hadoop.hbase.shaded.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.MergeTableRegionsRequest;
 
@@ -77,16 +79,16 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.MergeTable
  * Spins up the minicluster once at test start and then takes it down afterward.
  * Add any testing of HBaseAdmin functionality here.
  */
-@Category({LargeTests.class, ClientTests.class})
+@Category({ LargeTests.class, ClientTests.class })
 public class TestAdmin1 {
 
   @ClassRule
-  public static final HBaseClassTestRule CLASS_RULE =
-      HBaseClassTestRule.forClass(TestAdmin1.class);
+  public static final HBaseClassTestRule CLASS_RULE = HBaseClassTestRule.forClass(TestAdmin1.class);
 
   private static final Logger LOG = LoggerFactory.getLogger(TestAdmin1.class);
-  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-  private Admin admin;
+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static ConnectionImplementation CONN;
+  private static Admin ADMIN;
 
   @Rule
   public TestName name = new TestName();
@@ -98,21 +100,20 @@ public class TestAdmin1 {
     TEST_UTIL.getConfiguration().setInt("hbase.client.retries.number", 6);
     TEST_UTIL.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", true);
     TEST_UTIL.startMiniCluster(3);
+    ADMIN = TEST_UTIL.getAdmin();
+    CONN = ConnectionFactory.createConnectionImpl(TEST_UTIL.getConfiguration(), null,
+      UserProvider.instantiate(TEST_UTIL.getConfiguration()).getCurrent());
   }
 
   @AfterClass
   public static void tearDownAfterClass() throws Exception {
+    Closeables.close(CONN, true);
     TEST_UTIL.shutdownMiniCluster();
   }
 
-  @Before
-  public void setUp() throws Exception {
-    this.admin = TEST_UTIL.getAdmin();
-  }
-
   @After
   public void tearDown() throws Exception {
-    for (HTableDescriptor htd : this.admin.listTables()) {
+    for (HTableDescriptor htd : ADMIN.listTables()) {
       TEST_UTIL.deleteTable(htd.getTableName());
     }
   }
@@ -122,7 +123,7 @@ public class TestAdmin1 {
     final TableName unknowntable = TableName.valueOf(name.getMethodName());
     Exception exception = null;
     try {
-      this.admin.compact(unknowntable);
+      ADMIN.compact(unknowntable);
     } catch (IOException e) {
       exception = e;
     }
@@ -130,7 +131,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.flush(unknowntable);
+      ADMIN.flush(unknowntable);
     } catch (IOException e) {
       exception = e;
     }
@@ -138,7 +139,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.split(unknowntable);
+      ADMIN.split(unknowntable);
     } catch (IOException e) {
       exception = e;
     }
@@ -153,7 +154,7 @@ public class TestAdmin1 {
     HColumnDescriptor nonexistentHcd = new HColumnDescriptor(nonexistentColumn);
     Exception exception = null;
     try {
-      this.admin.addColumnFamily(nonexistentTable, nonexistentHcd);
+      ADMIN.addColumnFamily(nonexistentTable, nonexistentHcd);
     } catch (IOException e) {
       exception = e;
     }
@@ -161,7 +162,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.deleteTable(nonexistentTable);
+      ADMIN.deleteTable(nonexistentTable);
     } catch (IOException e) {
       exception = e;
     }
@@ -169,7 +170,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.deleteColumnFamily(nonexistentTable, nonexistentColumn);
+      ADMIN.deleteColumnFamily(nonexistentTable, nonexistentColumn);
     } catch (IOException e) {
       exception = e;
     }
@@ -177,7 +178,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.disableTable(nonexistentTable);
+      ADMIN.disableTable(nonexistentTable);
     } catch (IOException e) {
       exception = e;
     }
@@ -185,7 +186,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.enableTable(nonexistentTable);
+      ADMIN.enableTable(nonexistentTable);
     } catch (IOException e) {
       exception = e;
     }
@@ -193,7 +194,7 @@ public class TestAdmin1 {
 
     exception = null;
     try {
-      this.admin.modifyColumnFamily(nonexistentTable, nonexistentHcd);
+      ADMIN.modifyColumnFamily(nonexistentTable, nonexistentHcd);
     } catch (IOException e) {
       exception = e;
     }
@@ -203,7 +204,7 @@ public class TestAdmin1 {
     try {
       HTableDescriptor htd = new HTableDescriptor(nonexistentTable);
       htd.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-      this.admin.modifyTable(htd.getTableName(), htd);
+      ADMIN.modifyTable(htd.getTableName(), htd);
     } catch (IOException e) {
       exception = e;
     }
@@ -214,11 +215,11 @@ public class TestAdmin1 {
     final TableName tableName = TableName.valueOf(name.getMethodName() + System.currentTimeMillis());
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(new HColumnDescriptor("cf"));
-    this.admin.createTable(htd);
+    ADMIN.createTable(htd);
     try {
       exception = null;
       try {
-        this.admin.deleteColumnFamily(htd.getTableName(), nonexistentHcd.getName());
+        ADMIN.deleteColumnFamily(htd.getTableName(), nonexistentHcd.getName());
       } catch (IOException e) {
         exception = e;
       }
@@ -227,15 +228,15 @@ public class TestAdmin1 {
 
       exception = null;
       try {
-        this.admin.modifyColumnFamily(htd.getTableName(), nonexistentHcd);
+        ADMIN.modifyColumnFamily(htd.getTableName(), nonexistentHcd);
       } catch (IOException e) {
         exception = e;
       }
       assertTrue("found=" + exception.getClass().getName(),
           exception instanceof InvalidFamilyOperationException);
     } finally {
-      this.admin.disableTable(tableName);
-      this.admin.deleteTable(tableName);
+      ADMIN.disableTable(tableName);
+      ADMIN.deleteTable(tableName);
     }
   }
 
@@ -253,7 +254,7 @@ public class TestAdmin1 {
     get.addColumn(HConstants.CATALOG_FAMILY, qualifier);
     ht.get(get);
 
-    this.admin.disableTable(ht.getName());
+    ADMIN.disableTable(ht.getName());
     assertTrue("Table must be disabled.", TEST_UTIL.getHBaseCluster()
         .getMaster().getTableStateManager().isTableState(
             ht.getName(), TableState.State.DISABLED));
@@ -281,7 +282,7 @@ public class TestAdmin1 {
       ok = true;
     }
     assertTrue(ok);
-    this.admin.enableTable(table);
+    ADMIN.enableTable(table);
     assertTrue("Table must be enabled.", TEST_UTIL.getHBaseCluster()
         .getMaster().getTableStateManager().isTableState(
             ht.getName(), TableState.State.ENABLED));
@@ -322,7 +323,7 @@ public class TestAdmin1 {
     ht1.get(get);
     ht2.get(get);
 
-    this.admin.disableTables("testDisableAndEnableTable.*");
+    ADMIN.disableTables("testDisableAndEnableTable.*");
 
     // Test that tables are disabled
     get = new Get(row);
@@ -340,7 +341,7 @@ public class TestAdmin1 {
 
 
     assertTrue(ok);
-    this.admin.enableTables("testDisableAndEnableTable.*");
+    ADMIN.enableTables("testDisableAndEnableTable.*");
 
     // Test that tables are enabled
     try {
@@ -364,11 +365,11 @@ public class TestAdmin1 {
 
   @Test
   public void testCreateTable() throws IOException {
-    HTableDescriptor [] tables = admin.listTables();
+    HTableDescriptor [] tables = ADMIN.listTables();
     int numTables = tables.length;
     final TableName tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
-    tables = this.admin.listTables();
+    tables = ADMIN.listTables();
     assertEquals(numTables + 1, tables.length);
     assertTrue("Table must be enabled.",
         TEST_UTIL.getHBaseCluster().getMaster().getTableStateManager()
@@ -403,8 +404,8 @@ public class TestAdmin1 {
     assertEquals(3, TEST_UTIL.getHBaseCluster().getRegions(tableName).size());
 
     // Truncate & Verify
-    this.admin.disableTable(tableName);
-    this.admin.truncateTable(tableName, preserveSplits);
+    ADMIN.disableTable(tableName);
+    ADMIN.truncateTable(tableName, preserveSplits);
     table = TEST_UTIL.getConnection().getTable(tableName);
     try {
       assertEquals(0, TEST_UTIL.countRows(table));
@@ -427,7 +428,7 @@ public class TestAdmin1 {
     htd.addFamily(fam1);
     htd.addFamily(fam2);
     htd.addFamily(fam3);
-    this.admin.createTable(htd);
+    ADMIN.createTable(htd);
     Table table = TEST_UTIL.getConnection().getTable(htd.getTableName());
     TableDescriptor confirmedHtd = table.getDescriptor();
     assertEquals(0, TableDescriptor.COMPARATOR.compare(htd, confirmedHtd));
@@ -441,19 +442,19 @@ public class TestAdmin1 {
     final TableName tableName = TableName.valueOf(name.getMethodName());
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(fam1);
-    this.admin.createTable(htd);
+    ADMIN.createTable(htd);
     Table table = TEST_UTIL.getConnection().getTable(htd.getTableName());
-    long ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    long ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     assertEquals(0, ts);
     Put p = new Put(Bytes.toBytes("row1"));
     p.addColumn(Bytes.toBytes("fam1"), Bytes.toBytes("fam1"), Bytes.toBytes("fam1"));
     table.put(p);
-    ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     // no files written -> no data
     assertEquals(0, ts);
 
-    this.admin.flush(tableName);
-    ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    ADMIN.flush(tableName);
+    ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     // still 0, we flushed a file, but no major compaction happened
     assertEquals(0, ts);
 
@@ -461,30 +462,30 @@ public class TestAdmin1 {
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
       regionName = l.getAllRegionLocations().get(0).getRegionInfo().getRegionName();
     }
-    long ts1 = this.admin.getLastMajorCompactionTimestampForRegion(regionName);
+    long ts1 = ADMIN.getLastMajorCompactionTimestampForRegion(regionName);
     assertEquals(ts, ts1);
     p = new Put(Bytes.toBytes("row2"));
     p.addColumn(Bytes.toBytes("fam1"), Bytes.toBytes("fam1"), Bytes.toBytes("fam1"));
     table.put(p);
-    this.admin.flush(tableName);
-    ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    ADMIN.flush(tableName);
+    ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     // make sure the region API returns the same value, as the old file is still around
     assertEquals(ts1, ts);
 
     TEST_UTIL.compact(tableName, true);
     table.put(p);
     // forces a wait for the compaction
-    this.admin.flush(tableName);
-    ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    ADMIN.flush(tableName);
+    ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     // after a compaction our earliest timestamp will have progressed forward
     assertTrue(ts > ts1);
 
     // region api still the same
-    ts1 = this.admin.getLastMajorCompactionTimestampForRegion(regionName);
+    ts1 = ADMIN.getLastMajorCompactionTimestampForRegion(regionName);
     assertEquals(ts, ts1);
     table.put(p);
-    this.admin.flush(tableName);
-    ts = this.admin.getLastMajorCompactionTimestamp(tableName);
+    ADMIN.flush(tableName);
+    ts = ADMIN.getLastMajorCompactionTimestamp(tableName);
     assertEquals(ts, ts1);
     table.close();
   }
@@ -509,18 +510,18 @@ public class TestAdmin1 {
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
 
     // Make table read only
-    TableDescriptor htd = TableDescriptorBuilder.newBuilder(this.admin.getDescriptor(tableName))
+    TableDescriptor htd = TableDescriptorBuilder.newBuilder(ADMIN.getDescriptor(tableName))
       .setReadOnly(true).build();
-    admin.modifyTable(htd);
+    ADMIN.modifyTable(htd);
 
     // try to modify the read only table now
-    htd = TableDescriptorBuilder.newBuilder(this.admin.getDescriptor(tableName))
+    htd = TableDescriptorBuilder.newBuilder(ADMIN.getDescriptor(tableName))
       .setCompactionEnabled(false).build();
-    admin.modifyTable(htd);
+    ADMIN.modifyTable(htd);
     // Delete the table
-    this.admin.disableTable(tableName);
-    this.admin.deleteTable(tableName);
-    assertFalse(this.admin.tableExists(tableName));
+    ADMIN.disableTable(tableName);
+    ADMIN.deleteTable(tableName);
+    assertFalse(ADMIN.tableExists(tableName));
   }
 
   @Test(expected = TableNotDisabledException.class)
@@ -529,17 +530,17 @@ public class TestAdmin1 {
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
 
     // Modify region replication count
-    TableDescriptor htd = TableDescriptorBuilder.newBuilder(admin.getDescriptor(tableName))
+    TableDescriptor htd = TableDescriptorBuilder.newBuilder(ADMIN.getDescriptor(tableName))
         .setRegionReplication(3).build();
     try {
       // try to modify the region replication count without disabling the table
-      admin.modifyTable(htd);
+      ADMIN.modifyTable(htd);
       fail("Expected an exception");
     } finally {
       // Delete the table
-      admin.disableTable(tableName);
-      admin.deleteTable(tableName);
-      assertFalse(admin.tableExists(tableName));
+      ADMIN.disableTable(tableName);
+      ADMIN.deleteTable(tableName);
+      assertFalse(ADMIN.tableExists(tableName));
     }
   }
 
@@ -549,14 +550,14 @@ public class TestAdmin1 {
   @Test
   public void testOnlineChangeTableSchema() throws IOException, InterruptedException {
     final TableName tableName = TableName.valueOf(name.getMethodName());
-    HTableDescriptor [] tables = admin.listTables();
+    HTableDescriptor [] tables = ADMIN.listTables();
     int numTables = tables.length;
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
-    tables = this.admin.listTables();
+    tables = ADMIN.listTables();
     assertEquals(numTables + 1, tables.length);
 
     // FIRST, do htabledescriptor changes.
-    HTableDescriptor htd = this.admin.getTableDescriptor(tableName);
+    HTableDescriptor htd = ADMIN.getTableDescriptor(tableName);
     // Make a copy and assert copy is good.
     HTableDescriptor copy = new HTableDescriptor(htd);
     assertTrue(htd.equals(copy));
@@ -571,12 +572,12 @@ public class TestAdmin1 {
     copy.setValue(key, key);
     boolean expectedException = false;
     try {
-      admin.modifyTable(tableName, copy);
+      ADMIN.modifyTable(tableName, copy);
     } catch (TableNotDisabledException re) {
       expectedException = true;
     }
     assertFalse(expectedException);
-    HTableDescriptor modifiedHtd = new HTableDescriptor(this.admin.getTableDescriptor(tableName));
+    HTableDescriptor modifiedHtd = new HTableDescriptor(ADMIN.getTableDescriptor(tableName));
     assertFalse(htd.equals(modifiedHtd));
     assertTrue(copy.equals(modifiedHtd));
     assertEquals(newFlushSize, modifiedHtd.getMemStoreFlushSize());
@@ -592,44 +593,44 @@ public class TestAdmin1 {
     final byte [] hcdName = hcd.getName();
     expectedException = false;
     try {
-      this.admin.modifyColumnFamily(tableName, hcd);
+      ADMIN.modifyColumnFamily(tableName, hcd);
     } catch (TableNotDisabledException re) {
       expectedException = true;
     }
     assertFalse(expectedException);
-    modifiedHtd = this.admin.getTableDescriptor(tableName);
+    modifiedHtd = ADMIN.getTableDescriptor(tableName);
     HColumnDescriptor modifiedHcd = modifiedHtd.getFamily(hcdName);
     assertEquals(newMaxVersions, modifiedHcd.getMaxVersions());
 
     // Try adding a column
-    assertFalse(this.admin.isTableDisabled(tableName));
+    assertFalse(ADMIN.isTableDisabled(tableName));
     final String xtracolName = "xtracol";
     HColumnDescriptor xtracol = new HColumnDescriptor(xtracolName);
     xtracol.setValue(xtracolName, xtracolName);
     expectedException = false;
     try {
-      this.admin.addColumnFamily(tableName, xtracol);
+      ADMIN.addColumnFamily(tableName, xtracol);
     } catch (TableNotDisabledException re) {
       expectedException = true;
     }
     // Add column should work even if the table is enabled
     assertFalse(expectedException);
-    modifiedHtd = this.admin.getTableDescriptor(tableName);
+    modifiedHtd = ADMIN.getTableDescriptor(tableName);
     hcd = modifiedHtd.getFamily(xtracol.getName());
     assertTrue(hcd != null);
     assertTrue(hcd.getValue(xtracolName).equals(xtracolName));
 
     // Delete the just-added column.
-    this.admin.deleteColumnFamily(tableName, xtracol.getName());
-    modifiedHtd = this.admin.getTableDescriptor(tableName);
+    ADMIN.deleteColumnFamily(tableName, xtracol.getName());
+    modifiedHtd = ADMIN.getTableDescriptor(tableName);
     hcd = modifiedHtd.getFamily(xtracol.getName());
     assertTrue(hcd == null);
 
     // Delete the table
-    this.admin.disableTable(tableName);
-    this.admin.deleteTable(tableName);
-    this.admin.listTables();
-    assertFalse(this.admin.tableExists(tableName));
+    ADMIN.disableTable(tableName);
+    ADMIN.deleteTable(tableName);
+    ADMIN.listTables();
+    assertFalse(ADMIN.tableExists(tableName));
   }
 
   private void verifyRoundRobinDistribution(ConnectionImplementation c,
@@ -668,7 +669,7 @@ public class TestAdmin1 {
     final TableName tableName = TableName.valueOf(name.getMethodName());
     HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc);
+    ADMIN.createTable(desc);
     List<HRegionLocation> regions;
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
       regions = l.getAllRegionLocations();
@@ -678,7 +679,7 @@ public class TestAdmin1 {
     TableName TABLE_2 = TableName.valueOf(tableName.getNameAsString() + "_2");
     desc = new HTableDescriptor(TABLE_2);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, new byte[][]{new byte[]{42}});
+    ADMIN.createTable(desc, new byte[][]{new byte[]{42}});
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(TABLE_2)) {
       regions = l.getAllRegionLocations();
       assertEquals("Table should have only 2 region", 2, regions.size());
@@ -687,7 +688,7 @@ public class TestAdmin1 {
     TableName TABLE_3 = TableName.valueOf(tableName.getNameAsString() + "_3");
     desc = new HTableDescriptor(TABLE_3);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, Bytes.toBytes("a"), Bytes.toBytes("z"), 3);
+    ADMIN.createTable(desc, Bytes.toBytes("a"), Bytes.toBytes("z"), 3);
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(TABLE_3)) {
       regions = l.getAllRegionLocations();
       assertEquals("Table should have only 3 region", 3, regions.size());
@@ -697,16 +698,16 @@ public class TestAdmin1 {
     desc = new HTableDescriptor(TABLE_4);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
     try {
-      admin.createTable(desc, Bytes.toBytes("a"), Bytes.toBytes("z"), 2);
+      ADMIN.createTable(desc, Bytes.toBytes("a"), Bytes.toBytes("z"), 2);
       fail("Should not be able to create a table with only 2 regions using this API.");
     } catch (IllegalArgumentException eae) {
-    // Expected
+      // Expected
     }
 
     TableName TABLE_5 = TableName.valueOf(tableName.getNameAsString() + "_5");
     desc = new HTableDescriptor(TABLE_5);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, new byte[] { 1 }, new byte[] { 127 }, 16);
+    ADMIN.createTable(desc, new byte[] { 1 }, new byte[] { 127 }, 16);
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(TABLE_5)) {
       regions = l.getAllRegionLocations();
       assertEquals("Table should have 16 region", 16, regions.size());
@@ -732,15 +733,14 @@ public class TestAdmin1 {
 
     HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, splitKeys);
+    ADMIN.createTable(desc, splitKeys);
 
-    boolean tableAvailable = admin.isTableAvailable(tableName, splitKeys);
+    boolean tableAvailable = ADMIN.isTableAvailable(tableName, splitKeys);
     assertTrue("Table should be created with splitKyes + 1 rows in META", tableAvailable);
 
     List<HRegionLocation> regions;
     Iterator<HRegionLocation> hris;
     RegionInfo hri;
-    ConnectionImplementation conn = (ConnectionImplementation) TEST_UTIL.getConnection();
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
       regions = l.getAllRegionLocations();
 
@@ -780,7 +780,7 @@ public class TestAdmin1 {
       assertTrue(Bytes.equals(hri.getStartKey(), splitKeys[8]));
       assertTrue(hri.getEndKey() == null || hri.getEndKey().length == 0);
 
-      verifyRoundRobinDistribution(conn, l, expectedRegions);
+      verifyRoundRobinDistribution(CONN, l, expectedRegions);
     }
 
 
@@ -800,8 +800,8 @@ public class TestAdmin1 {
 
     desc = new HTableDescriptor(TABLE_2);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin = TEST_UTIL.getAdmin();
-    admin.createTable(desc, startKey, endKey, expectedRegions);
+    ADMIN = TEST_UTIL.getAdmin();
+    ADMIN.createTable(desc, startKey, endKey, expectedRegions);
 
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(TABLE_2)) {
       regions = l.getAllRegionLocations();
@@ -841,7 +841,7 @@ public class TestAdmin1 {
       assertTrue(Bytes.equals(hri.getStartKey(), new byte[] { 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 }));
       assertTrue(hri.getEndKey() == null || hri.getEndKey().length == 0);
 
-      verifyRoundRobinDistribution(conn, l, expectedRegions);
+      verifyRoundRobinDistribution(CONN, l, expectedRegions);
     }
 
     // Try once more with something that divides into something infinite
@@ -855,8 +855,8 @@ public class TestAdmin1 {
 
     desc = new HTableDescriptor(TABLE_3);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin = TEST_UTIL.getAdmin();
-    admin.createTable(desc, startKey, endKey, expectedRegions);
+    ADMIN = TEST_UTIL.getAdmin();
+    ADMIN.createTable(desc, startKey, endKey, expectedRegions);
 
 
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(TABLE_3)) {
@@ -865,7 +865,7 @@ public class TestAdmin1 {
           "but only found " + regions.size(), expectedRegions, regions.size());
       System.err.println("Found " + regions.size() + " regions");
 
-      verifyRoundRobinDistribution(conn, l, expectedRegions);
+      verifyRoundRobinDistribution(CONN, l, expectedRegions);
     }
 
 
@@ -881,7 +881,7 @@ public class TestAdmin1 {
     desc = new HTableDescriptor(TABLE_4);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
     try {
-      admin.createTable(desc, splitKeys);
+      ADMIN.createTable(desc, splitKeys);
       assertTrue("Should not be able to create this table because of " +
           "duplicate split keys", false);
     } catch(IllegalArgumentException iae) {
@@ -899,8 +899,8 @@ public class TestAdmin1 {
         new byte [] { 1, 1, 1 },
         new byte [] { 2, 2, 2 }
     };
-    admin.createTable(desc);
-    boolean tableAvailable = admin.isTableAvailable(tableName, splitKeys);
+    ADMIN.createTable(desc);
+    boolean tableAvailable = ADMIN.isTableAvailable(tableName, splitKeys);
     assertFalse("Table should be created with 1 row in META", tableAvailable);
   }
 
@@ -912,7 +912,7 @@ public class TestAdmin1 {
     HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(tableName));
     desc.addFamily(new HColumnDescriptor("col"));
     try {
-      admin.createTable(desc, splitKeys);
+      ADMIN.createTable(desc, splitKeys);
       fail("Test case should fail as empty split key is passed.");
     } catch (IllegalArgumentException e) {
     }
@@ -928,7 +928,7 @@ public class TestAdmin1 {
     HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(tableName));
     desc.addFamily(new HColumnDescriptor("col"));
     try {
-      admin.createTable(desc, splitKeys);
+      ADMIN.createTable(desc, splitKeys);
       fail("Test case should fail as empty split key is passed.");
     } catch (IllegalArgumentException e) {
       LOG.info("Expected ", e);
@@ -939,10 +939,10 @@ public class TestAdmin1 {
   public void testTableExist() throws IOException {
     final TableName table = TableName.valueOf(name.getMethodName());
     boolean exist;
-    exist = this.admin.tableExists(table);
+    exist = ADMIN.tableExists(table);
     assertEquals(false, exist);
     TEST_UTIL.createTable(table, HConstants.CATALOG_FAMILY);
-    exist = this.admin.tableExists(table);
+    exist = ADMIN.tableExists(table);
     assertEquals(true, exist);
   }
 
@@ -980,7 +980,7 @@ public class TestAdmin1 {
     int expectedRegions = splitKeys.length + 1;
     HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, splitKeys);
+    ADMIN.createTable(desc, splitKeys);
 
     try (RegionLocator l = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
       List<HRegionLocation> regions = l.getAllRegionLocations();
@@ -989,9 +989,9 @@ public class TestAdmin1 {
           "Tried to create " + expectedRegions + " regions " + "but only found " + regions.size(),
           expectedRegions, regions.size());
       // Disable table.
-      admin.disableTable(tableName);
+      ADMIN.disableTable(tableName);
       // Enable table, use retain assignment to assign regions.
-      admin.enableTable(tableName);
+      ADMIN.enableTable(tableName);
       List<HRegionLocation> regions2 = l.getAllRegionLocations();
 
       // Check the assignment.
@@ -1042,7 +1042,7 @@ public class TestAdmin1 {
     for (int i = 0; i < rowCounts.length; i++) {
       sb.append("_").append(Integer.toString(rowCounts[i]));
     }
-    assertFalse(admin.tableExists(tableName));
+    assertFalse(ADMIN.tableExists(tableName));
     try (final Table table = TEST_UTIL.createTable(tableName, familyNames,
       numVersions, blockSize);
       final RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
@@ -1090,7 +1090,7 @@ public class TestAdmin1 {
 
       // Split the table
       if (async) {
-        this.admin.split(tableName, splitPoint);
+        ADMIN.split(tableName, splitPoint);
         final AtomicInteger count = new AtomicInteger(0);
         Thread t = new Thread("CheckForSplit") {
           @Override public void run() {
@@ -1123,7 +1123,7 @@ public class TestAdmin1 {
         t.join();
       } else {
         // Sync split region, no need to create a thread to check
-        ((HBaseAdmin)admin).splitRegionSync(m.get(0).getRegionInfo().getRegionName(), splitPoint);
+        ((HBaseAdmin)ADMIN).splitRegionSync(m.get(0).getRegionInfo().getRegionName(), splitPoint);
       }
 
       // Verify row count
@@ -1284,54 +1284,54 @@ public class TestAdmin1 {
   public void testEnableDisableAddColumnDeleteColumn() throws Exception {
     final TableName tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
-    while (!this.admin.isTableEnabled(TableName.valueOf(name.getMethodName()))) {
+    while (!ADMIN.isTableEnabled(TableName.valueOf(name.getMethodName()))) {
       Thread.sleep(10);
     }
-    this.admin.disableTable(tableName);
+    ADMIN.disableTable(tableName);
     try {
       TEST_UTIL.getConnection().getTable(tableName);
     } catch (org.apache.hadoop.hbase.DoNotRetryIOException e) {
       //expected
     }
 
-    this.admin.addColumnFamily(tableName, new HColumnDescriptor("col2"));
-    this.admin.enableTable(tableName);
+    ADMIN.addColumnFamily(tableName, new HColumnDescriptor("col2"));
+    ADMIN.enableTable(tableName);
     try {
-      this.admin.deleteColumnFamily(tableName, Bytes.toBytes("col2"));
+      ADMIN.deleteColumnFamily(tableName, Bytes.toBytes("col2"));
     } catch (TableNotDisabledException e) {
       LOG.info(e.toString(), e);
     }
-    this.admin.disableTable(tableName);
-    this.admin.deleteTable(tableName);
+    ADMIN.disableTable(tableName);
+    ADMIN.deleteTable(tableName);
   }
 
   @Test
   public void testDeleteLastColumnFamily() throws Exception {
     final TableName tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.createTable(tableName, HConstants.CATALOG_FAMILY).close();
-    while (!this.admin.isTableEnabled(TableName.valueOf(name.getMethodName()))) {
+    while (!ADMIN.isTableEnabled(TableName.valueOf(name.getMethodName()))) {
       Thread.sleep(10);
     }
 
     // test for enabled table
     try {
-      this.admin.deleteColumnFamily(tableName, HConstants.CATALOG_FAMILY);
+      ADMIN.deleteColumnFamily(tableName, HConstants.CATALOG_FAMILY);
       fail("Should have failed to delete the only column family of a table");
     } catch (InvalidFamilyOperationException ex) {
       // expected
     }
 
     // test for disabled table
-    this.admin.disableTable(tableName);
+    ADMIN.disableTable(tableName);
 
     try {
-      this.admin.deleteColumnFamily(tableName, HConstants.CATALOG_FAMILY);
+      ADMIN.deleteColumnFamily(tableName, HConstants.CATALOG_FAMILY);
       fail("Should have failed to delete the only column family of a table");
     } catch (InvalidFamilyOperationException ex) {
       // expected
     }
 
-    this.admin.deleteTable(tableName);
+    ADMIN.deleteTable(tableName);
   }
 
   /*
@@ -1362,7 +1362,7 @@ public class TestAdmin1 {
     puts.add(p);
     try {
       table.put(puts);
-      admin.flush(tableName);
+      ADMIN.flush(tableName);
 
       List<HRegion> regions = TEST_UTIL.getMiniHBaseCluster().getRegions(tableName);
       for (HRegion r : regions) {
@@ -1382,9 +1382,9 @@ public class TestAdmin1 {
         }
       }
     } finally {
-      if (admin.isTableEnabled(tableName)) {
-        this.admin.disableTable(tableName);
-        this.admin.deleteTable(tableName);
+      if (ADMIN.isTableEnabled(tableName)) {
+        ADMIN.disableTable(tableName);
+        ADMIN.deleteTable(tableName);
       }
     }
   }
@@ -1407,29 +1407,29 @@ public class TestAdmin1 {
       RegionInfo regionB;
 
       // merge with full name
-      tableRegions = admin.getRegions(tableName);
-      assertEquals(3, admin.getTableRegions(tableName).size());
+      tableRegions = ADMIN.getRegions(tableName);
+      assertEquals(3, ADMIN.getTableRegions(tableName).size());
       regionA = tableRegions.get(0);
       regionB = tableRegions.get(1);
       // TODO convert this to version that is synchronous (See HBASE-16668)
-      admin.mergeRegionsAsync(regionA.getRegionName(), regionB.getRegionName(), false)
+      ADMIN.mergeRegionsAsync(regionA.getRegionName(), regionB.getRegionName(), false)
           .get(60, TimeUnit.SECONDS);
 
-      assertEquals(2, admin.getTableRegions(tableName).size());
+      assertEquals(2, ADMIN.getTableRegions(tableName).size());
 
       // merge with encoded name
-      tableRegions = admin.getRegions(tableName);
+      tableRegions = ADMIN.getRegions(tableName);
       regionA = tableRegions.get(0);
       regionB = tableRegions.get(1);
       // TODO convert this to version that is synchronous (See HBASE-16668)
-      admin.mergeRegionsAsync(
+      ADMIN.mergeRegionsAsync(
         regionA.getEncodedNameAsBytes(), regionB.getEncodedNameAsBytes(), false)
           .get(60, TimeUnit.SECONDS);
 
-      assertEquals(1, admin.getTableRegions(tableName).size());
+      assertEquals(1, ADMIN.getTableRegions(tableName).size());
     } finally {
-      this.admin.disableTable(tableName);
-      this.admin.deleteTable(tableName);
+      ADMIN.disableTable(tableName);
+      ADMIN.deleteTable(tableName);
     }
   }
 
@@ -1448,16 +1448,16 @@ public class TestAdmin1 {
       p.addColumn(Bytes.toBytes("f"), q1, v1);
       table.put(p);
     }
-    this.admin.flush(tableName);
+    ADMIN.flush(tableName);
     try {
-      this.admin.split(tableName, Bytes.toBytes("row5"));
+      ADMIN.split(tableName, Bytes.toBytes("row5"));
       Threads.sleep(10000);
     } catch (Exception e) {
       // Nothing to do.
     }
     // Split should not happen.
     List<RegionInfo> allRegions = MetaTableAccessor.getTableRegions(
-        this.admin.getConnection(), tableName, true);
+        ADMIN.getConnection(), tableName, true);
     assertEquals(1, allRegions.size());
   }
 
@@ -1499,15 +1499,15 @@ public class TestAdmin1 {
             .setTimeToLive(TTL)
             .build()
         ).build();
-    admin.createTable(tableDesc, splitKeys);
+    ADMIN.createTable(tableDesc, splitKeys);
 
     assertEquals(NUM_REGIONS, TEST_UTIL.getHBaseCluster().getRegions(tableName).size());
     assertTrue("Table should be created with splitKyes + 1 rows in META",
-        admin.isTableAvailable(tableName, splitKeys));
+        ADMIN.isTableAvailable(tableName, splitKeys));
 
     // clone & Verify
-    admin.cloneTableSchema(tableName, newTableName, preserveSplits);
-    TableDescriptor newTableDesc = admin.getDescriptor(newTableName);
+    ADMIN.cloneTableSchema(tableName, newTableName, preserveSplits);
+    TableDescriptor newTableDesc = ADMIN.getDescriptor(newTableName);
 
     assertEquals(NUM_FAMILYS, newTableDesc.getColumnFamilyCount());
     assertEquals(BLOCK_SIZE, newTableDesc.getColumnFamily(FAMILY_1).getBlocksize());
@@ -1518,7 +1518,7 @@ public class TestAdmin1 {
     if (preserveSplits) {
       assertEquals(NUM_REGIONS, TEST_UTIL.getHBaseCluster().getRegions(newTableName).size());
       assertTrue("New table should be created with splitKyes + 1 rows in META",
-          admin.isTableAvailable(newTableName, splitKeys));
+          ADMIN.isTableAvailable(newTableName, splitKeys));
     } else {
       assertEquals(1, TEST_UTIL.getHBaseCluster().getRegions(newTableName).size());
     }
@@ -1530,7 +1530,7 @@ public class TestAdmin1 {
     final TableName newTableName = TableName.valueOf(tableName.getNameAsString() + "_new");
     // test for non-existent source table
     try {
-      admin.cloneTableSchema(tableName, newTableName, false);
+      ADMIN.cloneTableSchema(tableName, newTableName, false);
       fail("Should have failed to create a new table by cloning non-existent source table.");
     } catch (TableNotFoundException ex) {
       // expected
@@ -1546,7 +1546,7 @@ public class TestAdmin1 {
     TEST_UTIL.createTable(newTableName, FAMILY_0);
     // test for existent destination table
     try {
-      admin.cloneTableSchema(tableName, newTableName, false);
+      ADMIN.cloneTableSchema(tableName, newTableName, false);
       fail("Should have failed to create a existent table.");
     } catch (TableExistsException ex) {
       // expected
@@ -1561,15 +1561,15 @@ public class TestAdmin1 {
         .setRegionReplication(5)
         .build();
 
-    admin.createTable(desc);
+    ADMIN.createTable(desc);
 
     int maxFileSize = 10000000;
     TableDescriptor newDesc = TableDescriptorBuilder.newBuilder(desc)
         .setMaxFileSize(maxFileSize)
         .build();
 
-    admin.modifyTable(newDesc);
-    TableDescriptor newTableDesc = admin.getDescriptor(tableName);
+    ADMIN.modifyTable(newDesc);
+    TableDescriptor newTableDesc = ADMIN.getDescriptor(tableName);
     assertEquals(maxFileSize, newTableDesc.getMaxFileSize());
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
index 19dfd30fee..e327753839 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
@@ -55,6 +55,7 @@ import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -63,7 +64,6 @@ import org.apache.hadoop.hbase.wal.AbstractFSWALProvider;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Assert;
-import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
 import org.junit.Rule;
@@ -73,6 +73,8 @@ import org.junit.rules.TestName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
+
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 
 /**
@@ -88,8 +90,9 @@ public class TestAdmin2 {
       HBaseClassTestRule.forClass(TestAdmin2.class);
 
   private static final Logger LOG = LoggerFactory.getLogger(TestAdmin2.class);
-  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-  private Admin admin;
+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static ConnectionImplementation CONN;
+  private static Admin ADMIN;
 
   @Rule
   public TestName name = new TestName();
@@ -103,21 +106,20 @@ public class TestAdmin2 {
     TEST_UTIL.getConfiguration().setInt(HConstants.REGION_SERVER_HANDLER_COUNT, 30);
     TEST_UTIL.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", true);
     TEST_UTIL.startMiniCluster(3);
+    CONN = ConnectionFactory.createConnectionImpl(TEST_UTIL.getConfiguration(), null,
+      UserProvider.instantiate(TEST_UTIL.getConfiguration()).getCurrent());
+    ADMIN = TEST_UTIL.getAdmin();
   }
 
   @AfterClass
   public static void tearDownAfterClass() throws Exception {
+    Closeables.close(CONN, true);
     TEST_UTIL.shutdownMiniCluster();
   }
 
-  @Before
-  public void setUp() throws Exception {
-    this.admin = TEST_UTIL.getHBaseAdmin();
-  }
-
   @After
   public void tearDown() throws Exception {
-    for (HTableDescriptor htd : this.admin.listTables()) {
+    for (HTableDescriptor htd : ADMIN.listTables()) {
       TEST_UTIL.deleteTable(htd.getTableName());
     }
   }
@@ -126,7 +128,7 @@ public class TestAdmin2 {
   public void testCreateBadTables() throws IOException {
     String msg = null;
     try {
-      this.admin.createTable(new HTableDescriptor(TableName.META_TABLE_NAME));
+      ADMIN.createTable(new HTableDescriptor(TableName.META_TABLE_NAME));
     } catch(TableExistsException e) {
       msg = e.toString();
     }
@@ -141,7 +143,7 @@ public class TestAdmin2 {
     Thread [] threads = new Thread [count];
     final AtomicInteger successes = new AtomicInteger(0);
     final AtomicInteger failures = new AtomicInteger(0);
-    final Admin localAdmin = this.admin;
+    final Admin localAdmin = ADMIN;
     for (int i = 0; i < count; i++) {
       threads[i] = new Thread(Integer.toString(i)) {
         @Override
@@ -186,8 +188,8 @@ public class TestAdmin2 {
     HTableDescriptor htd2 = new HTableDescriptor(TableName.valueOf(name));
     htd1.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
     htd2.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(htd1);
-    admin.createTable(htd2);
+    ADMIN.createTable(htd1);
+    ADMIN.createTable(htd2);
     // Before fix, below would fail throwing a NoServerForRegionException.
     TEST_UTIL.getConnection().getTable(htd2.getTableName()).close();
   }
@@ -284,28 +286,27 @@ public class TestAdmin2 {
   public void testTableNotEnabledExceptionWithATable() throws IOException {
     final TableName name = TableName.valueOf(this.name.getMethodName());
     TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY).close();
-    this.admin.disableTable(name);
-    this.admin.disableTable(name);
+    ADMIN.disableTable(name);
+    ADMIN.disableTable(name);
   }
 
   /**
    * Can't enable a table if the table isn't in disabled state
    * @throws IOException
    */
-  @Test (expected=TableNotDisabledException.class)
+  @Test(expected = TableNotDisabledException.class)
   public void testTableNotDisabledExceptionWithATable() throws IOException {
     final TableName name = TableName.valueOf(this.name.getMethodName());
     Table t = TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);
     try {
-    this.admin.enableTable(name);
-    }finally {
-       t.close();
+      ADMIN.enableTable(name);
+    } finally {
+      t.close();
     }
   }
 
   /**
    * For HADOOP-2579
-   * @throws IOException
    */
   @Test (expected=TableNotFoundException.class)
   public void testTableNotFoundExceptionWithoutAnyTables() throws IOException {
@@ -326,7 +327,7 @@ public class TestAdmin2 {
     for (RegionInfo regionInfo : onlineRegions) {
       if (!regionInfo.getTable().isSystemTable()) {
         info = regionInfo;
-        admin.unassign(regionInfo.getRegionName(), true);
+        ADMIN.unassign(regionInfo.getRegionName(), true);
       }
     }
     boolean isInList = ProtobufUtil.getOnlineRegions(
@@ -356,7 +357,7 @@ public class TestAdmin2 {
         if (regionInfo.getRegionNameAsString().contains(name)) {
           info = regionInfo;
           try {
-            admin.unassign(Bytes.toBytes("sample"), true);
+            ADMIN.unassign(Bytes.toBytes("sample"), true);
           } catch (UnknownRegionException nsre) {
             // expected, ignore it
           }
@@ -380,7 +381,7 @@ public class TestAdmin2 {
       if (!regionInfo.isMetaRegion()) {
         if (regionInfo.getRegionNameAsString().contains("TestHBACloseRegion2")) {
           info = regionInfo;
-          admin.unassign(regionInfo.getRegionName(), true);
+          ADMIN.unassign(regionInfo.getRegionName(), true);
         }
       }
     }
@@ -418,7 +419,7 @@ public class TestAdmin2 {
     HColumnDescriptor hcd = new HColumnDescriptor("value");
     htd.addFamily(hcd);
 
-    admin.createTable(htd, null);
+    ADMIN.createTable(htd, null);
   }
 
   /**
@@ -438,9 +439,9 @@ public class TestAdmin2 {
 
     HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc, startKey, endKey, expectedRegions);
+    ADMIN.createTable(desc, startKey, endKey, expectedRegions);
 
-    List<RegionInfo> RegionInfos = admin.getRegions(tableName);
+    List<RegionInfo> RegionInfos = ADMIN.getRegions(tableName);
 
     assertEquals("Tried to create " + expectedRegions + " regions " +
         "but only found " + RegionInfos.size(),
@@ -479,7 +480,7 @@ public class TestAdmin2 {
     for (HRegion r : regionServer.getOnlineRegionsLocalContext()) {
       r.flush(true);
     }
-    admin.rollWALWriter(regionServer.getServerName());
+    ADMIN.rollWALWriter(regionServer.getServerName());
     int count = AbstractFSWALProvider.getNumRolledLogFiles(regionServer.getWAL(null));
     LOG.info("after flushing all regions and rolling logs there are " +
         count + " log files");
@@ -536,7 +537,7 @@ public class TestAdmin2 {
     // Create the test table and open it
     HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
-    admin.createTable(desc);
+    ADMIN.createTable(desc);
     Table table = TEST_UTIL.getConnection().getTable(tableName);
 
     HRegionServer regionServer = TEST_UTIL.getRSForFirstRegionInTable(tableName);
@@ -585,7 +586,7 @@ public class TestAdmin2 {
   @Test
   public void testDisableCatalogTable() throws Exception {
     try {
-      this.admin.disableTable(TableName.META_TABLE_NAME);
+      ADMIN.disableTable(TableName.META_TABLE_NAME);
       fail("Expected to throw ConstraintException");
     } catch (ConstraintException e) {
     }
@@ -601,13 +602,13 @@ public class TestAdmin2 {
   @Test
   public void testIsEnabledOrDisabledOnUnknownTable() throws Exception {
     try {
-      admin.isTableEnabled(TableName.valueOf(name.getMethodName()));
+      ADMIN.isTableEnabled(TableName.valueOf(name.getMethodName()));
       fail("Test should fail if isTableEnabled called on unknown table.");
     } catch (IOException e) {
     }
 
     try {
-      admin.isTableDisabled(TableName.valueOf(name.getMethodName()));
+      ADMIN.isTableDisabled(TableName.valueOf(name.getMethodName()));
       fail("Test should fail if isTableDisabled called on unknown table.");
     } catch (IOException e) {
     }
@@ -636,46 +637,46 @@ public class TestAdmin2 {
 
   @Test
   public void testBalancer() throws Exception {
-    boolean initialState = admin.isBalancerEnabled();
+    boolean initialState = ADMIN.isBalancerEnabled();
 
     // Start the balancer, wait for it.
-    boolean prevState = admin.setBalancerRunning(!initialState, true);
+    boolean prevState = ADMIN.setBalancerRunning(!initialState, true);
 
     // The previous state should be the original state we observed
     assertEquals(initialState, prevState);
 
     // Current state should be opposite of the original
-    assertEquals(!initialState, admin.isBalancerEnabled());
+    assertEquals(!initialState, ADMIN.isBalancerEnabled());
 
     // Reset it back to what it was
-    prevState = admin.setBalancerRunning(initialState, true);
+    prevState = ADMIN.setBalancerRunning(initialState, true);
 
     // The previous state should be the opposite of the initial state
     assertEquals(!initialState, prevState);
     // Current state should be the original state again
-    assertEquals(initialState, admin.isBalancerEnabled());
+    assertEquals(initialState, ADMIN.isBalancerEnabled());
   }
 
   @Test
   public void testRegionNormalizer() throws Exception {
-    boolean initialState = admin.isNormalizerEnabled();
+    boolean initialState = ADMIN.isNormalizerEnabled();
 
     // flip state
-    boolean prevState = admin.setNormalizerRunning(!initialState);
+    boolean prevState = ADMIN.setNormalizerRunning(!initialState);
 
     // The previous state should be the original state we observed
     assertEquals(initialState, prevState);
 
     // Current state should be opposite of the original
-    assertEquals(!initialState, admin.isNormalizerEnabled());
+    assertEquals(!initialState, ADMIN.isNormalizerEnabled());
 
     // Reset it back to what it was
-    prevState = admin.setNormalizerRunning(initialState);
+    prevState = ADMIN.setNormalizerRunning(initialState);
 
     // The previous state should be the opposite of the initial state
     assertEquals(!initialState, prevState);
     // Current state should be the original state again
-    assertEquals(initialState, admin.isNormalizerEnabled());
+    assertEquals(initialState, ADMIN.isNormalizerEnabled());
   }
 
   @Test
@@ -683,32 +684,32 @@ public class TestAdmin2 {
     Random randomGenerator = new Random();
     long procId = randomGenerator.nextLong();
 
-    boolean abortResult = admin.abortProcedure(procId, true);
+    boolean abortResult = ADMIN.abortProcedure(procId, true);
     assertFalse(abortResult);
   }
 
   @Test
   public void testGetProcedures() throws Exception {
-    String procList = admin.getProcedures();
+    String procList = ADMIN.getProcedures();
     assertTrue(procList.startsWith("["));
   }
 
   @Test
   public void testGetLocks() throws Exception {
-    String lockList = admin.getLocks();
+    String lockList = ADMIN.getLocks();
     assertTrue(lockList.startsWith("["));
   }
 
   @Test
   public void testDecommissionRegionServers() throws Exception {
-    List<ServerName> decommissionedRegionServers = admin.listDecommissionedRegionServers();
+    List<ServerName> decommissionedRegionServers = ADMIN.listDecommissionedRegionServers();
     assertTrue(decommissionedRegionServers.isEmpty());
 
     final TableName tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.createMultiRegionTable(tableName, Bytes.toBytes("f"), 6);
 
     ArrayList<ServerName> clusterRegionServers =
-        new ArrayList<>(admin.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS))
+        new ArrayList<>(ADMIN.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS))
           .getLiveServerMetrics().keySet());
 
     assertEquals(3, clusterRegionServers.size());
@@ -718,8 +719,8 @@ public class TestAdmin2 {
     // leaving one online.
     int i;
     for (i = 0; i < clusterRegionServers.size(); i++) {
-      List<RegionInfo> regionsOnServer = admin.getRegions(clusterRegionServers.get(i));
-      if (admin.getRegions(clusterRegionServers.get(i)).stream().anyMatch(p -> p.isMetaRegion())) {
+      List<RegionInfo> regionsOnServer = ADMIN.getRegions(clusterRegionServers.get(i));
+      if (ADMIN.getRegions(clusterRegionServers.get(i)).stream().anyMatch(p -> p.isMetaRegion())) {
         serversToDecommssion.put(clusterRegionServers.get(i), regionsOnServer);
         break;
       }
@@ -728,13 +729,13 @@ public class TestAdmin2 {
     clusterRegionServers.remove(i);
     // Get another server to decommission.
     serversToDecommssion.put(clusterRegionServers.get(0),
-      admin.getRegions(clusterRegionServers.get(0)));
+      ADMIN.getRegions(clusterRegionServers.get(0)));
 
     ServerName remainingServer = clusterRegionServers.get(1);
 
     // Decommission
-    admin.decommissionRegionServers(new ArrayList<ServerName>(serversToDecommssion.keySet()), true);
-    assertEquals(2, admin.listDecommissionedRegionServers().size());
+    ADMIN.decommissionRegionServers(new ArrayList<ServerName>(serversToDecommssion.keySet()), true);
+    assertEquals(2, ADMIN.listDecommissionedRegionServers().size());
 
     // Verify the regions have been off the decommissioned servers, all on the one
     // remaining server.
@@ -748,9 +749,9 @@ public class TestAdmin2 {
     for (ServerName server : serversToDecommssion.keySet()) {
       List<byte[]> encodedRegionNames = serversToDecommssion.get(server).stream()
           .map(region -> region.getEncodedNameAsBytes()).collect(Collectors.toList());
-      admin.recommissionRegionServer(server, encodedRegionNames);
+      ADMIN.recommissionRegionServer(server, encodedRegionNames);
     }
-    assertTrue(admin.listDecommissionedRegionServers().isEmpty());
+    assertTrue(ADMIN.listDecommissionedRegionServers().isEmpty());
     // Verify the regions have been moved to the recommissioned servers
     for (ServerName server : serversToDecommssion.keySet()) {
       for (RegionInfo region : serversToDecommssion.get(server)) {
@@ -770,7 +771,7 @@ public class TestAdmin2 {
       table.put(new Put(Bytes.toBytes(i)).addColumn(Bytes.toBytes("f"), Bytes.toBytes("q"),
         Bytes.toBytes(i)));
     }
-    admin.flush(tableName);
+    ADMIN.flush(tableName);
 
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(table.getName());
     List<HRegion> regions = rs.getRegions(tableName);
@@ -782,12 +783,10 @@ public class TestAdmin2 {
     long expectedStoreFilesSize = store.getStorefilesSize();
     Assert.assertNotNull(store);
     Assert.assertEquals(expectedStoreFilesSize, store.getSize());
-
-    ConnectionImplementation conn = (ConnectionImplementation) admin.getConnection();
-    HBaseRpcController controller = conn.getRpcControllerFactory().newController();
+    HBaseRpcController controller = CONN.getRpcControllerFactory().newController();
     for (int i = 0; i < 10; i++) {
       RegionInfo ri =
-          ProtobufUtil.getRegionInfo(controller, conn.getAdmin(rs.getServerName()), regionName);
+          ProtobufUtil.getRegionInfo(controller, CONN.getAdmin(rs.getServerName()), regionName);
       Assert.assertEquals(region.getRegionInfo(), ri);
 
       // Make sure that the store size is still the actual file system's store size.
@@ -801,26 +800,26 @@ public class TestAdmin2 {
     TEST_UTIL.createTable(tableName, Bytes.toBytes("f"));
 
     // get the original table region count
-    List<RegionInfo> regions = admin.getRegions(tableName);
+    List<RegionInfo> regions = ADMIN.getRegions(tableName);
     int originalCount = regions.size();
     assertEquals(1, originalCount);
 
     // split the table and wait until region count increases
-    admin.split(tableName, Bytes.toBytes(3));
+    ADMIN.split(tableName, Bytes.toBytes(3));
     TEST_UTIL.waitFor(30000, new Predicate<Exception>() {
 
       @Override
       public boolean evaluate() throws Exception {
-        return admin.getRegions(tableName).size() > originalCount;
+        return ADMIN.getRegions(tableName).size() > originalCount;
       }
     });
 
     // do some table modification
-    TableDescriptor tableDesc = TableDescriptorBuilder.newBuilder(admin.getDescriptor(tableName))
+    TableDescriptor tableDesc = TableDescriptorBuilder.newBuilder(ADMIN.getDescriptor(tableName))
         .setMaxFileSize(11111111)
         .build();
-    admin.modifyTable(tableDesc);
-    assertEquals(11111111, admin.getDescriptor(tableName).getMaxFileSize());
+    ADMIN.modifyTable(tableDesc);
+    assertEquals(11111111, ADMIN.getDescriptor(tableName).getMaxFileSize());
   }
 
   @Test
@@ -830,7 +829,7 @@ public class TestAdmin2 {
       new byte[][] { Bytes.toBytes(3) });
 
     // assert we have at least 2 regions in the table
-    List<RegionInfo> regions = admin.getRegions(tableName);
+    List<RegionInfo> regions = ADMIN.getRegions(tableName);
     int originalCount = regions.size();
     assertTrue(originalCount >= 2);
 
@@ -838,20 +837,20 @@ public class TestAdmin2 {
     byte[] nameOfRegionB = regions.get(1).getEncodedNameAsBytes();
 
     // merge the table regions and wait until region count decreases
-    admin.mergeRegionsAsync(nameOfRegionA, nameOfRegionB, true);
+    ADMIN.mergeRegionsAsync(nameOfRegionA, nameOfRegionB, true);
     TEST_UTIL.waitFor(30000, new Predicate<Exception>() {
 
       @Override
       public boolean evaluate() throws Exception {
-        return admin.getRegions(tableName).size() < originalCount;
+        return ADMIN.getRegions(tableName).size() < originalCount;
       }
     });
 
     // do some table modification
-    TableDescriptor tableDesc = TableDescriptorBuilder.newBuilder(admin.getDescriptor(tableName))
+    TableDescriptor tableDesc = TableDescriptorBuilder.newBuilder(ADMIN.getDescriptor(tableName))
         .setMaxFileSize(11111111)
         .build();
-    admin.modifyTable(tableDesc);
-    assertEquals(11111111, admin.getDescriptor(tableName).getMaxFileSize());
+    ADMIN.modifyTable(tableDesc);
+    assertEquals(11111111, ADMIN.getDescriptor(tableName).getMaxFileSize());
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAlwaysSetScannerId.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAlwaysSetScannerId.java
index 2ede1ddc3f..66d3d3d721 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAlwaysSetScannerId.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAlwaysSetScannerId.java
@@ -23,8 +23,9 @@ import static org.junit.Assert.assertTrue;
 import java.io.IOException;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.ipc.BlockingRpcCallback;
+import org.apache.hadoop.hbase.ipc.HBaseRpcControllerImpl;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.testclassification.RegionServerTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -34,6 +35,7 @@ import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
 import org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;
 
 import org.apache.hadoop.hbase.shaded.protobuf.RequestConverter;
@@ -49,7 +51,7 @@ public class TestAlwaysSetScannerId {
 
   @ClassRule
   public static final HBaseClassTestRule CLASS_RULE =
-      HBaseClassTestRule.forClass(TestAlwaysSetScannerId.class);
+    HBaseClassTestRule.forClass(TestAlwaysSetScannerId.class);
 
   private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
 
@@ -61,9 +63,11 @@ public class TestAlwaysSetScannerId {
 
   private static final int COUNT = 10;
 
-  private static HRegionInfo HRI;
+  private static RegionInfo HRI;
 
-  private static ClientProtos.ClientService.BlockingInterface STUB;
+  private static AsyncConnectionImpl CONN;
+
+  private static ClientProtos.ClientService.Interface STUB;
 
   @BeforeClass
   public static void setUp() throws Exception {
@@ -73,39 +77,47 @@ public class TestAlwaysSetScannerId {
         table.put(new Put(Bytes.toBytes(i)).addColumn(CF, CQ, Bytes.toBytes(i)));
       }
     }
-    HRI = UTIL.getAdmin().getTableRegions(TABLE_NAME).get(0);
-    STUB = ((ConnectionImplementation) UTIL.getConnection())
-        .getClient(UTIL.getHBaseCluster().getRegionServer(0).getServerName());
+    HRI = UTIL.getAdmin().getRegions(TABLE_NAME).get(0);
+    CONN =
+      (AsyncConnectionImpl) ConnectionFactory.createAsyncConnection(UTIL.getConfiguration()).get();
+    STUB = CONN.getRegionServerStub(UTIL.getHBaseCluster().getRegionServer(0).getServerName());
   }
 
   @AfterClass
   public static void tearDown() throws Exception {
+    Closeables.close(CONN, true);
     UTIL.shutdownMiniCluster();
   }
 
+  private ScanResponse scan(ScanRequest req) throws IOException {
+    BlockingRpcCallback<ScanResponse> callback = new BlockingRpcCallback<>();
+    STUB.scan(new HBaseRpcControllerImpl(), req, callback);
+    return callback.get();
+  }
+
   @Test
   public void test() throws ServiceException, IOException {
     Scan scan = new Scan();
     ScanRequest req = RequestConverter.buildScanRequest(HRI.getRegionName(), scan, 1, false);
-    ScanResponse resp = STUB.scan(null, req);
+    ScanResponse resp = scan(req);
     assertTrue(resp.hasScannerId());
     long scannerId = resp.getScannerId();
     int nextCallSeq = 0;
     // test next
     for (int i = 0; i < COUNT / 2; i++) {
       req = RequestConverter.buildScanRequest(scannerId, 1, false, nextCallSeq++, false, false, -1);
-      resp = STUB.scan(null, req);
+      resp = scan(req);
       assertTrue(resp.hasScannerId());
       assertEquals(scannerId, resp.getScannerId());
     }
     // test renew
     req = RequestConverter.buildScanRequest(scannerId, 0, false, nextCallSeq++, false, true, -1);
-    resp = STUB.scan(null, req);
+    resp = scan(req);
     assertTrue(resp.hasScannerId());
     assertEquals(scannerId, resp.getScannerId());
     // test close
     req = RequestConverter.buildScanRequest(scannerId, 0, true, false);
-    resp = STUB.scan(null, req);
+    resp = scan(req);
     assertTrue(resp.hasScannerId());
     assertEquals(scannerId, resp.getScannerId());
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncTableAdminApi.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncTableAdminApi.java
index 38f7aabe6b..fba1deb107 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncTableAdminApi.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncTableAdminApi.java
@@ -274,7 +274,7 @@ public class TestAsyncTableAdminApi extends TestAsyncAdminBase {
 
   private void verifyRoundRobinDistribution(List<HRegionLocation> regions, int expectedRegions)
       throws IOException {
-    int numRS = ((ConnectionImplementation) TEST_UTIL.getConnection()).getCurrentNrHRS();
+    int numRS = TEST_UTIL.getMiniHBaseCluster().getRegionServerThreads().size();
 
     Map<ServerName, List<RegionInfo>> server2Regions = new HashMap<>();
     regions.stream().forEach((loc) -> {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCIBadHostname.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCIBadHostname.java
index 83d4bfaf4b..e046afa179 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCIBadHostname.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCIBadHostname.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.junit.AfterClass;
@@ -43,28 +44,29 @@ public class TestCIBadHostname {
   public static final HBaseClassTestRule CLASS_RULE =
       HBaseClassTestRule.forClass(TestCIBadHostname.class);
 
-  private static HBaseTestingUtility testUtil;
-  private static ConnectionImplementation conn;
+  private static HBaseTestingUtility TEST_UTIL;
+  private static ConnectionImplementation CONN;
 
   @BeforeClass
   public static void setupBeforeClass() throws Exception {
-    testUtil = HBaseTestingUtility.createLocalHTU();
-    testUtil.startMiniCluster();
-    conn = (ConnectionImplementation) testUtil.getConnection();
+    TEST_UTIL = HBaseTestingUtility.createLocalHTU();
+    TEST_UTIL.startMiniCluster();
+    CONN = ConnectionFactory.createConnectionImpl(TEST_UTIL.getConfiguration(), null,
+      UserProvider.instantiate(TEST_UTIL.getConfiguration()).getCurrent());
   }
 
   @AfterClass
   public static void teardownAfterClass() throws Exception {
-    conn.close();
-    testUtil.shutdownMiniCluster();
+    CONN.close();
+    TEST_UTIL.shutdownMiniCluster();
   }
 
   @Test(expected = UnknownHostException.class)
   public void testGetAdminBadHostname() throws Exception {
     // verify that we can get an instance with the cluster hostname
-    ServerName master = testUtil.getHBaseCluster().getMaster().getServerName();
+    ServerName master = TEST_UTIL.getHBaseCluster().getMaster().getServerName();
     try {
-      conn.getAdmin(master);
+      CONN.getAdmin(master);
     } catch (UnknownHostException uhe) {
       fail("Obtaining admin to the cluster master should have succeeded");
     }
@@ -74,16 +76,16 @@ public class TestCIBadHostname {
     ServerName badHost =
         ServerName.valueOf("unknownhost.invalid:" + HConstants.DEFAULT_MASTER_PORT,
         System.currentTimeMillis());
-    conn.getAdmin(badHost);
+    CONN.getAdmin(badHost);
     fail("Obtaining admin to unresolvable hostname should have failed");
   }
 
   @Test(expected = UnknownHostException.class)
   public void testGetClientBadHostname() throws Exception {
     // verify that we can get an instance with the cluster hostname
-    ServerName rs = testUtil.getHBaseCluster().getRegionServer(0).getServerName();
+    ServerName rs = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();
     try {
-      conn.getClient(rs);
+      CONN.getClient(rs);
     } catch (UnknownHostException uhe) {
       fail("Obtaining client to the cluster regionserver should have succeeded");
     }
@@ -93,7 +95,7 @@ public class TestCIBadHostname {
     ServerName badHost =
         ServerName.valueOf("unknownhost.invalid:" + HConstants.DEFAULT_REGIONSERVER_PORT,
         System.currentTimeMillis());
-    conn.getAdmin(badHost);
+    CONN.getAdmin(badHost);
     fail("Obtaining client to unresolvable hostname should have failed");
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCISleep.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCISleep.java
index fd0eb7b87a..d91491286a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCISleep.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCISleep.java
@@ -20,13 +20,13 @@ package org.apache.hadoop.hbase.client;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
-import java.net.SocketTimeoutException;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
 import org.apache.hadoop.hbase.ipc.RpcControllerFactory;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.junit.Before;
@@ -81,7 +81,7 @@ public class TestCISleep extends AbstractTestCITimeout {
         // Beacuse 2s + 3s + 2s > 6s
         table.get(new Get(FAM_NAM));
         fail("We expect an exception here");
-      } catch (SocketTimeoutException e) {
+      } catch (RetriesExhaustedException e) {
         LOG.info("We received an exception, as expected ", e);
       }
     }
@@ -93,8 +93,10 @@ public class TestCISleep extends AbstractTestCITimeout {
     long baseTime = 100;
     final TableName tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.createTable(tableName, FAM_NAM);
-    ClientServiceCallable<Object> regionServerCallable =
-      new ClientServiceCallable<Object>((ConnectionImplementation) TEST_UTIL.getConnection(),
+    try (ConnectionImplementation conn =
+      ConnectionFactory.createConnectionImpl(TEST_UTIL.getConfiguration(), null,
+        UserProvider.instantiate(TEST_UTIL.getConfiguration()).getCurrent())) {
+      ClientServiceCallable<Object> regionServerCallable = new ClientServiceCallable<Object>(conn,
         tableName, FAM_NAM, new RpcControllerFactory(TEST_UTIL.getConfiguration()).newController(),
         HConstants.PRIORITY_UNSET) {
         @Override
@@ -103,42 +105,41 @@ public class TestCISleep extends AbstractTestCITimeout {
         }
       };
 
-    regionServerCallable.prepare(false);
-    for (int i = 0; i < HConstants.RETRY_BACKOFF.length; i++) {
-      pauseTime = regionServerCallable.sleep(baseTime, i);
-      assertTrue(pauseTime >= (baseTime * HConstants.RETRY_BACKOFF[i]));
-      assertTrue(pauseTime <= (baseTime * HConstants.RETRY_BACKOFF[i] * 1.01f));
-    }
-
-    RegionAdminServiceCallable<Object> regionAdminServiceCallable =
-      new RegionAdminServiceCallable<Object>((ConnectionImplementation) TEST_UTIL.getConnection(),
-        new RpcControllerFactory(TEST_UTIL.getConfiguration()), tableName, FAM_NAM) {
-        @Override
-        public Object call(HBaseRpcController controller) throws Exception {
-          return null;
-        }
-      };
-
-    regionAdminServiceCallable.prepare(false);
-    for (int i = 0; i < HConstants.RETRY_BACKOFF.length; i++) {
-      pauseTime = regionAdminServiceCallable.sleep(baseTime, i);
-      assertTrue(pauseTime >= (baseTime * HConstants.RETRY_BACKOFF[i]));
-      assertTrue(pauseTime <= (baseTime * HConstants.RETRY_BACKOFF[i] * 1.01f));
-    }
+      regionServerCallable.prepare(false);
+      for (int i = 0; i < HConstants.RETRY_BACKOFF.length; i++) {
+        pauseTime = regionServerCallable.sleep(baseTime, i);
+        assertTrue(pauseTime >= (baseTime * HConstants.RETRY_BACKOFF[i]));
+        assertTrue(pauseTime <= (baseTime * HConstants.RETRY_BACKOFF[i] * 1.01f));
+      }
+      RegionAdminServiceCallable<Object> regionAdminServiceCallable =
+        new RegionAdminServiceCallable<Object>(conn,
+          new RpcControllerFactory(TEST_UTIL.getConfiguration()), tableName, FAM_NAM) {
+          @Override
+          public Object call(HBaseRpcController controller) throws Exception {
+            return null;
+          }
+        };
 
-    try (MasterCallable<Object> masterCallable =
-      new MasterCallable<Object>((ConnectionImplementation) TEST_UTIL.getConnection(),
-        new RpcControllerFactory(TEST_UTIL.getConfiguration())) {
-        @Override
-        protected Object rpcCall() throws Exception {
-          return null;
-        }
-      }) {
+      regionAdminServiceCallable.prepare(false);
       for (int i = 0; i < HConstants.RETRY_BACKOFF.length; i++) {
-        pauseTime = masterCallable.sleep(baseTime, i);
+        pauseTime = regionAdminServiceCallable.sleep(baseTime, i);
         assertTrue(pauseTime >= (baseTime * HConstants.RETRY_BACKOFF[i]));
         assertTrue(pauseTime <= (baseTime * HConstants.RETRY_BACKOFF[i] * 1.01f));
       }
+
+      try (MasterCallable<Object> masterCallable =
+        new MasterCallable<Object>(conn, new RpcControllerFactory(TEST_UTIL.getConfiguration())) {
+          @Override
+          protected Object rpcCall() throws Exception {
+            return null;
+          }
+        }) {
+        for (int i = 0; i < HConstants.RETRY_BACKOFF.length; i++) {
+          pauseTime = masterCallable.sleep(baseTime, i);
+          assertTrue(pauseTime >= (baseTime * HConstants.RETRY_BACKOFF[i]));
+          assertTrue(pauseTime <= (baseTime * HConstants.RETRY_BACKOFF[i] * 1.01f));
+        }
+      }
     }
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCheckAndMutate.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCheckAndMutate.java
index 15ef065d47..53353d2d42 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCheckAndMutate.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCheckAndMutate.java
@@ -17,6 +17,8 @@
  */
 package org.apache.hadoop.hbase.client;
 
+import static org.hamcrest.CoreMatchers.instanceOf;
+import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
@@ -135,18 +137,16 @@ public class TestCheckAndMutate {
       // get row back and assert the values
       getOneRowAndAssertAllButCExist(table);
 
-      //Test that we get a region level exception
+      // Test that we get a region level exception
       try {
         rm = getBogusRowMutations();
         table.checkAndMutate(ROWKEY, FAMILY).qualifier(Bytes.toBytes("A"))
-            .ifEquals(Bytes.toBytes("a")).thenMutate(rm);
+          .ifEquals(Bytes.toBytes("a")).thenMutate(rm);
         fail("Expected NoSuchColumnFamilyException");
-      } catch (RetriesExhaustedWithDetailsException e) {
-        try {
-          throw e.getCause(0);
-        } catch (NoSuchColumnFamilyException e1) {
-          // expected
-        }
+      } catch (NoSuchColumnFamilyException e) {
+        // expected
+      } catch (RetriesExhaustedException e) {
+        assertThat(e.getCause(), instanceOf(NoSuchColumnFamilyException.class));
       }
     }
   }
@@ -168,18 +168,16 @@ public class TestCheckAndMutate {
       // get row back and assert the values
       getOneRowAndAssertAllButCExist(table);
 
-      //Test that we get a region level exception
+      // Test that we get a region level exception
       try {
         rm = getBogusRowMutations();
         table.checkAndMutate(ROWKEY, FAMILY).qualifier(Bytes.toBytes("A"))
-            .ifEquals(Bytes.toBytes("a")).thenMutate(rm);
+          .ifEquals(Bytes.toBytes("a")).thenMutate(rm);
         fail("Expected NoSuchColumnFamilyException");
-      } catch (RetriesExhaustedWithDetailsException e) {
-        try {
-          throw e.getCause(0);
-        } catch (NoSuchColumnFamilyException e1) {
-          // expected
-        }
+      } catch (NoSuchColumnFamilyException e) {
+        // expected
+      } catch (RetriesExhaustedException e) {
+        assertThat(e.getCause(), instanceOf(NoSuchColumnFamilyException.class));
       }
     }
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
index ae217cd820..8a268ff036 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.hbase.client.backoff.ServerStatistics;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -91,121 +92,120 @@ public class TestClientPushback {
   }
 
   @Test
-  public void testClientTracksServerPushback() throws Exception{
+  public void testClientTracksServerPushback() throws Exception {
     Configuration conf = UTIL.getConfiguration();
 
-    ConnectionImplementation conn =
-      (ConnectionImplementation) ConnectionFactory.createConnection(conf);
-    BufferedMutatorImpl mutator = (BufferedMutatorImpl) conn.getBufferedMutator(tableName);
-
-    HRegionServer rs = UTIL.getHBaseCluster().getRegionServer(0);
-    Region region = rs.getRegions(tableName).get(0);
-
-    LOG.debug("Writing some data to "+tableName);
-    // write some data
-    Put p = new Put(Bytes.toBytes("row"));
-    p.addColumn(family, qualifier, Bytes.toBytes("value1"));
-    mutator.mutate(p);
-    mutator.flush();
-
-    // get the current load on RS. Hopefully memstore isn't flushed since we wrote the the data
-    int load = (int) ((region.getMemStoreHeapSize() * 100)
-        / flushSizeBytes);
-    LOG.debug("Done writing some data to "+tableName);
-
-    // get the stats for the region hosting our table
-    ClientBackoffPolicy backoffPolicy = conn.getBackoffPolicy();
-    assertTrue("Backoff policy is not correctly configured",
-      backoffPolicy instanceof ExponentialClientBackoffPolicy);
-
-    ServerStatisticTracker stats = conn.getStatisticsTracker();
-    assertNotNull( "No stats configured for the client!", stats);
-    // get the names so we can query the stats
-    ServerName server = rs.getServerName();
-    byte[] regionName = region.getRegionInfo().getRegionName();
-
-    // check to see we found some load on the memstore
-    ServerStatistics serverStats = stats.getServerStatsForTesting(server);
-    ServerStatistics.RegionStatistics regionStats = serverStats.getStatsForRegion(regionName);
-    assertEquals("We did not find some load on the memstore", load,
-      regionStats.getMemStoreLoadPercent());
-    // check that the load reported produces a nonzero delay
-    long backoffTime = backoffPolicy.getBackoffTime(server, regionName, serverStats);
-    assertNotEquals("Reported load does not produce a backoff", 0, backoffTime);
-    LOG.debug("Backoff calculated for " + region.getRegionInfo().getRegionNameAsString() + " @ " +
-      server + " is " + backoffTime);
-
-    // Reach into the connection and submit work directly to AsyncProcess so we can
-    // monitor how long the submission was delayed via a callback
-    List<Row> ops = new ArrayList<>(1);
-    ops.add(p);
-    final CountDownLatch latch = new CountDownLatch(1);
-    final AtomicLong endTime = new AtomicLong();
-    long startTime = EnvironmentEdgeManager.currentTime();
-    Batch.Callback<Result> callback = (byte[] r, byte[] row, Result result) -> {
+    try (ConnectionImplementation conn = ConnectionFactory.createConnectionImpl(conf, null,
+      UserProvider.instantiate(conf).getCurrent())) {
+      BufferedMutatorImpl mutator = (BufferedMutatorImpl) conn.getBufferedMutator(tableName);
+
+      HRegionServer rs = UTIL.getHBaseCluster().getRegionServer(0);
+      Region region = rs.getRegions(tableName).get(0);
+
+      LOG.debug("Writing some data to " + tableName);
+      // write some data
+      Put p = new Put(Bytes.toBytes("row"));
+      p.addColumn(family, qualifier, Bytes.toBytes("value1"));
+      mutator.mutate(p);
+      mutator.flush();
+
+      // get the current load on RS. Hopefully memstore isn't flushed since we wrote the the data
+      int load = (int) ((region.getMemStoreHeapSize() * 100) / flushSizeBytes);
+      LOG.debug("Done writing some data to " + tableName);
+
+      // get the stats for the region hosting our table
+      ClientBackoffPolicy backoffPolicy = conn.getBackoffPolicy();
+      assertTrue("Backoff policy is not correctly configured",
+        backoffPolicy instanceof ExponentialClientBackoffPolicy);
+
+      ServerStatisticTracker stats = conn.getStatisticsTracker();
+      assertNotNull("No stats configured for the client!", stats);
+      // get the names so we can query the stats
+      ServerName server = rs.getServerName();
+      byte[] regionName = region.getRegionInfo().getRegionName();
+
+      // check to see we found some load on the memstore
+      ServerStatistics serverStats = stats.getServerStatsForTesting(server);
+      ServerStatistics.RegionStatistics regionStats = serverStats.getStatsForRegion(regionName);
+      assertEquals("We did not find some load on the memstore", load,
+        regionStats.getMemStoreLoadPercent());
+      // check that the load reported produces a nonzero delay
+      long backoffTime = backoffPolicy.getBackoffTime(server, regionName, serverStats);
+      assertNotEquals("Reported load does not produce a backoff", 0, backoffTime);
+      LOG.debug("Backoff calculated for " + region.getRegionInfo().getRegionNameAsString() + " @ " +
+        server + " is " + backoffTime);
+
+      // Reach into the connection and submit work directly to AsyncProcess so we can
+      // monitor how long the submission was delayed via a callback
+      List<Row> ops = new ArrayList<>(1);
+      ops.add(p);
+      final CountDownLatch latch = new CountDownLatch(1);
+      final AtomicLong endTime = new AtomicLong();
+      long startTime = EnvironmentEdgeManager.currentTime();
+      Batch.Callback<Result> callback = (byte[] r, byte[] row, Result result) -> {
         endTime.set(EnvironmentEdgeManager.currentTime());
         latch.countDown();
-    };
-    AsyncProcessTask<Result> task = AsyncProcessTask.newBuilder(callback)
-            .setPool(mutator.getPool())
-            .setTableName(tableName)
-            .setRowAccess(ops)
-            .setSubmittedRows(AsyncProcessTask.SubmittedRows.AT_LEAST_ONE)
-            .setOperationTimeout(conn.getConnectionConfiguration().getOperationTimeout())
-            .setRpcTimeout(60 * 1000)
-            .build();
-    mutator.getAsyncProcess().submit(task);
-    // Currently the ExponentialClientBackoffPolicy under these test conditions
-    // produces a backoffTime of 151 milliseconds. This is long enough so the
-    // wait and related checks below are reasonable. Revisit if the backoff
-    // time reported by above debug logging has significantly deviated.
-    String name = server.getServerName() + "," + Bytes.toStringBinary(regionName);
-    MetricsConnection.RegionStats rsStats = conn.getConnectionMetrics().
-            serverStats.get(server).get(regionName);
-    assertEquals(name, rsStats.name);
-    assertEquals(rsStats.heapOccupancyHist.getSnapshot().getMean(),
-        (double)regionStats.getHeapOccupancyPercent(), 0.1 );
-    assertEquals(rsStats.memstoreLoadHist.getSnapshot().getMean(),
-        (double)regionStats.getMemStoreLoadPercent(), 0.1);
-
-    MetricsConnection.RunnerStats runnerStats = conn.getConnectionMetrics().runnerStats;
-
-    assertEquals(1, runnerStats.delayRunners.getCount());
-    assertEquals(1, runnerStats.normalRunners.getCount());
-    assertEquals("", runnerStats.delayIntevalHist.getSnapshot().getMean(),
-      (double)backoffTime, 0.1);
-
-    latch.await(backoffTime * 2, TimeUnit.MILLISECONDS);
-    assertNotEquals("AsyncProcess did not submit the work time", 0, endTime.get());
-    assertTrue("AsyncProcess did not delay long enough", endTime.get() - startTime >= backoffTime);
+      };
+      AsyncProcessTask<Result> task =
+        AsyncProcessTask.newBuilder(callback).setPool(mutator.getPool()).setTableName(tableName)
+          .setRowAccess(ops).setSubmittedRows(AsyncProcessTask.SubmittedRows.AT_LEAST_ONE)
+          .setOperationTimeout(conn.getConnectionConfiguration().getOperationTimeout())
+          .setRpcTimeout(60 * 1000).build();
+      mutator.getAsyncProcess().submit(task);
+      // Currently the ExponentialClientBackoffPolicy under these test conditions
+      // produces a backoffTime of 151 milliseconds. This is long enough so the
+      // wait and related checks below are reasonable. Revisit if the backoff
+      // time reported by above debug logging has significantly deviated.
+      String name = server.getServerName() + "," + Bytes.toStringBinary(regionName);
+      MetricsConnection.RegionStats rsStats =
+        conn.getConnectionMetrics().serverStats.get(server).get(regionName);
+      assertEquals(name, rsStats.name);
+      assertEquals(rsStats.heapOccupancyHist.getSnapshot().getMean(),
+        (double) regionStats.getHeapOccupancyPercent(), 0.1);
+      assertEquals(rsStats.memstoreLoadHist.getSnapshot().getMean(),
+        (double) regionStats.getMemStoreLoadPercent(), 0.1);
+
+      MetricsConnection.RunnerStats runnerStats = conn.getConnectionMetrics().runnerStats;
+
+      assertEquals(1, runnerStats.delayRunners.getCount());
+      assertEquals(1, runnerStats.normalRunners.getCount());
+      assertEquals("", runnerStats.delayIntevalHist.getSnapshot().getMean(), (double) backoffTime,
+        0.1);
+
+      latch.await(backoffTime * 2, TimeUnit.MILLISECONDS);
+      assertNotEquals("AsyncProcess did not submit the work time", 0, endTime.get());
+      assertTrue("AsyncProcess did not delay long enough",
+        endTime.get() - startTime >= backoffTime);
+    }
   }
 
   @Test
   public void testMutateRowStats() throws IOException {
     Configuration conf = UTIL.getConfiguration();
-    ConnectionImplementation conn =
-      (ConnectionImplementation) ConnectionFactory.createConnection(conf);
-    Table table = conn.getTable(tableName);
-    HRegionServer rs = UTIL.getHBaseCluster().getRegionServer(0);
-    Region region = rs.getRegions(tableName).get(0);
-
-    RowMutations mutations = new RowMutations(Bytes.toBytes("row"));
-    Put p = new Put(Bytes.toBytes("row"));
-    p.addColumn(family, qualifier, Bytes.toBytes("value2"));
-    mutations.add(p);
-    table.mutateRow(mutations);
-
-    ServerStatisticTracker stats = conn.getStatisticsTracker();
-    assertNotNull( "No stats configured for the client!", stats);
-    // get the names so we can query the stats
-    ServerName server = rs.getServerName();
-    byte[] regionName = region.getRegionInfo().getRegionName();
-
-    // check to see we found some load on the memstore
-    ServerStatistics serverStats = stats.getServerStatsForTesting(server);
-    ServerStatistics.RegionStatistics regionStats = serverStats.getStatsForRegion(regionName);
-
-    assertNotNull(regionStats);
-    assertTrue(regionStats.getMemStoreLoadPercent() > 0);
+    try (ConnectionImplementation conn = ConnectionFactory.createConnectionImpl(conf, null,
+      UserProvider.instantiate(conf).getCurrent())) {
+      Table table = conn.getTable(tableName);
+      HRegionServer rs = UTIL.getHBaseCluster().getRegionServer(0);
+      Region region = rs.getRegions(tableName).get(0);
+
+      RowMutations mutations = new RowMutations(Bytes.toBytes("row"));
+      Put p = new Put(Bytes.toBytes("row"));
+      p.addColumn(family, qualifier, Bytes.toBytes("value2"));
+      mutations.add(p);
+      table.mutateRow(mutations);
+
+      ServerStatisticTracker stats = conn.getStatisticsTracker();
+      assertNotNull("No stats configured for the client!", stats);
+      // get the names so we can query the stats
+      ServerName server = rs.getServerName();
+      byte[] regionName = region.getRegionInfo().getRegionName();
+
+      // check to see we found some load on the memstore
+      ServerStatistics serverStats = stats.getServerStatsForTesting(server);
+      ServerStatistics.RegionStatistics regionStats = serverStats.getStatsForRegion(regionName);
+
+      assertNotNull(regionStats);
+      assertTrue(regionStats.getMemStoreLoadPercent() > 0);
     }
+  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java
index 76b83feac9..88d0dd06d4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java
@@ -84,7 +84,10 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
 
 /**
  * This class is for testing HBaseConnectionManager features
+ * <p/>
+ * Will be removed in the future, ignore.
  */
+@Ignore
 @Category({LargeTests.class})
 public class TestConnectionImplementation {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index 0b8e2ad81e..868009745e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -17,12 +17,14 @@
  */
 package org.apache.hadoop.hbase.client;
 
+import static org.hamcrest.CoreMatchers.instanceOf;
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
@@ -182,31 +184,23 @@ public class TestFromClientSide {
     // Client will retry beacuse rpc timeout is small than the sleep time of first rpc call
     c.setInt(HConstants.HBASE_RPC_TIMEOUT_KEY, 1500);
 
-    Connection connection = ConnectionFactory.createConnection(c);
-    Table t = connection.getTable(TableName.valueOf(name.getMethodName()));
-    if (t instanceof HTable) {
-      HTable table = (HTable) t;
-      table.setOperationTimeout(3 * 1000);
+    try (Connection connection = ConnectionFactory.createConnection(c);
+        Table table = connection.getTableBuilder(TableName.valueOf(name.getMethodName()), null)
+          .setOperationTimeout(3 * 1000).build()) {
+      Append append = new Append(ROW);
+      append.addColumn(HBaseTestingUtility.fam1, QUALIFIER, VALUE);
+      Result result = table.append(append);
 
-      try {
-        Append append = new Append(ROW);
-        append.addColumn(HBaseTestingUtility.fam1, QUALIFIER, VALUE);
-        Result result = table.append(append);
-
-        // Verify expected result
-        Cell[] cells = result.rawCells();
-        assertEquals(1, cells.length);
-        assertKey(cells[0], ROW, HBaseTestingUtility.fam1, QUALIFIER, VALUE);
-
-        // Verify expected result again
-        Result readResult = table.get(new Get(ROW));
-        cells = readResult.rawCells();
-        assertEquals(1, cells.length);
-        assertKey(cells[0], ROW, HBaseTestingUtility.fam1, QUALIFIER, VALUE);
-      } finally {
-        table.close();
-        connection.close();
-      }
+      // Verify expected result
+      Cell[] cells = result.rawCells();
+      assertEquals(1, cells.length);
+      assertKey(cells[0], ROW, HBaseTestingUtility.fam1, QUALIFIER, VALUE);
+
+      // Verify expected result again
+      Result readResult = table.get(new Get(ROW));
+      cells = readResult.rawCells();
+      assertEquals(1, cells.length);
+      assertKey(cells[0], ROW, HBaseTestingUtility.fam1, QUALIFIER, VALUE);
     }
   }
 
@@ -2220,10 +2214,6 @@ public class TestFromClientSide {
     Table foo = TEST_UTIL.createTable(tableName, new byte[][] {FAMILY}, 10);
 
     int NUM_OPS = 100;
-    int FAILED_OPS = 50;
-
-    RetriesExhaustedWithDetailsException expectedException = null;
-    IllegalArgumentException iae = null;
 
     // 1.1 Put with no column families (local validation, runtime exception)
     List<Put> puts = new ArrayList<Put>(NUM_OPS);
@@ -2234,16 +2224,16 @@ public class TestFromClientSide {
 
     try {
       foo.put(puts);
+      fail();
     } catch (IllegalArgumentException e) {
-      iae = e;
+      // expected
+      assertEquals(NUM_OPS, puts.size());
     }
-    assertNotNull(iae);
-    assertEquals(NUM_OPS, puts.size());
+
 
     // 1.2 Put with invalid column family
-    iae = null;
     puts.clear();
-    for (int i = 0; i != NUM_OPS; i++) {
+    for (int i = 0; i < NUM_OPS; i++) {
       Put put = new Put(Bytes.toBytes(i));
       put.addColumn((i % 2) == 0 ? FAMILY : INVALID_FAMILY, FAMILY, Bytes.toBytes(i));
       puts.add(put);
@@ -2251,47 +2241,46 @@ public class TestFromClientSide {
 
     try {
       foo.put(puts);
-    } catch (RetriesExhaustedWithDetailsException e) {
-      expectedException = e;
+      fail();
+    } catch (RetriesExhaustedException e) {
+      // expected
+      assertThat(e.getCause(), instanceOf(NoSuchColumnFamilyException.class));
     }
-    assertNotNull(expectedException);
-    assertEquals(FAILED_OPS, expectedException.exceptions.size());
-    assertTrue(expectedException.actions.contains(puts.get(1)));
 
     // 2.1 Get non-existent rows
     List<Get> gets = new ArrayList<>(NUM_OPS);
     for (int i = 0; i < NUM_OPS; i++) {
       Get get = new Get(Bytes.toBytes(i));
-      // get.addColumn(FAMILY, FAMILY);
       gets.add(get);
     }
     Result[] getsResult = foo.get(gets);
-
     assertNotNull(getsResult);
     assertEquals(NUM_OPS, getsResult.length);
-    assertNull(getsResult[1].getRow());
+    for (int i = 0; i < NUM_OPS; i++) {
+      Result getResult = getsResult[i];
+      if (i % 2 == 0) {
+        assertFalse(getResult.isEmpty());
+      } else {
+        assertTrue(getResult.isEmpty());
+      }
+    }
 
     // 2.2 Get with invalid column family
     gets.clear();
-    getsResult = null;
-    expectedException = null;
     for (int i = 0; i < NUM_OPS; i++) {
       Get get = new Get(Bytes.toBytes(i));
       get.addColumn((i % 2) == 0 ? FAMILY : INVALID_FAMILY, FAMILY);
       gets.add(get);
     }
     try {
-      getsResult = foo.get(gets);
-    } catch (RetriesExhaustedWithDetailsException e) {
-      expectedException = e;
+      foo.get(gets);
+      fail();
+    } catch (RetriesExhaustedException e) {
+      // expected
+      assertThat(e.getCause(), instanceOf(NoSuchColumnFamilyException.class));
     }
-    assertNull(getsResult);
-    assertNotNull(expectedException);
-    assertEquals(FAILED_OPS, expectedException.exceptions.size());
-    assertTrue(expectedException.actions.contains(gets.get(1)));
 
     // 3.1 Delete with invalid column family
-    expectedException = null;
     List<Delete> deletes = new ArrayList<>(NUM_OPS);
     for (int i = 0; i < NUM_OPS; i++) {
       Delete delete = new Delete(Bytes.toBytes(i));
@@ -2300,14 +2289,24 @@ public class TestFromClientSide {
     }
     try {
       foo.delete(deletes);
-    } catch (RetriesExhaustedWithDetailsException e) {
-      expectedException = e;
+      fail();
+    } catch (RetriesExhaustedException e) {
+      // expected
+      assertThat(e.getCause(), instanceOf(NoSuchColumnFamilyException.class));
     }
-    assertEquals((NUM_OPS - FAILED_OPS), deletes.size());
-    assertNotNull(expectedException);
-    assertEquals(FAILED_OPS, expectedException.exceptions.size());
-    assertTrue(expectedException.actions.contains(deletes.get(1)));
 
+    // all valid rows should have been deleted
+    gets.clear();
+    for (int i = 0; i < NUM_OPS; i++) {
+      Get get = new Get(Bytes.toBytes(i));
+      gets.add(get);
+    }
+    getsResult = foo.get(gets);
+    assertNotNull(getsResult);
+    assertEquals(NUM_OPS, getsResult.length);
+    for (Result getResult : getsResult) {
+      assertTrue(getResult.isEmpty());
+    }
 
     // 3.2 Delete non-existent rows
     deletes.clear();
@@ -2316,58 +2315,6 @@ public class TestFromClientSide {
       deletes.add(delete);
     }
     foo.delete(deletes);
-
-    assertTrue(deletes.isEmpty());
-  }
-
-  /*
-   * Baseline "scalability" test.
-   *
-   * Tests one hundred families, one million columns, one million versions
-   */
-  @Ignore @Test
-  public void testMillions() throws Exception {
-
-    // 100 families
-
-    // millions of columns
-
-    // millions of versions
-
-  }
-
-  @Ignore @Test
-  public void testMultipleRegionsAndBatchPuts() throws Exception {
-    // Two family table
-
-    // Insert lots of rows
-
-    // Insert to the same row with batched puts
-
-    // Insert to multiple rows with batched puts
-
-    // Split the table
-
-    // Get row from first region
-
-    // Get row from second region
-
-    // Scan all rows
-
-    // Insert to multiple regions with batched puts
-
-    // Get row from first region
-
-    // Get row from second region
-
-    // Scan all rows
-
-
-  }
-
-  @Ignore @Test
-  public void testMultipleRowMultipleFamily() throws Exception {
-
   }
 
   //
@@ -4325,7 +4272,7 @@ public class TestFromClientSide {
     // Test user metadata
     Admin admin = TEST_UTIL.getAdmin();
     // make a modifiable descriptor
-    HTableDescriptor desc = new HTableDescriptor(a.getTableDescriptor());
+    HTableDescriptor desc = new HTableDescriptor(a.getDescriptor());
     // offline the table
     admin.disableTable(tableAname);
     // add a user attribute to HTD
@@ -4339,7 +4286,7 @@ public class TestFromClientSide {
     admin.enableTable(tableAname);
 
     // Test that attribute changes were applied
-    desc = a.getTableDescriptor();
+    desc = new HTableDescriptor(a.getDescriptor());
     assertEquals("wrong table descriptor returned", desc.getTableName(), tableAname);
     // check HTD attribute
     value = desc.getValue(attrName);
@@ -4551,6 +4498,8 @@ public class TestFromClientSide {
       arm.add(p);
       t.mutateRow(arm);
       fail("Expected NoSuchColumnFamilyException");
+    } catch (NoSuchColumnFamilyException e) {
+      return;
     } catch(RetriesExhaustedWithDetailsException e) {
       for(Throwable rootCause: e.getCauses()){
         if(rootCause instanceof NoSuchColumnFamilyException){
@@ -4666,10 +4615,11 @@ public class TestFromClientSide {
       for (int j = 0; j != resultWithWal.rawCells().length; ++j) {
         Cell cellWithWal = resultWithWal.rawCells()[j];
         Cell cellWithoutWal = resultWithoutWal.rawCells()[j];
-        assertTrue(Bytes.equals(CellUtil.cloneRow(cellWithWal), CellUtil.cloneRow(cellWithoutWal)));
-        assertTrue(Bytes.equals(CellUtil.cloneFamily(cellWithWal), CellUtil.cloneFamily(cellWithoutWal)));
-        assertTrue(Bytes.equals(CellUtil.cloneQualifier(cellWithWal), CellUtil.cloneQualifier(cellWithoutWal)));
-        assertTrue(Bytes.equals(CellUtil.cloneValue(cellWithWal), CellUtil.cloneValue(cellWithoutWal)));
+        assertArrayEquals(CellUtil.cloneRow(cellWithWal), CellUtil.cloneRow(cellWithoutWal));
+        assertArrayEquals(CellUtil.cloneFamily(cellWithWal), CellUtil.cloneFamily(cellWithoutWal));
+        assertArrayEquals(CellUtil.cloneQualifier(cellWithWal),
+          CellUtil.cloneQualifier(cellWithoutWal));
+        assertArrayEquals(CellUtil.cloneValue(cellWithWal), CellUtil.cloneValue(cellWithoutWal));
       }
     }
   }
@@ -6407,6 +6357,8 @@ public class TestFromClientSide {
     }
   }
 
+  // to be removed
+  @Ignore
   @Test
   public void testRegionCache() throws IOException {
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name.getMethodName()));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
index ff151fe6a2..a6f9169ec2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
@@ -47,6 +47,8 @@ import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.RegionMetrics;
+import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
@@ -55,6 +57,8 @@ import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.coprocessor.RegionObserver;
 import org.apache.hadoop.hbase.exceptions.UnknownProtocolException;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils;
+import org.apache.hadoop.hbase.ipc.RpcClient;
+import org.apache.hadoop.hbase.ipc.RpcClientFactory;
 import org.apache.hadoop.hbase.ipc.ServerRpcController;
 import org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -78,9 +82,6 @@ import org.junit.rules.TestName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos;
-
 @Category({LargeTests.class, ClientTests.class})
 public class TestFromClientSide3 {
 
@@ -151,24 +152,11 @@ public class TestFromClientSide3 {
     table.put(put);
   }
 
-  private void performMultiplePutAndFlush(HBaseAdmin admin, Table table,
-      byte[] row, byte[] family, int nFlushes, int nPuts)
-  throws Exception {
-
-    try (RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(table.getName())) {
-      // connection needed for poll-wait
-      HRegionLocation loc = locator.getRegionLocation(row, true);
-      AdminProtos.AdminService.BlockingInterface server =
-        ((ConnectionImplementation) admin.getConnection()).getAdmin(loc.getServerName());
-      byte[] regName = loc.getRegionInfo().getRegionName();
-
-      for (int i = 0; i < nFlushes; i++) {
-        randomCFPuts(table, row, family, nPuts);
-        List<String> sf = ProtobufUtil.getStoreFiles(server, regName, FAMILY);
-        int sfCount = sf.size();
-
-        admin.flush(table.getName());
-      }
+  private void performMultiplePutAndFlush(HBaseAdmin admin, Table table, byte[] row, byte[] family,
+      int nFlushes, int nPuts) throws Exception {
+    for (int i = 0; i < nFlushes; i++) {
+      randomCFPuts(table, row, family, nPuts);
+      admin.flush(table.getName());
     }
   }
 
@@ -272,6 +260,15 @@ public class TestFromClientSide3 {
     }
   }
 
+  private int getStoreFileCount(Admin admin, ServerName serverName, RegionInfo region) throws IOException {
+    for (RegionMetrics metrics :admin.getRegionMetrics(serverName, region.getTable())) {
+      if (Bytes.equals(region.getRegionName(), metrics.getRegionName())) {
+        return metrics.getStoreFileCount();
+      }
+    }
+    return 0;
+  }
+
   // override the config settings at the CF level and ensure priority
   @Test
   public void testAdvancedConfigOverride() throws Exception {
@@ -289,7 +286,6 @@ public class TestFromClientSide3 {
     final TableName tableName = TableName.valueOf(name.getMethodName());
     Table hTable = TEST_UTIL.createTable(tableName, FAMILY, 10);
     Admin admin = TEST_UTIL.getAdmin();
-    ConnectionImplementation connection = (ConnectionImplementation) TEST_UTIL.getConnection();
 
     // Create 3 store files.
     byte[] row = Bytes.toBytes(random.nextInt());
@@ -298,9 +294,7 @@ public class TestFromClientSide3 {
     try (RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName)) {
       // Verify we have multiple store files.
       HRegionLocation loc = locator.getRegionLocation(row, true);
-      byte[] regionName = loc.getRegionInfo().getRegionName();
-      AdminProtos.AdminService.BlockingInterface server = connection.getAdmin(loc.getServerName());
-      assertTrue(ProtobufUtil.getStoreFiles(server, regionName, FAMILY).size() > 1);
+      assertTrue(getStoreFileCount(admin, loc.getServerName(), loc.getRegion()) > 1);
 
       // Issue a compaction request
       admin.compact(tableName);
@@ -309,21 +303,19 @@ public class TestFromClientSide3 {
       for (int i = 0; i < 10 * 1000 / 40; ++i) {
         // The number of store files after compaction should be lesser.
         loc = locator.getRegionLocation(row, true);
-        if (!loc.getRegionInfo().isOffline()) {
-          regionName = loc.getRegionInfo().getRegionName();
-          server = connection.getAdmin(loc.getServerName());
-          if (ProtobufUtil.getStoreFiles(server, regionName, FAMILY).size() <= 1) {
+        if (!loc.getRegion().isOffline()) {
+          if (getStoreFileCount(admin, loc.getServerName(), loc.getRegion()) <= 1) {
             break;
           }
         }
         Thread.sleep(40);
       }
       // verify the compactions took place and that we didn't just time out
-      assertTrue(ProtobufUtil.getStoreFiles(server, regionName, FAMILY).size() <= 1);
+      assertTrue(getStoreFileCount(admin, loc.getServerName(), loc.getRegion()) <= 1);
 
       // change the compaction.min config option for this table to 5
       LOG.info("hbase.hstore.compaction.min should now be 5");
-      HTableDescriptor htd = new HTableDescriptor(hTable.getTableDescriptor());
+      HTableDescriptor htd = new HTableDescriptor(hTable.getDescriptor());
       htd.setValue("hbase.hstore.compaction.min", String.valueOf(5));
       admin.modifyTable(tableName, htd);
       Pair<Integer, Integer> st;
@@ -342,9 +334,7 @@ public class TestFromClientSide3 {
       // This time, the compaction request should not happen
       Thread.sleep(10 * 1000);
       loc = locator.getRegionLocation(row, true);
-      regionName = loc.getRegionInfo().getRegionName();
-      server = connection.getAdmin(loc.getServerName());
-      int sfCount = ProtobufUtil.getStoreFiles(server, regionName, FAMILY).size();
+      int sfCount = getStoreFileCount(admin, loc.getServerName(), loc.getRegion());
       assertTrue(sfCount > 1);
 
       // change an individual CF's config option to 2 & online schema update
@@ -365,21 +355,19 @@ public class TestFromClientSide3 {
       // poll wait for the compactions to happen
       for (int i = 0; i < 10 * 1000 / 40; ++i) {
         loc = locator.getRegionLocation(row, true);
-        regionName = loc.getRegionInfo().getRegionName();
         try {
-          server = connection.getAdmin(loc.getServerName());
-          if (ProtobufUtil.getStoreFiles(server, regionName, FAMILY).size() < sfCount) {
+          if (getStoreFileCount(admin, loc.getServerName(), loc.getRegion()) < sfCount) {
             break;
           }
         } catch (Exception e) {
-          LOG.debug("Waiting for region to come online: " + Bytes.toString(regionName));
+          LOG.debug("Waiting for region to come online: " +
+            Bytes.toStringBinary(loc.getRegion().getRegionName()));
         }
         Thread.sleep(40);
       }
 
       // verify the compaction took place and that we didn't just time out
-      assertTrue(ProtobufUtil.getStoreFiles(
-        server, regionName, FAMILY).size() < sfCount);
+      assertTrue(getStoreFileCount(admin, loc.getServerName(), loc.getRegion()) < sfCount);
 
       // Finally, ensure that we can remove a custom config value after we made it
       LOG.info("Removing CF config value");
@@ -393,8 +381,8 @@ public class TestFromClientSide3 {
         Thread.sleep(40);
       }
       LOG.info("alter status finished");
-      assertNull(hTable.getTableDescriptor().getFamily(FAMILY).getValue(
-          "hbase.hstore.compaction.min"));
+      assertNull(hTable.getDescriptor().getColumnFamily(FAMILY).getValue(
+          Bytes.toBytes("hbase.hstore.compaction.min")));
     }
   }
 
@@ -481,7 +469,7 @@ public class TestFromClientSide3 {
           new Put(ROW).addColumn(new byte[]{'b', 'o', 'g', 'u', 's'}, QUALIFIERS[0], VALUE)));
         t.batch(Arrays.asList(arm), batchResult);
         fail("Expected RetriesExhaustedWithDetailsException with NoSuchColumnFamilyException");
-      } catch(RetriesExhaustedWithDetailsException e) {
+      } catch(RetriesExhaustedException e) {
         String msg = e.getMessage();
         assertTrue(msg.contains("NoSuchColumnFamilyException"));
       }
@@ -568,7 +556,7 @@ public class TestFromClientSide3 {
       getList.add(get);
       getList.add(get2);
 
-      boolean[] exists = table.existsAll(getList);
+      boolean[] exists = table.exists(getList);
       assertEquals(true, exists[0]);
       assertEquals(true, exists[1]);
 
@@ -617,7 +605,7 @@ public class TestFromClientSide3 {
     gets.add(new Get(Bytes.add(ANOTHERROW, new byte[] { 0x00 })));
 
     LOG.info("Calling exists");
-    boolean[] results = table.existsAll(gets);
+    boolean[] results = table.exists(gets);
     assertFalse(results[0]);
     assertFalse(results[1]);
     assertTrue(results[2]);
@@ -631,7 +619,7 @@ public class TestFromClientSide3 {
     gets = new ArrayList<>();
     gets.add(new Get(new byte[] { 0x00 }));
     gets.add(new Get(new byte[] { 0x00, 0x00 }));
-    results = table.existsAll(gets);
+    results = table.exists(gets);
     assertTrue(results[0]);
     assertFalse(results[1]);
 
@@ -644,7 +632,7 @@ public class TestFromClientSide3 {
     gets.add(new Get(new byte[] { (byte) 0xff }));
     gets.add(new Get(new byte[] { (byte) 0xff, (byte) 0xff }));
     gets.add(new Get(new byte[] { (byte) 0xff, (byte) 0xff, (byte) 0xff }));
-    results = table.existsAll(gets);
+    results = table.exists(gets);
     assertFalse(results[0]);
     assertTrue(results[1]);
     assertFalse(results[2]);
@@ -681,8 +669,9 @@ public class TestFromClientSide3 {
 
   @Test
   public void testConnectionDefaultUsesCodec() throws Exception {
-    ConnectionImplementation con = (ConnectionImplementation) TEST_UTIL.getConnection();
-    assertTrue(con.hasCellBlockSupport());
+    try (RpcClient client = RpcClientFactory.createClient(TEST_UTIL.getConfiguration(), "cluster")) {
+      assertTrue(client.hasCellBlockSupport());
+    }
   }
 
   @Test
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideScanExcpetion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideScanExcpetion.java
index 34481bae66..d8d94108bc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideScanExcpetion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideScanExcpetion.java
@@ -78,6 +78,7 @@ public class TestFromClientSideScanExcpetion {
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     Configuration conf = TEST_UTIL.getConfiguration();
+    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 3);
     conf.setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 6000000);
     conf.setClass(HConstants.REGION_IMPL, MyHRegion.class, HRegion.class);
     conf.setBoolean("hbase.client.log.scanner.activity", true);
@@ -222,7 +223,6 @@ public class TestFromClientSideScanExcpetion {
   @Test
   public void testScannerFailsAfterRetriesWhenCoprocessorThrowsIOE()
       throws IOException, InterruptedException {
-    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 3);
     TableName tableName = TableName.valueOf(name.getMethodName());
     reset();
     THROW_ONCE.set(false); // throw exceptions in every retry
@@ -232,11 +232,12 @@ public class TestFromClientSideScanExcpetion {
       inject();
       TEST_UTIL.countRows(t, new Scan().addColumn(FAMILY, FAMILY));
       fail("Should have thrown an exception");
-    } catch (DoNotRetryIOException expected) {
-      assertThat(expected, instanceOf(ScannerResetException.class));
+    } catch (ScannerResetException expected) {
+      // expected
+    } catch (RetriesExhaustedException e) {
       // expected
+      assertThat(e.getCause(), instanceOf(ScannerResetException.class));
     }
     assertTrue(REQ_COUNT.get() >= 3);
   }
-
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestGetProcedureResult.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestGetProcedureResult.java
index 4186594c51..f2979e4c6c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestGetProcedureResult.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestGetProcedureResult.java
@@ -43,7 +43,6 @@ import org.junit.experimental.categories.Category;
 
 import org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;
 
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.GetProcedureResultRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.GetProcedureResultResponse;
 
@@ -115,10 +114,8 @@ public class TestGetProcedureResult {
 
   private GetProcedureResultResponse.State getState(long procId)
       throws MasterNotRunningException, IOException, ServiceException {
-    MasterProtos.MasterService.BlockingInterface master =
-      ((ConnectionImplementation) UTIL.getConnection()).getMaster();
-    GetProcedureResultResponse resp = master.getProcedureResult(null,
-      GetProcedureResultRequest.newBuilder().setProcId(procId).build());
+    GetProcedureResultResponse resp = UTIL.getMiniHBaseCluster().getMaster().getMasterRpcServices()
+      .getProcedureResult(null, GetProcedureResultRequest.newBuilder().setProcId(procId).build());
     return resp.getState();
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
index b1aba6a11b..58b30e4ce7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
@@ -112,30 +112,23 @@ public class TestIncrementsFromClientSide {
     // Client will retry beacuse rpc timeout is small than the sleep time of first rpc call
     c.setInt(HConstants.HBASE_RPC_TIMEOUT_KEY, 1500);
 
-    Connection connection = ConnectionFactory.createConnection(c);
-    Table t = connection.getTable(TableName.valueOf(name.getMethodName()));
-    if (t instanceof HTable) {
-      HTable table = (HTable) t;
-      table.setOperationTimeout(3 * 1000);
-
-      try {
-        Increment inc = new Increment(ROW);
-        inc.addColumn(TEST_UTIL.fam1, QUALIFIER, 1);
-        Result result = table.increment(inc);
-
-        Cell [] cells = result.rawCells();
-        assertEquals(1, cells.length);
-        assertIncrementKey(cells[0], ROW, TEST_UTIL.fam1, QUALIFIER, 1);
-
-        // Verify expected result
-        Result readResult = table.get(new Get(ROW));
-        cells = readResult.rawCells();
-        assertEquals(1, cells.length);
-        assertIncrementKey(cells[0], ROW, TEST_UTIL.fam1, QUALIFIER, 1);
-      } finally {
-        table.close();
-        connection.close();
-      }
+
+    try (Connection connection = ConnectionFactory.createConnection(c);
+        Table table = connection.getTableBuilder(TableName.valueOf(name.getMethodName()), null)
+          .setOperationTimeout(3 * 1000).build()) {
+      Increment inc = new Increment(ROW);
+      inc.addColumn(TEST_UTIL.fam1, QUALIFIER, 1);
+      Result result = table.increment(inc);
+
+      Cell[] cells = result.rawCells();
+      assertEquals(1, cells.length);
+      assertIncrementKey(cells[0], ROW, TEST_UTIL.fam1, QUALIFIER, 1);
+
+      // Verify expected result
+      Result readResult = table.get(new Get(ROW));
+      cells = readResult.rawCells();
+      assertEquals(1, cells.length);
+      assertIncrementKey(cells[0], ROW, TEST_UTIL.fam1, QUALIFIER, 1);
     }
   }
 
@@ -216,38 +209,36 @@ public class TestIncrementsFromClientSide {
   public void testIncrementInvalidArguments() throws Exception {
     LOG.info("Starting " + this.name.getMethodName());
     final TableName TABLENAME =
-        TableName.valueOf(filterStringSoTableNameSafe(this.name.getMethodName()));
+      TableName.valueOf(filterStringSoTableNameSafe(this.name.getMethodName()));
     Table ht = TEST_UTIL.createTable(TABLENAME, FAMILY);
     final byte[] COLUMN = Bytes.toBytes("column");
     try {
       // try null row
       ht.incrementColumnValue(null, FAMILY, COLUMN, 5);
-      fail("Should have thrown IOException");
-    } catch (IOException iox) {
+      fail("Should have thrown NPE/IOE");
+    } catch (NullPointerException | IOException error) {
       // success
     }
     try {
       // try null family
       ht.incrementColumnValue(ROW, null, COLUMN, 5);
-      fail("Should have thrown IOException");
-    } catch (IOException iox) {
+      fail("Should have thrown NPE/IOE");
+    } catch (NullPointerException | IOException error) {
       // success
     }
     // try null row
     try {
-      Increment incNoRow = new Increment((byte [])null);
+      Increment incNoRow = new Increment((byte[]) null);
       incNoRow.addColumn(FAMILY, COLUMN, 5);
-      fail("Should have thrown IllegalArgumentException");
-    } catch (IllegalArgumentException iax) {
-      // success
-    } catch (NullPointerException npe) {
+      fail("Should have thrown IAE/NPE");
+    } catch (IllegalArgumentException | NullPointerException error) {
       // success
     }
     // try null family
     try {
       Increment incNoFamily = new Increment(ROW);
       incNoFamily.addColumn(null, COLUMN, 5);
-      fail("Should have thrown IllegalArgumentException");
+      fail("Should have thrown IAE");
     } catch (IllegalArgumentException iax) {
       // success
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMalformedCellFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMalformedCellFromClient.java
index ef4ca253f4..ab7d070404 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMalformedCellFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMalformedCellFromClient.java
@@ -122,12 +122,8 @@ public class TestMalformedCellFromClient {
       try {
         table.batch(batches, results);
         fail("Where is the exception? We put the malformed cells!!!");
-      } catch (RetriesExhaustedWithDetailsException e) {
-        for (Throwable throwable : e.getCauses()) {
-          assertNotNull(throwable);
-        }
-        assertEquals(1, e.getNumExceptions());
-        exceptionByCaught = e.getCause(0);
+      } catch (RetriesExhaustedException e) {
+        exceptionByCaught = e.getCause();
       }
       for (Object obj : results) {
         assertNotNull(obj);
@@ -285,12 +281,14 @@ public class TestMalformedCellFromClient {
     try (Table table = TEST_UTIL.getConnection().getTable(TABLE_NAME)) {
       table.batch(batches, objs);
       fail("Where is the exception? We put the malformed cells!!!");
-    } catch (RetriesExhaustedWithDetailsException e) {
-      assertEquals(2, e.getNumExceptions());
-      for (int i = 0; i != e.getNumExceptions(); ++i) {
-        assertNotNull(e.getCause(i));
-        assertEquals(DoNotRetryIOException.class, e.getCause(i).getClass());
-        assertEquals("fail", Bytes.toString(e.getRow(i).getRow()));
+    } catch (RetriesExhaustedException e) {
+      Throwable error = e.getCause();
+      for (;;) {
+        assertNotNull("Can not find a DoNotRetryIOException on stack trace", error);
+        if (error instanceof DoNotRetryIOException) {
+          break;
+        }
+        error = error.getCause();
       }
     } finally {
       assertObjects(objs, batches.size());
@@ -319,12 +317,14 @@ public class TestMalformedCellFromClient {
     try (Table table = TEST_UTIL.getConnection().getTable(TABLE_NAME)) {
       table.batch(batches, objs);
       fail("Where is the exception? We put the malformed cells!!!");
-    } catch (RetriesExhaustedWithDetailsException e) {
-      assertEquals(1, e.getNumExceptions());
-      for (int i = 0; i != e.getNumExceptions(); ++i) {
-        assertNotNull(e.getCause(i));
-        assertTrue(e.getCause(i) instanceof IOException);
-        assertEquals("fail", Bytes.toString(e.getRow(i).getRow()));
+    } catch (RetriesExhaustedException e) {
+      Throwable error = e.getCause();
+      for (;;) {
+        assertNotNull("Can not find a DoNotRetryIOException on stack trace", error);
+        if (error instanceof DoNotRetryIOException) {
+          break;
+        }
+        error = error.getCause();
       }
     } finally {
       assertObjects(objs, batches.size());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaCache.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaCache.java
index 50db5f03a0..a6b73aec9a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaCache.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaCache.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
+import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
@@ -50,6 +51,10 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.GetResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 
+/**
+ * Will be removed along with ConnectionImplementation soon.
+ */
+@Ignore
 @Category({MediumTests.class, ClientTests.class})
 public class TestMetaCache {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaWithReplicas.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaWithReplicas.java
index 336a2721d2..280f183e21 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaWithReplicas.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaWithReplicas.java
@@ -51,6 +51,7 @@ import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
 import org.apache.hadoop.hbase.master.assignment.AssignmentTestingUtil;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.StorefileRefresherChore;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.HBaseFsck;
@@ -217,7 +218,9 @@ public class TestMetaWithReplicas {
       util.getAdmin().deleteTable(TABLE);
     }
     ServerName master = null;
-    try (Connection c = ConnectionFactory.createConnection(util.getConfiguration())) {
+    try (
+      ConnectionImplementation c = ConnectionFactory.createConnectionImpl(util.getConfiguration(),
+        null, UserProvider.instantiate(util.getConfiguration()).getCurrent())) {
       try (Table htable = util.createTable(TABLE, FAMILIES)) {
         util.getAdmin().flush(TableName.META_TABLE_NAME);
         Thread.sleep(conf.getInt(StorefileRefresherChore.REGIONSERVER_STOREFILE_REFRESH_PERIOD,
@@ -287,6 +290,10 @@ public class TestMetaWithReplicas {
     }
   }
 
+  /**
+   * Will removed along with ConnectionImplementation soon.
+   */
+  @Ignore
   @Test
   public void testMetaLookupThreadPoolCreated() throws Exception {
     final TableName tableName = TableName.valueOf(name.getMethodName());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiActionMetricsFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiActionMetricsFromClient.java
index 349f052684..9db29cea7f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiActionMetricsFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiActionMetricsFromClient.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -49,7 +50,7 @@ public class TestMultiActionMetricsFromClient {
   public static void setUpBeforeClass() throws Exception {
     TEST_UTIL.startMiniCluster(1);
     TEST_UTIL.getHBaseCluster().waitForActiveAndReadyMaster();
-    TEST_UTIL.waitUntilAllRegionsAssigned(TABLE_NAME.META_TABLE_NAME);
+    TEST_UTIL.waitUntilAllRegionsAssigned(TableName.META_TABLE_NAME);
     TEST_UTIL.createTable(TABLE_NAME, FAMILY);
   }
 
@@ -62,12 +63,10 @@ public class TestMultiActionMetricsFromClient {
   public void testMultiMetrics() throws Exception {
     Configuration conf = new Configuration(TEST_UTIL.getConfiguration());
     conf.set(MetricsConnection.CLIENT_SIDE_METRICS_ENABLED_KEY, "true");
-    ConnectionImplementation conn =
-      (ConnectionImplementation) ConnectionFactory.createConnection(conf);
-
-    try {
+    try (ConnectionImplementation conn = ConnectionFactory.createConnectionImpl(conf, null,
+      UserProvider.instantiate(conf).getCurrent())) {
       BufferedMutator mutator = conn.getBufferedMutator(TABLE_NAME);
-      byte[][] keys = {Bytes.toBytes("aaa"), Bytes.toBytes("mmm"), Bytes.toBytes("zzz")};
+      byte[][] keys = { Bytes.toBytes("aaa"), Bytes.toBytes("mmm"), Bytes.toBytes("zzz") };
       for (byte[] key : keys) {
         Put p = new Put(key);
         p.addColumn(FAMILY, QUALIFIER, Bytes.toBytes(10));
@@ -81,8 +80,6 @@ public class TestMultiActionMetricsFromClient {
       assertEquals(1, metrics.multiTracker.reqHist.getCount());
       assertEquals(3, metrics.numActionsPerServerHist.getSnapshot().getMean(), 1e-15);
       assertEquals(1, metrics.numActionsPerServerHist.getCount());
-    } finally {
-      conn.close();
     }
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
index 50c9bd8d9e..e2b7e56622 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
@@ -48,6 +48,7 @@ import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
 import org.apache.hadoop.hbase.coprocessor.MasterObserver;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.FlakeyTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -170,38 +171,6 @@ public class TestMultiParallel {
     return keys.toArray(new byte [][] {new byte [] {}});
   }
 
-
-  /**
-   * This is for testing the active number of threads that were used while
-   * doing a batch operation. It inserts one row per region via the batch
-   * operation, and then checks the number of active threads.
-   * <p/>
-   * For HBASE-3553
-   */
-  @Test
-  public void testActiveThreadsCount() throws Exception {
-    UTIL.getConfiguration().setLong("hbase.htable.threads.coresize", slaves + 1);
-    try (Connection connection = ConnectionFactory.createConnection(UTIL.getConfiguration())) {
-      ThreadPoolExecutor executor = HTable.getDefaultExecutor(UTIL.getConfiguration());
-      try {
-        try (Table t = connection.getTable(TEST_TABLE, executor)) {
-          List<Put> puts = constructPutRequests(); // creates a Put for every region
-          t.batch(puts, null);
-          HashSet<ServerName> regionservers = new HashSet<>();
-          try (RegionLocator locator = connection.getRegionLocator(TEST_TABLE)) {
-            for (Row r : puts) {
-              HRegionLocation location = locator.getRegionLocation(r.getRow());
-              regionservers.add(location.getServerName());
-            }
-          }
-          assertEquals(regionservers.size(), executor.getLargestPoolSize());
-        }
-      } finally {
-        executor.shutdownNow();
-      }
-    }
-  }
-
   @Test
   public void testBatchWithGet() throws Exception {
     LOG.info("test=testBatchWithGet");
@@ -256,14 +225,12 @@ public class TestMultiParallel {
 
     // row1 and row2 should be in the same region.
 
-    Object [] r = new Object[actions.size()];
+    Object[] r = new Object[actions.size()];
     try {
       table.batch(actions, r);
       fail();
-    } catch (RetriesExhaustedWithDetailsException ex) {
-      LOG.debug(ex.toString(), ex);
-      // good!
-      assertFalse(ex.mayHaveClusterIssues());
+    } catch (RetriesExhaustedException ex) {
+      // expected
     }
     assertEquals(2, r.length);
     assertTrue(r[0] instanceof Throwable);
@@ -434,7 +401,6 @@ public class TestMultiParallel {
       deletes.add(delete);
     }
     table.delete(deletes);
-    Assert.assertTrue(deletes.isEmpty());
 
     // Get to make sure ...
     for (byte[] k : KEYS) {
@@ -522,41 +488,44 @@ public class TestMultiParallel {
   @Test
   public void testNonceCollision() throws Exception {
     LOG.info("test=testNonceCollision");
-    final Connection connection = ConnectionFactory.createConnection(UTIL.getConfiguration());
-    Table table = connection.getTable(TEST_TABLE);
-    Put put = new Put(ONE_ROW);
-    put.addColumn(BYTES_FAMILY, QUALIFIER, Bytes.toBytes(0L));
+    try (
+      ConnectionImplementation connection =
+        ConnectionFactory.createConnectionImpl(UTIL.getConfiguration(), null,
+          UserProvider.instantiate(UTIL.getConfiguration()).getCurrent());
+      Table table = connection.getTable(TEST_TABLE)) {
+      Put put = new Put(ONE_ROW);
+      put.addColumn(BYTES_FAMILY, QUALIFIER, Bytes.toBytes(0L));
 
-    // Replace nonce manager with the one that returns each nonce twice.
-    NonceGenerator cnm = new NonceGenerator() {
+      // Replace nonce manager with the one that returns each nonce twice.
+      NonceGenerator cnm = new NonceGenerator() {
 
-      private final PerClientRandomNonceGenerator delegate = PerClientRandomNonceGenerator.get();
+        private final PerClientRandomNonceGenerator delegate = PerClientRandomNonceGenerator.get();
 
-      private long lastNonce = -1;
+        private long lastNonce = -1;
 
-      @Override
-      public synchronized long newNonce() {
-        long nonce = 0;
-        if (lastNonce == -1) {
-          lastNonce = nonce = delegate.newNonce();
-        } else {
-          nonce = lastNonce;
-          lastNonce = -1L;
+        @Override
+        public synchronized long newNonce() {
+          long nonce = 0;
+          if (lastNonce == -1) {
+            nonce = delegate.newNonce();
+            lastNonce = nonce;
+          } else {
+            nonce = lastNonce;
+            lastNonce = -1L;
+          }
+          return nonce;
         }
-        return nonce;
-      }
 
-      @Override
-      public long getNonceGroup() {
-        return delegate.getNonceGroup();
-      }
-    };
+        @Override
+        public long getNonceGroup() {
+          return delegate.getNonceGroup();
+        }
+      };
 
-    NonceGenerator oldCnm =
-      ConnectionUtils.injectNonceGeneratorForTesting((ConnectionImplementation) connection, cnm);
+      NonceGenerator oldCnm =
+        ConnectionUtils.injectNonceGeneratorForTesting((ConnectionImplementation) connection, cnm);
 
-    // First test sequential requests.
-    try {
+      // First test sequential requests.
       Increment inc = new Increment(ONE_ROW);
       inc.addColumn(BYTES_FAMILY, QUALIFIER, 1L);
       table.increment(inc);
@@ -613,10 +582,6 @@ public class TestMultiParallel {
       get.addColumn(BYTES_FAMILY, QUALIFIER);
       result = table.get(get);
       validateResult(result, QUALIFIER, Bytes.toBytes((numRequests / 2) + 1L));
-      table.close();
-    } finally {
-      ConnectionImplementation.injectNonceGeneratorForTesting((ConnectionImplementation) connection,
-        oldCnm);
     }
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRegionLocationCaching.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRegionLocationCaching.java
index b877ad79bd..44efcbc595 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRegionLocationCaching.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRegionLocationCaching.java
@@ -33,9 +33,14 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
+import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+/**
+ * To be rewrite to check async meta cache.
+ */
+@Ignore
 @Category({MediumTests.class, ClientTests.class})
 public class TestRegionLocationCaching {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
index d53353e409..96e9de8875 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
@@ -60,6 +60,7 @@ import org.junit.AfterClass;
 import org.junit.Assert;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
+import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.slf4j.Logger;
@@ -646,6 +647,8 @@ public class TestReplicaWithCluster {
   // This test is to test when hbase.client.metaReplicaCallTimeout.scan is configured, meta table
   // scan will always get the result from primary meta region as long as the result is returned
   // within configured hbase.client.metaReplicaCallTimeout.scan from primary meta region.
+  // To be rewrite, and meta replicas is not stable
+  @Ignore
   @Test
   public void testGetRegionLocationFromPrimaryMetaRegion() throws IOException, InterruptedException {
     HTU.getAdmin().balancerSwitch(false, true);
@@ -679,6 +682,8 @@ public class TestReplicaWithCluster {
   // are down, hbase client is able to access user replica regions and return stale data.
   // Meta replica is enabled to show the case that the meta replica region could be out of sync
   // with the primary meta region.
+  // To be rewrite, and meta replicas is not stable
+  @Ignore
   @Test
   public void testReplicaGetWithPrimaryAndMetaDown() throws IOException, InterruptedException {
     HTU.getAdmin().balancerSwitch(false, true);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
index c8a7ca188f..befe28bebb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
@@ -42,6 +42,7 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.RegionLocations;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;
@@ -51,6 +52,7 @@ import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.StorefileRefresherChore;
 import org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster;
+import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -86,8 +88,9 @@ public class TestReplicasClient {
   private static final Logger LOG = LoggerFactory.getLogger(TestReplicasClient.class);
 
   private static final int NB_SERVERS = 1;
-  private static Table table = null;
-  private static final byte[] row = Bytes.toBytes(TestReplicasClient.class.getName());
+  private static TableName TABLE_NAME;
+  private Table table = null;
+  private static final byte[] row = Bytes.toBytes(TestReplicasClient.class.getName());;
 
   private static HRegionInfo hriPrimary;
   private static HRegionInfo hriSecondary;
@@ -202,8 +205,8 @@ public class TestReplicasClient {
     // Create table then get the single region for our new table.
     HTableDescriptor hdt = HTU.createTableDescriptor(TestReplicasClient.class.getSimpleName());
     hdt.addCoprocessor(SlowMeCopro.class.getName());
-    table = HTU.createTable(hdt, new byte[][]{f}, null);
-
+    HTU.createTable(hdt, new byte[][]{f}, null);
+    TABLE_NAME = hdt.getTableName();
     try (RegionLocator locator = HTU.getConnection().getRegionLocator(hdt.getTableName())) {
       hriPrimary = locator.getRegionLocation(row, false).getRegionInfo();
     }
@@ -223,7 +226,6 @@ public class TestReplicasClient {
   @AfterClass
   public static void afterClass() throws Exception {
     HRegionServer.TEST_SKIP_REPORTING_TRANSITION = false;
-    if (table != null) table.close();
     HTU.shutdownMiniCluster();
   }
 
@@ -238,6 +240,7 @@ public class TestReplicasClient {
       openRegion(hriSecondary);
     } catch (Exception ignored) {
     }
+    table = HTU.getConnection().getTable(TABLE_NAME);
   }
 
   @After
@@ -328,9 +331,10 @@ public class TestReplicasClient {
   public void testLocations() throws Exception {
     byte[] b1 = Bytes.toBytes("testLocations");
     openRegion(hriSecondary);
-    ConnectionImplementation hc = (ConnectionImplementation) HTU.getConnection();
 
-    try {
+    try (
+        ConnectionImplementation hc = ConnectionFactory.createConnectionImpl(HTU.getConfiguration(),
+          null, UserProvider.instantiate(HTU.getConfiguration()).getCurrent())) {
       hc.clearRegionLocationCache();
       RegionLocations rl = hc.locateRegion(table.getName(), b1, false, false);
       Assert.assertEquals(2, rl.size());
@@ -551,6 +555,10 @@ public class TestReplicasClient {
     }
   }
 
+  /**
+   * To be rewrite without ConnectionImplementation
+   */
+  @Ignore
   @Test
   public void testHedgedRead() throws Exception {
     byte[] b1 = Bytes.toBytes("testHedgedRead");
@@ -690,24 +698,40 @@ public class TestReplicasClient {
     }
   }
 
+  /**
+   * To be rewrite
+   */
+  @Ignore
   @Test
   public void testScanWithReplicas() throws Exception {
     //simple scan
     runMultipleScansOfOneType(false, false);
   }
 
+  /**
+   * To be rewrite
+   */
+  @Ignore
   @Test
   public void testSmallScanWithReplicas() throws Exception {
     //small scan
     runMultipleScansOfOneType(false, true);
   }
 
+  /**
+   * To be rewrite
+   */
+  @Ignore
   @Test
   public void testReverseScanWithReplicas() throws Exception {
     //reverse scan
     runMultipleScansOfOneType(true, false);
   }
 
+  /**
+   * To be rewrite
+   */
+  @Ignore
   @Test
   public void testCancelOfScan() throws Exception {
     openRegion(hriSecondary);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScanWithoutFetchingData.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScanWithoutFetchingData.java
index f5c217605e..858fbb753b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScanWithoutFetchingData.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScanWithoutFetchingData.java
@@ -23,8 +23,8 @@ import static org.junit.Assert.assertTrue;
 import java.io.IOException;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.ipc.BlockingRpcCallback;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
 import org.apache.hadoop.hbase.ipc.HBaseRpcControllerImpl;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
@@ -65,9 +65,11 @@ public class TestScanWithoutFetchingData {
 
   private static final int COUNT = 10;
 
-  private static HRegionInfo HRI;
+  private static RegionInfo HRI;
 
-  private static ClientProtos.ClientService.BlockingInterface STUB;
+  private static AsyncConnectionImpl CONN;
+
+  private static ClientProtos.ClientService.Interface STUB;
 
   @BeforeClass
   public static void setUp() throws Exception {
@@ -77,9 +79,10 @@ public class TestScanWithoutFetchingData {
         table.put(new Put(Bytes.toBytes(i)).addColumn(CF, CQ, Bytes.toBytes(i)));
       }
     }
-    HRI = UTIL.getAdmin().getTableRegions(TABLE_NAME).get(0);
-    STUB = ((ConnectionImplementation) UTIL.getConnection())
-        .getClient(UTIL.getHBaseCluster().getRegionServer(0).getServerName());
+    HRI = UTIL.getAdmin().getRegions(TABLE_NAME).get(0);
+    CONN =
+      (AsyncConnectionImpl) ConnectionFactory.createAsyncConnection(UTIL.getConfiguration()).get();
+    STUB = CONN.getRegionServerStub(UTIL.getHBaseCluster().getRegionServer(0).getServerName());
   }
 
   @AfterClass
@@ -87,6 +90,12 @@ public class TestScanWithoutFetchingData {
     UTIL.shutdownMiniCluster();
   }
 
+  private ScanResponse scan(HBaseRpcController hrc, ScanRequest req) throws IOException {
+    BlockingRpcCallback<ScanResponse> callback = new BlockingRpcCallback<>();
+    STUB.scan(hrc, req, callback);
+    return callback.get();
+  }
+
   private void assertResult(int row, Result result) {
     assertEquals(row, Bytes.toInt(result.getRow()));
     assertEquals(row, Bytes.toInt(result.getValue(CF, CQ)));
@@ -97,7 +106,7 @@ public class TestScanWithoutFetchingData {
     Scan scan = new Scan();
     ScanRequest req = RequestConverter.buildScanRequest(HRI.getRegionName(), scan, 0, false);
     HBaseRpcController hrc = new HBaseRpcControllerImpl();
-    ScanResponse resp = STUB.scan(hrc, req);
+    ScanResponse resp = scan(hrc, req);
     assertTrue(resp.getMoreResults());
     assertTrue(resp.getMoreResultsInRegion());
     assertEquals(0, ResponseConverter.getResults(hrc.cellScanner(), resp).length);
@@ -107,7 +116,7 @@ public class TestScanWithoutFetchingData {
     for (int i = 0; i < COUNT / 2; i++) {
       req = RequestConverter.buildScanRequest(scannerId, 1, false, nextCallSeq++, false, false, -1);
       hrc.reset();
-      resp = STUB.scan(hrc, req);
+      resp = scan(hrc, req);
       assertTrue(resp.getMoreResults());
       assertTrue(resp.getMoreResultsInRegion());
       Result[] results = ResponseConverter.getResults(hrc.cellScanner(), resp);
@@ -117,14 +126,14 @@ public class TestScanWithoutFetchingData {
     // test zero next
     req = RequestConverter.buildScanRequest(scannerId, 0, false, nextCallSeq++, false, false, -1);
     hrc.reset();
-    resp = STUB.scan(hrc, req);
+    resp = scan(hrc, req);
     assertTrue(resp.getMoreResults());
     assertTrue(resp.getMoreResultsInRegion());
     assertEquals(0, ResponseConverter.getResults(hrc.cellScanner(), resp).length);
     for (int i = COUNT / 2; i < COUNT; i++) {
       req = RequestConverter.buildScanRequest(scannerId, 1, false, nextCallSeq++, false, false, -1);
       hrc.reset();
-      resp = STUB.scan(hrc, req);
+      resp = scan(hrc, req);
       assertTrue(resp.getMoreResults());
       assertEquals(i != COUNT - 1, resp.getMoreResultsInRegion());
       Result[] results = ResponseConverter.getResults(hrc.cellScanner(), resp);
@@ -133,6 +142,7 @@ public class TestScanWithoutFetchingData {
     }
     // close
     req = RequestConverter.buildScanRequest(scannerId, 0, true, false);
-    resp = STUB.scan(null, req);
+    hrc.reset();
+    resp = scan(hrc, req);
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
index af0248215c..a1a5136e75 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
@@ -20,20 +20,19 @@ package org.apache.hadoop.hbase.client;
 import static org.apache.hadoop.hbase.HConstants.RPC_CODEC_CONF_KEY;
 import static org.apache.hadoop.hbase.client.TestFromClientSide3.generateHugeValue;
 import static org.apache.hadoop.hbase.ipc.RpcClient.DEFAULT_CODEC_CLASS;
+import static org.hamcrest.CoreMatchers.instanceOf;
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.concurrent.TimeUnit;
-import java.util.function.Consumer;
-import java.util.stream.IntStream;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CompareOperator;
@@ -90,9 +89,6 @@ public class TestScannersFromClientSide {
   @Rule
   public TestName name = new TestName();
 
-  /**
-   * @throws java.lang.Exception
-   */
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     Configuration conf = TEST_UTIL.getConfiguration();
@@ -100,17 +96,11 @@ public class TestScannersFromClientSide {
     TEST_UTIL.startMiniCluster(3);
   }
 
-  /**
-   * @throws java.lang.Exception
-   */
   @AfterClass
   public static void tearDownAfterClass() throws Exception {
     TEST_UTIL.shutdownMiniCluster();
   }
 
-  /**
-   * @throws java.lang.Exception
-   */
   @Before
   public void setUp() throws Exception {
     // Nothing to do.
@@ -126,8 +116,6 @@ public class TestScannersFromClientSide {
 
   /**
    * Test from client side for batch of scan
-   *
-   * @throws Exception
    */
   @Test
   public void testScanBatch() throws Exception {
@@ -236,17 +224,15 @@ public class TestScannersFromClientSide {
     // Create a scan with the default configuration.
     Scan scan = new Scan();
 
-    ResultScanner scanner = ht.getScanner(scan);
-    assertTrue(scanner instanceof ClientScanner);
-    ClientScanner clientScanner = (ClientScanner) scanner;
-
-    // Call next to issue a single RPC to the server
-    scanner.next();
-
-    // The scanner should have, at most, a single result in its cache. If there more results exists
-    // in the cache it means that more than the expected max result size was fetched.
-    assertTrue("The cache contains: " + clientScanner.getCacheSize() + " results",
-      clientScanner.getCacheSize() <= 1);
+    try (ResultScanner scanner = ht.getScanner(scan)) {
+      assertThat(scanner, instanceOf(AsyncTableResultScanner.class));
+      scanner.next();
+      AsyncTableResultScanner s = (AsyncTableResultScanner) scanner;
+      // The scanner should have, at most, a single result in its cache. If there more results
+      // exists
+      // in the cache it means that more than the expected max result size was fetched.
+      assertTrue("The cache contains: " + s.getCacheSize() + " results", s.getCacheSize() <= 1);
+    }
   }
 
   /**
@@ -304,11 +290,6 @@ public class TestScannersFromClientSide {
 
   /**
    * Run through a variety of test configurations with a small scan
-   * @param table
-   * @param reversed
-   * @param rows
-   * @param columns
-   * @throws Exception
    */
   private void testSmallScan(Table table, boolean reversed, int rows, int columns) throws Exception {
     Scan baseScan = new Scan();
@@ -349,8 +330,6 @@ public class TestScannersFromClientSide {
 
   /**
    * Test from client side for get with maxResultPerCF set
-   *
-   * @throws Exception
    */
   @Test
   public void testGetMaxResults() throws Exception {
@@ -469,8 +448,6 @@ public class TestScannersFromClientSide {
 
   /**
    * Test from client side for scan with maxResultPerCF set
-   *
-   * @throws Exception
    */
   @Test
   public void testScanMaxResults() throws Exception {
@@ -519,8 +496,6 @@ public class TestScannersFromClientSide {
 
   /**
    * Test from client side for get with rowOffset
-   *
-   * @throws Exception
    */
   @Test
   public void testGetRowOffset() throws Exception {
@@ -639,8 +614,6 @@ public class TestScannersFromClientSide {
   /**
    * Test from client side for scan while the region is reopened
    * on the same region server.
-   *
-   * @throws Exception
    */
   @Test
   public void testScanOnReopenedRegion() throws Exception {
@@ -713,125 +686,6 @@ public class TestScannersFromClientSide {
     verifyResult(result, kvListExp, toLog, "Testing scan on re-opened region");
   }
 
-  @Test
-  public void testAsyncScannerWithSmallData() throws Exception {
-    testAsyncScanner(TableName.valueOf(name.getMethodName()),
-      2,
-      3,
-      10,
-      -1,
-      null);
-  }
-
-  @Test
-  public void testAsyncScannerWithManyRows() throws Exception {
-    testAsyncScanner(TableName.valueOf(name.getMethodName()),
-      30000,
-      1,
-      1,
-      -1,
-      null);
-  }
-
-  @Test
-  public void testAsyncScannerWithoutCaching() throws Exception {
-    testAsyncScanner(TableName.valueOf(name.getMethodName()),
-      5,
-      1,
-      1,
-      1,
-      (b) -> {
-        try {
-          TimeUnit.MILLISECONDS.sleep(500);
-        } catch (InterruptedException ex) {
-        }
-      });
-  }
-
-  private void testAsyncScanner(TableName table, int rowNumber, int familyNumber,
-      int qualifierNumber, int caching, Consumer<Boolean> listener) throws Exception {
-    assert rowNumber > 0;
-    assert familyNumber > 0;
-    assert qualifierNumber > 0;
-    byte[] row = Bytes.toBytes("r");
-    byte[] family = Bytes.toBytes("f");
-    byte[] qualifier = Bytes.toBytes("q");
-    byte[][] rows = makeNAsciiWithZeroPrefix(row, rowNumber);
-    byte[][] families = makeNAsciiWithZeroPrefix(family, familyNumber);
-    byte[][] qualifiers = makeNAsciiWithZeroPrefix(qualifier, qualifierNumber);
-
-    Table ht = TEST_UTIL.createTable(table, families);
-
-    boolean toLog = true;
-    List<Cell> kvListExp = new ArrayList<>();
-
-    List<Put> puts = new ArrayList<>();
-    for (byte[] r : rows) {
-      Put put = new Put(r);
-      for (byte[] f : families) {
-        for (byte[] q : qualifiers) {
-          KeyValue kv = new KeyValue(r, f, q, 1, VALUE);
-          put.add(kv);
-          kvListExp.add(kv);
-        }
-      }
-      puts.add(put);
-      if (puts.size() > 1000) {
-        ht.put(puts);
-        puts.clear();
-      }
-    }
-    if (!puts.isEmpty()) {
-      ht.put(puts);
-      puts.clear();
-    }
-
-    Scan scan = new Scan();
-    scan.setAsyncPrefetch(true);
-    if (caching > 0) {
-      scan.setCaching(caching);
-    }
-    try (ResultScanner scanner = ht.getScanner(scan)) {
-      assertTrue("Not instance of async scanner",scanner instanceof ClientAsyncPrefetchScanner);
-      ((ClientAsyncPrefetchScanner) scanner).setPrefetchListener(listener);
-      List<Cell> kvListScan = new ArrayList<>();
-      Result result;
-      boolean first = true;
-      int actualRows = 0;
-      while ((result = scanner.next()) != null) {
-        ++actualRows;
-        // waiting for cache. see HBASE-17376
-        if (first) {
-          TimeUnit.SECONDS.sleep(1);
-          first = false;
-        }
-        for (Cell kv : result.listCells()) {
-          kvListScan.add(kv);
-        }
-      }
-      assertEquals(rowNumber, actualRows);
-      // These cells may have different rows but it is ok. The Result#getRow
-      // isn't used in the verifyResult()
-      result = Result.create(kvListScan);
-      verifyResult(result, kvListExp, toLog, "Testing async scan");
-    }
-
-    TEST_UTIL.deleteTable(table);
-  }
-
-  private static byte[][] makeNAsciiWithZeroPrefix(byte[] base, int n) {
-    int maxLength = Integer.toString(n).length();
-    byte [][] ret = new byte[n][];
-    for (int i = 0; i < n; i++) {
-      int length = Integer.toString(i).length();
-      StringBuilder buf = new StringBuilder(Integer.toString(i));
-      IntStream.range(0, maxLength - length).forEach(v -> buf.insert(0, "0"));
-      byte[] tail = Bytes.toBytes(buf.toString());
-      ret[i] = Bytes.add(base, tail);
-    }
-    return ret;
-  }
-
   static void verifyResult(Result result, List<Cell> expKvList, boolean toLog,
       String msg) {
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShortCircuitConnection.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShortCircuitConnection.java
deleted file mode 100644
index f74338865c..0000000000
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShortCircuitConnection.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client;
-
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-import org.apache.hadoop.hbase.HBaseClassTestRule;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.regionserver.RSRpcServices;
-import org.apache.hadoop.hbase.testclassification.ClientTests;
-import org.apache.hadoop.hbase.testclassification.MediumTests;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.ClassRule;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.junit.rules.TestName;
-
-import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.AdminService;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.ClientService;
-
-@Category({ MediumTests.class, ClientTests.class })
-public class TestShortCircuitConnection {
-
-  @ClassRule
-  public static final HBaseClassTestRule CLASS_RULE =
-      HBaseClassTestRule.forClass(TestShortCircuitConnection.class);
-
-  private final static HBaseTestingUtility UTIL = new HBaseTestingUtility();
-
-  @Rule
-  public TestName name = new TestName();
-
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    UTIL.startMiniCluster(1);
-  }
-
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    UTIL.shutdownMiniCluster();
-  }
-
-  @Test
-  @SuppressWarnings("deprecation")
-  public void testShortCircuitConnection() throws IOException, InterruptedException {
-    final TableName tableName = TableName.valueOf(name.getMethodName());
-    HTableDescriptor htd = UTIL.createTableDescriptor(tableName);
-    HColumnDescriptor hcd = new HColumnDescriptor(Bytes.toBytes("cf"));
-    htd.addFamily(hcd);
-    UTIL.createTable(htd, null);
-    HRegionServer regionServer = UTIL.getRSForFirstRegionInTable(tableName);
-    ConnectionImplementation connection = (ConnectionImplementation) regionServer.getConnection();
-    Table tableIf = connection.getTable(tableName);
-    assertTrue(tableIf instanceof HTable);
-    HTable table = (HTable) tableIf;
-    assertTrue(table.getConnection() == connection);
-    AdminService.BlockingInterface admin = connection.getAdmin(regionServer.getServerName());
-    ClientService.BlockingInterface client = connection.getClient(regionServer.getServerName());
-    assertTrue(admin instanceof RSRpcServices);
-    assertTrue(client instanceof RSRpcServices);
-    ServerName anotherSn = ServerName.valueOf(regionServer.getServerName().getHostAndPort(),
-      EnvironmentEdgeManager.currentTime());
-    admin = connection.getAdmin(anotherSn);
-    client = connection.getClient(anotherSn);
-    assertFalse(admin instanceof RSRpcServices);
-    assertFalse(client instanceof RSRpcServices);
-    assertTrue(connection.getAdmin().getConnection() == connection);
-  }
-}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotCloneIndependence.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotCloneIndependence.java
index 2f95a9087d..afe08c959b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotCloneIndependence.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotCloneIndependence.java
@@ -335,7 +335,7 @@ public class TestSnapshotCloneIndependence {
     // get a description of the cloned table
     // get a list of its families
     // assert that the family is there
-    HTableDescriptor originalTableDescriptor = originalTable.getTableDescriptor();
+    HTableDescriptor originalTableDescriptor = new HTableDescriptor(originalTable.getDescriptor());
     HTableDescriptor clonedTableDescriptor = admin.getTableDescriptor(cloneTableName);
 
     Assert.assertTrue("The original family was not found. There is something wrong. ",
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
index ef52057ad7..2af9f70fff 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
@@ -318,7 +318,7 @@ public class TestSnapshotMetadata {
     Table original = UTIL.getConnection().getTable(originalTableName);
     try {
       assertTrue(originalTableDescriptor.equals(admin.getTableDescriptor(originalTableName)));
-      assertTrue(originalTableDescriptor.equals(original.getTableDescriptor()));
+      assertTrue(originalTableDescriptor.equals(new HTableDescriptor(original.getDescriptor())));
     } finally {
       original.close();
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorShortCircuitRPC.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorSharedConnection.java
similarity index 80%
rename from hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorShortCircuitRPC.java
rename to hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorSharedConnection.java
index 6cff379a44..40a3d65bbc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorShortCircuitRPC.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorSharedConnection.java
@@ -24,9 +24,8 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.SharedConnection;
 import org.apache.hadoop.hbase.client.Connection;
-import org.apache.hadoop.hbase.client.ConnectionUtils;
+import org.apache.hadoop.hbase.client.SharedConnection;
 import org.apache.hadoop.hbase.testclassification.CoprocessorTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.junit.AfterClass;
@@ -38,19 +37,19 @@ import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
 /**
- * Ensure Coprocessors get ShortCircuit Connections when they get a Connection from their
+ * Ensure Coprocessors get ShardConnections when they get a Connection from their
  * CoprocessorEnvironment.
  */
-@Category({CoprocessorTests.class, MediumTests.class})
-public class TestCoprocessorShortCircuitRPC {
+@Category({ CoprocessorTests.class, MediumTests.class })
+public class TestCoprocessorSharedConnection {
 
   @ClassRule
   public static final HBaseClassTestRule CLASS_RULE =
-      HBaseClassTestRule.forClass(TestCoprocessorShortCircuitRPC.class);
+      HBaseClassTestRule.forClass(TestCoprocessorSharedConnection.class);
 
   @Rule
   public TestName name = new TestName();
-  private static final HBaseTestingUtility HTU = HBaseTestingUtility.createLocalHTU();
+  private static final HBaseTestingUtility HTU = new HBaseTestingUtility();
 
   /**
    * Start up a mini cluster with my three CPs loaded.
@@ -83,8 +82,6 @@ public class TestCoprocessorShortCircuitRPC {
     public void start(CoprocessorEnvironment env) throws IOException {
       // At start, we get base CoprocessorEnvironment Type, not MasterCoprocessorEnvironment,
       checkShared(((MasterCoprocessorEnvironment) env).getConnection());
-      checkShortCircuit(
-        ((MasterCoprocessorEnvironment) env).createConnection(env.getConfiguration()));
     }
   }
 
@@ -96,8 +93,6 @@ public class TestCoprocessorShortCircuitRPC {
     public void start(CoprocessorEnvironment env) throws IOException {
       // At start, we get base CoprocessorEnvironment Type, not RegionServerCoprocessorEnvironment,
       checkShared(((RegionServerCoprocessorEnvironment) env).getConnection());
-      checkShortCircuit(
-        ((RegionServerCoprocessorEnvironment) env).createConnection(env.getConfiguration()));
     }
   }
 
@@ -109,8 +104,6 @@ public class TestCoprocessorShortCircuitRPC {
     public void start(CoprocessorEnvironment env) throws IOException {
       // At start, we get base CoprocessorEnvironment Type, not RegionCoprocessorEnvironment,
       checkShared(((RegionCoprocessorEnvironment) env).getConnection());
-      checkShortCircuit(
-        ((RegionCoprocessorEnvironment) env).createConnection(env.getConfiguration()));
     }
   }
 
@@ -118,10 +111,6 @@ public class TestCoprocessorShortCircuitRPC {
     assertTrue(connection instanceof SharedConnection);
   }
 
-  private static void checkShortCircuit(Connection connection) {
-    assertTrue(connection instanceof ConnectionUtils.ShortCircuitingClusterConnection);
-  }
-
   @Test
   public void test() throws IOException {
     // Nothing to do in here. The checks are done as part of the cluster spinup when CPs get
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestPassCustomCellViaRegionObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestPassCustomCellViaRegionObserver.java
index d55e8e0182..69b9132d64 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestPassCustomCellViaRegionObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestPassCustomCellViaRegionObserver.java
@@ -156,10 +156,11 @@ public class TestPassCustomCellViaRegionObserver {
       table.get(new Get(ROW)).isEmpty());
     assertObserverHasExecuted();
 
-    assertTrue(table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put));
+    assertTrue(table.checkAndMutate(ROW, FAMILY).qualifier(QUALIFIER).ifNotExists().thenPut(put));
     assertObserverHasExecuted();
 
-    assertTrue(table.checkAndDelete(ROW, FAMILY, QUALIFIER, VALUE, delete));
+    assertTrue(
+      table.checkAndMutate(ROW, FAMILY).qualifier(QUALIFIER).ifEquals(VALUE).thenDelete(delete));
     assertObserverHasExecuted();
 
     assertTrue(table.get(new Get(ROW)).isEmpty());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
index 695b9f5c39..584261de72 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
@@ -381,29 +381,28 @@ public class TestMultiRowRangeFilter {
   public void testMultiRowRangeFilterWithExclusive() throws IOException {
     tableName = TableName.valueOf(name.getMethodName());
     TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 6000000);
-    Table ht = TEST_UTIL.createTable(tableName, family, Integer.MAX_VALUE);
-    ht.setReadRpcTimeout(600000);
-    ht.setOperationTimeout(6000000);
-    generateRows(numRows, ht, family, qf, value);
-
-    Scan scan = new Scan();
-    scan.setMaxVersions();
-
-    List<RowRange> ranges = new ArrayList<>();
-    ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
-    ranges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(40), false));
-    ranges.add(new RowRange(Bytes.toBytes(65), true, Bytes.toBytes(75), false));
-
-    MultiRowRangeFilter filter = new MultiRowRangeFilter(ranges);
-    scan.setFilter(filter);
-    int resultsSize = getResultsSize(ht, scan);
-    LOG.info("found " + resultsSize + " results");
-    List<Cell> results1 = getScanResult(Bytes.toBytes(10), Bytes.toBytes(40), ht);
-    List<Cell> results2 = getScanResult(Bytes.toBytes(65), Bytes.toBytes(75), ht);
-
-    assertEquals((results1.size() - 1) + results2.size(), resultsSize);
-
-    ht.close();
+    TEST_UTIL.createTable(tableName, family, Integer.MAX_VALUE);
+    try (Table ht = TEST_UTIL.getConnection().getTableBuilder(tableName, null)
+      .setReadRpcTimeout(600000).setOperationTimeout(6000000).build()) {
+      generateRows(numRows, ht, family, qf, value);
+
+      Scan scan = new Scan();
+      scan.setMaxVersions();
+
+      List<RowRange> ranges = new ArrayList<>();
+      ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
+      ranges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(40), false));
+      ranges.add(new RowRange(Bytes.toBytes(65), true, Bytes.toBytes(75), false));
+
+      MultiRowRangeFilter filter = new MultiRowRangeFilter(ranges);
+      scan.setFilter(filter);
+      int resultsSize = getResultsSize(ht, scan);
+      LOG.info("found " + resultsSize + " results");
+      List<Cell> results1 = getScanResult(Bytes.toBytes(10), Bytes.toBytes(40), ht);
+      List<Cell> results2 = getScanResult(Bytes.toBytes(65), Bytes.toBytes(75), ht);
+
+      assertEquals((results1.size() - 1) + results2.size(), resultsSize);
+    }
   }
 
   @Test
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestWarmupRegion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestWarmupRegion.java
index 3babd2ea79..a930076d39 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestWarmupRegion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestWarmupRegion.java
@@ -24,7 +24,6 @@ import java.io.IOException;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.Waiter;
@@ -32,6 +31,7 @@ import org.apache.hadoop.hbase.client.CompactionState;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.RegionInfo;
 import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
@@ -139,7 +139,7 @@ public class TestWarmupRegion {
         RegionInfo info = region.getRegionInfo();
 
         try {
-          HTableDescriptor htd = table.getTableDescriptor();
+          TableDescriptor htd = table.getDescriptor();
           for (int i = 0; i < 10; i++) {
             warmupHRegion(info, htd, rs.getWAL(info), rs.getConfiguration(), rs, null);
           }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
index 3c06d6e77c..dbbb981979 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
@@ -17,18 +17,19 @@
  */
 package org.apache.hadoop.hbase.regionserver;
 
+import com.google.protobuf.Descriptors.MethodDescriptor;
+import com.google.protobuf.Message;
+import com.google.protobuf.Service;
+import com.google.protobuf.ServiceException;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.CompareOperator;
 import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Durability;
@@ -45,14 +46,8 @@ import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
 import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
 import org.apache.hadoop.hbase.client.metrics.ScanMetrics;
-import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
 
-import com.google.protobuf.Descriptors.MethodDescriptor;
-import com.google.protobuf.Message;
-import com.google.protobuf.Service;
-import com.google.protobuf.ServiceException;
-
 /**
  * An implementation of {@link Table} that sits directly on a Region; it decorates the passed in
  * Region instance with the Table API. Some API is not implemented yet (throws
@@ -83,12 +78,6 @@ public class RegionAsTable implements Table {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public HTableDescriptor getTableDescriptor() throws IOException {
-    return new HTableDescriptor(this.region.getTableDescriptor());
-  }
-
   @Override
   public TableDescriptor getDescriptor() throws IOException {
     return this.region.getTableDescriptor();
@@ -213,29 +202,6 @@ public class RegionAsTable implements Table {
     for (Put put: puts) put(put);
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier, byte[] value, Put put)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier, CompareOp compareOp,
-      byte[] value, Put put)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-                             CompareOperator compareOp, byte[] value, Put put)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public void delete(Delete delete) throws IOException {
     this.region.delete(delete);
@@ -246,29 +212,6 @@ public class RegionAsTable implements Table {
     for(Delete delete: deletes) delete(delete);
   }
 
-  @Override
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier, byte[] value,
-      Delete delete)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      CompareOp compareOp, byte[] value, Delete delete)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-                                CompareOperator compareOp, byte[] value, Delete delete)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public CheckAndMutateBuilder checkAndMutate(byte[] row, byte[] family) {
     throw new UnsupportedOperationException();
@@ -343,78 +286,23 @@ public class RegionAsTable implements Table {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier, CompareOp compareOp,
-      byte[] value, RowMutations mutation)
-  throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier,
-      CompareOperator compareOp, byte[] value, RowMutations mutation) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public void setOperationTimeout(int operationTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public int getOperationTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  @Deprecated
-  public void setRpcTimeout(int rpcTimeout) {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getReadRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public void setWriteRpcTimeout(int writeRpcTimeout) {throw new UnsupportedOperationException(); }
-
   @Override
   public long getOperationTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public void setReadRpcTimeout(int readRpcTimeout) {throw new UnsupportedOperationException(); }
-
   @Override
   public long getWriteRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
 
-  @Override
-  @Deprecated
-  public int getRpcTimeout() {
-    throw new UnsupportedOperationException();
-  }
-
   @Override
   public long getRpcTimeout(TimeUnit unit) {
     throw new UnsupportedOperationException();
   }
-
-  @Override
-  @Deprecated
-  public int getWriteRpcTimeout() { throw new UnsupportedOperationException(); }
-
-  @Override
-  @Deprecated
-  public int getReadRpcTimeout() { throw new UnsupportedOperationException(); }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
index 1418d6e74f..bc12438cc8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
@@ -222,7 +222,7 @@ public class TestEndToEndSplitTransaction {
     RegionSplitter(Table table) throws IOException {
       this.table = table;
       this.tableName = table.getName();
-      this.family = table.getTableDescriptor().getFamiliesKeys().iterator().next();
+      this.family = table.getDescriptor().getColumnFamilyNames().iterator().next();
       admin = TEST_UTIL.getAdmin();
       rs = TEST_UTIL.getMiniHBaseCluster().getRegionServer(0);
       connection = TEST_UTIL.getConnection();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java
index be29f1a9e3..dc538b019b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java
@@ -39,10 +39,11 @@ import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Admin;
-import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.RegionInfo;
 import org.apache.hadoop.hbase.client.RegionInfoBuilder;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.testclassification.RegionServerTests;
@@ -81,9 +82,9 @@ public class TestHRegionFileSystem {
     TEST_UTIL = new HBaseTestingUtility();
     Configuration conf = TEST_UTIL.getConfiguration();
     TEST_UTIL.startMiniCluster();
-    HTable table = (HTable) TEST_UTIL.createTable(TABLE_NAME, FAMILIES);
+    Table table = TEST_UTIL.createTable(TABLE_NAME, FAMILIES);
     assertEquals("Should start with empty table", 0, TEST_UTIL.countRows(table));
-    HRegionFileSystem regionFs = getHRegionFS(table, conf);
+    HRegionFileSystem regionFs = getHRegionFS(TEST_UTIL.getConnection(), table, conf);
     // the original block storage policy would be HOT
     String spA = regionFs.getStoragePolicyName(Bytes.toString(FAMILIES[0]));
     String spB = regionFs.getStoragePolicyName(Bytes.toString(FAMILIES[1]));
@@ -96,8 +97,8 @@ public class TestHRegionFileSystem {
     TEST_UTIL.shutdownMiniCluster();
     TEST_UTIL.getConfiguration().set(HStore.BLOCK_STORAGE_POLICY_KEY, "WARM");
     TEST_UTIL.startMiniCluster();
-    table = (HTable) TEST_UTIL.createTable(TABLE_NAME, FAMILIES);
-    regionFs = getHRegionFS(table, conf);
+    table = TEST_UTIL.createTable(TABLE_NAME, FAMILIES);
+    regionFs = getHRegionFS(TEST_UTIL.getConnection(), table, conf);
 
     try (Admin admin = TEST_UTIL.getConnection().getAdmin()) {
       spA = regionFs.getStoragePolicyName(Bytes.toString(FAMILIES[0]));
@@ -180,14 +181,16 @@ public class TestHRegionFileSystem {
     }
   }
 
-  private HRegionFileSystem getHRegionFS(HTable table, Configuration conf) throws IOException {
+  private HRegionFileSystem getHRegionFS(Connection conn, Table table, Configuration conf)
+      throws IOException {
     FileSystem fs = TEST_UTIL.getDFSCluster().getFileSystem();
     Path tableDir = FSUtils.getTableDir(TEST_UTIL.getDefaultRootDirPath(), table.getName());
     List<Path> regionDirs = FSUtils.getRegionDirs(fs, tableDir);
     assertEquals(1, regionDirs.size());
     List<Path> familyDirs = FSUtils.getFamilyDirs(fs, regionDirs.get(0));
     assertEquals(2, familyDirs.size());
-    RegionInfo hri = table.getRegionLocator().getAllRegionLocations().get(0).getRegionInfo();
+    RegionInfo hri =
+      conn.getRegionLocator(table.getName()).getAllRegionLocations().get(0).getRegionInfo();
     HRegionFileSystem regionFs = new HRegionFileSystem(conf, new HFileSystem(fs), tableDir, hri);
     return regionFs;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestNewVersionBehaviorFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestNewVersionBehaviorFromClientSide.java
index 897fc3c00d..5d80f550e0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestNewVersionBehaviorFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestNewVersionBehaviorFromClientSide.java
@@ -311,9 +311,10 @@ public class TestNewVersionBehaviorFromClientSide {
 
   @Test
   public void testgetColumnHint() throws IOException {
-    try (Table t = createTable()) {
-      t.setOperationTimeout(10000);
-      t.setRpcTimeout(10000);
+    createTable();
+    try (Table t =
+      TEST_UTIL.getConnection().getTableBuilder(TableName.valueOf(name.getMethodName()), null)
+        .setOperationTimeout(10000).setRpcTimeout(10000).build()) {
       t.put(new Put(ROW).addColumn(FAMILY, col1, 100, value));
       t.put(new Put(ROW).addColumn(FAMILY, col1, 101, value));
       t.put(new Put(ROW).addColumn(FAMILY, col1, 102, value));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPerColumnFamilyFlush.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPerColumnFamilyFlush.java
index 0e7c019de6..68ba2e04b4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPerColumnFamilyFlush.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPerColumnFamilyFlush.java
@@ -353,13 +353,6 @@ public class TestPerColumnFamilyFlush {
       TEST_UTIL.getAdmin().createNamespace(
         NamespaceDescriptor.create(TABLENAME.getNamespaceAsString()).build());
       Table table = TEST_UTIL.createTable(TABLENAME, FAMILIES);
-      HTableDescriptor htd = table.getTableDescriptor();
-
-      for (byte[] family : FAMILIES) {
-        if (!htd.hasFamily(family)) {
-          htd.addFamily(new HColumnDescriptor(family));
-        }
-      }
 
       // Add 100 edits for CF1, 20 for CF2, 20 for CF3.
       // These will all be interleaved in the log.
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSettingTimeoutOnBlockingPoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSettingTimeoutOnBlockingPoint.java
index 130b6514ac..3885312c01 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSettingTimeoutOnBlockingPoint.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSettingTimeoutOnBlockingPoint.java
@@ -107,12 +107,10 @@ public class TestSettingTimeoutOnBlockingPoint {
       }
     });
     Thread getThread = new Thread(() -> {
-      try {
-        try( Table table = TEST_UTIL.getConnection().getTable(tableName)) {
-          table.setRpcTimeout(1000);
-          Delete delete = new Delete(ROW1);
-          table.delete(delete);
-        }
+      try (Table table =
+        TEST_UTIL.getConnection().getTableBuilder(tableName, null).setRpcTimeout(1000).build()) {
+        Delete delete = new Delete(ROW1);
+        table.delete(delete);
       } catch (IOException e) {
         Assert.fail(e.getMessage());
       }
@@ -122,12 +120,12 @@ public class TestSettingTimeoutOnBlockingPoint {
     Threads.sleep(1000);
     getThread.start();
     Threads.sleep(2000);
-    try (Table table = TEST_UTIL.getConnection().getTable(tableName)) {
+    try (Table table =
+      TEST_UTIL.getConnection().getTableBuilder(tableName, null).setRpcTimeout(1000).build()) {
       // We have only two handlers. The first thread will get a write lock for row1 and occupy
       // the first handler. The second thread need a read lock for row1, it should quit after 1000
       // ms and give back the handler because it can not get the lock in time.
       // So we can get the value using the second handler.
-      table.setRpcTimeout(1000);
       table.get(new Get(ROW2)); // Will throw exception if the timeout checking is failed
     } finally {
       incrementThread.interrupt();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
index dd60597e11..ddc3176d80 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
@@ -212,7 +212,6 @@ public class TestReplicationBase {
     // than default
     conf1 = utility1.getConfiguration();
     zkw1 = new ZKWatcher(conf1, "cluster1", null, true);
-    admin = new ReplicationAdmin(conf1);
     LOG.info("Setup first Zk");
 
     utility2.setZkCluster(miniZK);
@@ -225,6 +224,7 @@ public class TestReplicationBase {
     // as a component in deciding maximum number of parallel batches to send to the peer cluster.
     utility2.startMiniCluster(4);
 
+    admin = new ReplicationAdmin(conf1);
     hbaseAdmin = ConnectionFactory.createConnection(conf1).getAdmin();
 
     TableDescriptor table = TableDescriptorBuilder.newBuilder(tableName)
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
index 8c5299e902..5a185a0ddc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
@@ -121,7 +121,6 @@ public class TestReplicationWithTags {
     // Have to reget conf1 in case zk cluster location different
     // than default
     conf1 = utility1.getConfiguration();
-    replicationAdmin = new ReplicationAdmin(conf1);
     LOG.info("Setup first Zk");
 
     // Base conf2 on conf1 so it gets the right zk cluster.
@@ -141,6 +140,7 @@ public class TestReplicationWithTags {
     utility1.startMiniCluster(2);
     utility2.startMiniCluster(2);
 
+    replicationAdmin = new ReplicationAdmin(conf1);
     ReplicationPeerConfig rpc = new ReplicationPeerConfig();
     rpc.setClusterKey(utility2.getClusterKey());
     replicationAdmin.addPeer("2", rpc, null);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCoprocessorWhitelistMasterObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCoprocessorWhitelistMasterObserver.java
index f9b610af10..1ead7a981d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCoprocessorWhitelistMasterObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCoprocessorWhitelistMasterObserver.java
@@ -109,7 +109,7 @@ public class TestCoprocessorWhitelistMasterObserver extends SecureTestUtil {
     UTIL.waitUntilAllRegionsAssigned(TEST_TABLE);
     Connection connection = ConnectionFactory.createConnection(conf);
     Table t = connection.getTable(TEST_TABLE);
-    HTableDescriptor htd = new HTableDescriptor(t.getTableDescriptor());
+    HTableDescriptor htd = new HTableDescriptor(t.getDescriptor());
     htd.addCoprocessor("net.clayb.hbase.coprocessor.NotWhitelisted",
       new Path(coprocessorPath),
       Coprocessor.PRIORITY_USER, null);
@@ -121,7 +121,7 @@ public class TestCoprocessorWhitelistMasterObserver extends SecureTestUtil {
       // swallow exception from coprocessor
     }
     LOG.info("Done Modifying Table");
-    assertEquals(0, t.getTableDescriptor().getCoprocessors().size());
+    assertEquals(0, t.getDescriptor().getCoprocessorDescriptors().size());
   }
 
   /**
@@ -155,13 +155,13 @@ public class TestCoprocessorWhitelistMasterObserver extends SecureTestUtil {
     // coprocessor file
     admin.disableTable(TEST_TABLE);
     Table t = connection.getTable(TEST_TABLE);
-    HTableDescriptor htd = new HTableDescriptor(t.getTableDescriptor());
+    HTableDescriptor htd = new HTableDescriptor(t.getDescriptor());
     htd.addCoprocessor("net.clayb.hbase.coprocessor.Whitelisted",
       new Path(coprocessorPath),
       Coprocessor.PRIORITY_USER, null);
     LOG.info("Modifying Table");
     admin.modifyTable(TEST_TABLE, htd);
-    assertEquals(1, t.getTableDescriptor().getCoprocessors().size());
+    assertEquals(1, t.getDescriptor().getCoprocessorDescriptors().size());
     LOG.info("Done Modifying Table");
   }
 
@@ -327,6 +327,6 @@ public class TestCoprocessorWhitelistMasterObserver extends SecureTestUtil {
     // ensure table was created and coprocessor is added to table
     LOG.info("Done Creating Table");
     Table t = connection.getTable(TEST_TABLE);
-    assertEquals(1, t.getTableDescriptor().getCoprocessors().size());
+    assertEquals(1, t.getDescriptor().getCoprocessorDescriptors().size());
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRegionSnapshotTask.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRegionSnapshotTask.java
index 17674af16a..fb7da12327 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRegionSnapshotTask.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRegionSnapshotTask.java
@@ -127,7 +127,7 @@ public class TestRegionSnapshotTask {
     Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir, conf);
     final SnapshotManifest manifest =
         SnapshotManifest.create(conf, fs, workingDir, snapshot, monitor);
-    manifest.addTableDescriptor(table.getTableDescriptor());
+    manifest.addTableDescriptor(table.getDescriptor());
 
     if (!fs.exists(workingDir)) {
       fs.mkdirs(workingDir);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/TestCanaryTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/TestCanaryTool.java
index 6c23764a46..9848733b39 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/TestCanaryTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/TestCanaryTool.java
@@ -249,8 +249,7 @@ public class TestCanaryTool {
   private void testZookeeperCanaryWithArgs(String[] args) throws Exception {
     Integer port =
       Iterables.getOnlyElement(testingUtility.getZkCluster().getClientPortList(), null);
-    testingUtility.getConfiguration().set(HConstants.ZOOKEEPER_QUORUM,
-      "localhost:" + port + "/hbase");
+    testingUtility.getConfiguration().set(HConstants.ZOOKEEPER_QUORUM, "localhost:" + port);
     ExecutorService executor = new ScheduledThreadPoolExecutor(2);
     Canary.ZookeeperStdOutSink sink = spy(new Canary.ZookeeperStdOutSink());
     Canary canary = new Canary(executor, sink);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
index f245384083..681499cdd3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
@@ -144,9 +144,8 @@ public abstract class MultiThreadedAction {
 
   public static final int REPORTING_INTERVAL_MS = 5000;
 
-  public MultiThreadedAction(LoadTestDataGenerator dataGen, Configuration conf,
-                             TableName tableName,
-                             String actionLetter) throws IOException {
+  public MultiThreadedAction(LoadTestDataGenerator dataGen, Configuration conf, TableName tableName,
+      String actionLetter) throws IOException {
     this.conf = conf;
     this.dataGenerator = dataGen;
     this.tableName = tableName;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
index 531d08c441..b8ba4982f7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
@@ -22,7 +22,6 @@ import static org.junit.Assert.assertEquals;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
@@ -47,6 +46,7 @@ import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.client.TableDescriptor;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.testclassification.MiscTests;
@@ -137,7 +137,7 @@ public class OfflineMetaRebuildTestCore {
     return this.connection.getTable(tablename);
   }
 
-  private void dumpMeta(HTableDescriptor htd) throws IOException {
+  private void dumpMeta(TableDescriptor htd) throws IOException {
     List<byte[]> metaRows = TEST_UTIL.getMetaTableRows(htd.getTableName());
     for (byte[] row : metaRows) {
       LOG.info(Bytes.toString(row));
@@ -162,7 +162,7 @@ public class OfflineMetaRebuildTestCore {
       byte[] startKey, byte[] endKey) throws IOException {
 
     LOG.info("Before delete:");
-    HTableDescriptor htd = tbl.getTableDescriptor();
+    TableDescriptor htd = tbl.getDescriptor();
     dumpMeta(htd);
 
     List<HRegionLocation> regions;
@@ -171,7 +171,7 @@ public class OfflineMetaRebuildTestCore {
     }
 
     for (HRegionLocation e : regions) {
-      RegionInfo hri = e.getRegionInfo();
+      RegionInfo hri = e.getRegion();
       ServerName hsa = e.getServerName();
       if (Bytes.compareTo(hri.getStartKey(), startKey) == 0
           && Bytes.compareTo(hri.getEndKey(), endKey) == 0) {
@@ -203,7 +203,6 @@ public class OfflineMetaRebuildTestCore {
   protected RegionInfo createRegion(Configuration conf, final Table htbl,
       byte[] startKey, byte[] endKey) throws IOException {
     Table meta = TEST_UTIL.getConnection().getTable(TableName.META_TABLE_NAME);
-    HTableDescriptor htd = htbl.getTableDescriptor();
     RegionInfo hri = RegionInfoBuilder.newBuilder(htbl.getName())
         .setStartKey(startKey)
         .setEndKey(endKey)
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java
index 34bf5e8705..358a519baa 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java
@@ -30,7 +30,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellBuilder;
@@ -47,6 +46,7 @@ import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.client.Append;
+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Durability;
 import org.apache.hadoop.hbase.client.Get;
@@ -103,16 +103,12 @@ public class ThriftHBaseServiceHandler extends HBaseServiceHandler implements Hb
 
   /**
    * Returns a list of all the column families for a given Table.
-   *
-   * @param table table
-   * @throws IOException
    */
   byte[][] getAllColumns(Table table) throws IOException {
-    HColumnDescriptor[] cds = table.getTableDescriptor().getColumnFamilies();
+    ColumnFamilyDescriptor[] cds = table.getDescriptor().getColumnFamilies();
     byte[][] columns = new byte[cds.length][];
     for (int i = 0; i < cds.length; i++) {
-      columns[i] = Bytes.add(cds[i].getName(),
-          KeyValue.COLUMN_FAMILY_DELIM_ARRAY);
+      columns[i] = Bytes.add(cds[i].getName(), KeyValue.COLUMN_FAMILY_DELIM_ARRAY);
     }
     return columns;
   }
@@ -1090,7 +1086,7 @@ public class ThriftHBaseServiceHandler extends HBaseServiceHandler implements Hb
       TreeMap<ByteBuffer, ColumnDescriptor> columns = new TreeMap<>();
 
       table = getTable(tableName);
-      HTableDescriptor desc = table.getTableDescriptor();
+      HTableDescriptor desc = new HTableDescriptor(table.getDescriptor());
 
       for (HColumnDescriptor e : desc.getFamilies()) {
         ColumnDescriptor col = ThriftUtilities.colDescFromHbase(e);
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
index a112cef7cb..9a4fbbb75f 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
@@ -227,7 +227,7 @@ public class ThriftHBaseServiceHandler extends HBaseServiceHandler implements TH
   public List<Boolean> existsAll(ByteBuffer table, List<TGet> gets) throws TIOError, TException {
     Table htable = getTable(table);
     try {
-      boolean[] exists = htable.existsAll(getsFromThrift(gets));
+      boolean[] exists = htable.exists(getsFromThrift(gets));
       List<Boolean> result = new ArrayList<>(exists.length);
       for (boolean exist : exists) {
         result.add(exist);
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java
index abaaba0242..4db8fd6e7e 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java
@@ -35,6 +35,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.AsyncConnection;
 import org.apache.hadoop.hbase.client.BufferedMutator;
 import org.apache.hadoop.hbase.client.BufferedMutatorParams;
 import org.apache.hadoop.hbase.client.Connection;
@@ -367,4 +368,9 @@ public class ThriftConnection implements Connection {
   public void clearRegionLocationCache() {
     throw new NotImplementedException("clearRegionLocationCache not supported in ThriftTable");
   }
+
+  @Override
+  public AsyncConnection toAsyncConnection() {
+    throw new NotImplementedException("toAsyncConnection not supported in ThriftTable");
+  }
 }
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java
index 5c826b972c..1119a4bedb 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java
@@ -408,9 +408,7 @@ public class ThriftTable implements Table {
     }
   }
 
-
-  @Override
-  public boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier, CompareOperator op,
+  private boolean checkAndMutate(byte[] row, byte[] family, byte[] qualifier, CompareOperator op,
       byte[] value, RowMutations mutation) throws IOException {
     try {
       ByteBuffer valueBuffer = value == null? null : ByteBuffer.wrap(value);
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftConnection.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftConnection.java
index 74795924d3..7b5aa6653a 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftConnection.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftConnection.java
@@ -593,7 +593,7 @@ public class TestThriftConnection {
       assertTrue(Bytes.equals(VALUE_1, value1));
       assertNull(value2);
       assertTrue(table.exists(get));
-      assertEquals(1, table.existsAll(Collections.singletonList(get)).length);
+      assertEquals(1, table.exists(Collections.singletonList(get)).length);
       Delete delete = new Delete(ROW_1);
 
       table.checkAndMutate(ROW_1, FAMILYA).qualifier(QUALIFIER_1)
-- 
2.17.1

